{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "ae875319",
   "metadata": {},
   "outputs": [],
   "source": [
    "#this is jupyter file for building neural network from scratch so that i can understand each thing in detail\n",
    "# this is to understand neural network in deep level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c342dc41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# i am also following a book called basics of neural network in raw python "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "607a63ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# making a single neuron\n",
    "inputs = [1,2,3]\n",
    "weights = [0.2,0.8,-0.5]\n",
    "bias = 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81365d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3\n"
     ]
    }
   ],
   "source": [
    "# iutputing it\n",
    "output = (inputs[0]*weights[0]+ inputs[1]*weights[1]+ inputs[2]*weights[2]+ bias)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4a21d323",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "# single neuron with 4 input channel\n",
    "inputs = [1,2,3,2.5]\n",
    "weights = [0.2,0.8,-0.5,1.0]\n",
    "bias = 2\n",
    "output = (inputs[0]*weights[0]+ inputs[1]*weights[1]+ inputs[2]*weights[2]+ inputs[3]*weights[3]+ bias)\n",
    "print(output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "101de401",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "# 3 neurons and 4 inputs long and impractical method\n",
    "inputs = [1,2,3,2.5]\n",
    "\n",
    "weights1 = [0.2, 0.8, -0.5, 1] \n",
    "weights2 = [0.5, -0.91, 0.26, -0.5] \n",
    "weights3 = [-0.26, -0.27, 0.17, 0.87] \n",
    "\n",
    "bias1 = 2 \n",
    "bias2 = 3 \n",
    "bias3 = 0.5 \n",
    "\n",
    "\n",
    "outputs = [ \n",
    "  # Neuron 1: \n",
    "  inputs[0]*weights1[0] + \n",
    "  inputs[1]*weights1[1] + \n",
    "  inputs[2]*weights1[2] + \n",
    "  inputs[3]*weights1[3] + bias1, \n",
    "\n",
    "  # Neuron 2: \n",
    "  inputs[0]*weights2[0] + \n",
    "  inputs[1]*weights2[1] + \n",
    "  inputs[2]*weights2[2] + \n",
    "  inputs[3]*weights2[3] + bias2, \n",
    "\n",
    "  # Neuron 3: \n",
    "  inputs[0]*weights3[0] + \n",
    "  inputs[1]*weights3[1] + \n",
    "  inputs[2]*weights3[2] + \n",
    "  inputs[3]*weights3[3] + bias3] \n",
    " \n",
    "print(outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8db16083",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "#better method\n",
    "inputs = [1, 2, 3, 2.5] \n",
    "weights = [[0.2, 0.8, -0.5, 1], \n",
    "  [0.5, -0.91, 0.26, -0.5], \n",
    "  [-0.26, -0.27, 0.17, 0.87]] \n",
    "biases = [2, 3, 0.5] \n",
    "\n",
    "# Output of current layer \n",
    "layer_outputs = [] \n",
    "\n",
    "# For each neuron \n",
    "for neuron_weights, neuron_bias in zip(weights, biases): \n",
    "    # Zeroed output of given neuron \n",
    "    neuron_output = 0 \n",
    "    # For each input and weight to the neuron \n",
    "    for n_input, weight in zip(inputs, neuron_weights): \n",
    "        # Multiply this input by associated weight \n",
    "        # and add to the neuron‚Äôs output variable \n",
    "        neuron_output += n_input*weight \n",
    "    # Add bias \n",
    "    neuron_output += neuron_bias \n",
    "    # Put neuron‚Äôs result to the layer‚Äôs output list \n",
    "    layer_outputs.append(neuron_output) \n",
    " \n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ec6833dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we could modify our number of inputs or neurons in our \n",
    "# layer to be whatever we wanted, and our loop would handle it. As we said earlier, it would be \n",
    "# a disservice not to show NumPy here since Python alone doesn‚Äôt do matrix/tensor/array math \n",
    "# very efficiently.  But first, the reason the most popular deep learning library in Python is \n",
    "# called ‚ÄúTensorFlow‚Äù is that it‚Äôs all about doing operations on tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d41a3dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#What is a tensor, to a computer scientist, in the context of deep learning?‚Äù\n",
    "# A tensor object is an object that can be represented as an array. meaning we can treat tensors as array in deep learning\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c2d5655b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "20\n"
     ]
    }
   ],
   "source": [
    "#lets review dot product\n",
    "a = [1,2,3] \n",
    "b = [2,3,4] \n",
    "dot_product =  a[0]*b[0] + a[1]*b[1] + a[2]*b[2]\n",
    "print(dot_product)\n",
    "# this looks like input and weight multiplication we did in simple 3 neuron and 4 input case \n",
    "# so we can use dot porduct to do product in easy eay \n",
    "# now we will import numpy to do do product and vector mainpulation also "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e2111396",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.8\n"
     ]
    }
   ],
   "source": [
    "# a single neuron with nunpy\n",
    "import numpy as np \n",
    "inputs = [1.0, 2.0, 3.0, 2.5] \n",
    "weights = [0.2, 0.8, -0.5, 1.0] \n",
    "bias = 2.0 \n",
    "output = np.dot(inputs,weights) + bias\n",
    "print(output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "7f587cc8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8, 1.21, 2.385]\n"
     ]
    }
   ],
   "source": [
    "# layer of neuron in nunpy\n",
    "# better method done by me \n",
    "inputs = [1, 2, 3, 2.5] \n",
    "weights = [[0.2, 0.8, -0.5, 1], \n",
    "  [0.5, -0.91, 0.26, -0.5], \n",
    "  [-0.26, -0.27, 0.17, 0.87]] \n",
    "biases = [2, 3, 0.5] \n",
    "\n",
    "# Output of current layer \n",
    "layer_outputs = []\n",
    "i = 0 \n",
    "for weight in weights:\n",
    "  layer_outputs.append(np.dot(weight, inputs) + biases[i])\n",
    "  i += 1\n",
    "print(layer_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d1762bcd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[4.8   1.21  2.385]\n"
     ]
    }
   ],
   "source": [
    "# book method which is better then mine üò≠üò≠\n",
    "import numpy as np \n",
    "inputs = [1.0, 2.0, 3.0, 2.5] \n",
    "weights = [[0.2, 0.8, -0.5, 1], \n",
    "[0.5, -0.91, 0.26, -0.5], \n",
    "[-0.26, -0.27, 0.17, 0.87]] \n",
    "biases = [2.0, 3.0, 0.5] \n",
    "layer_outputs = np.dot(weights, inputs) + biases # this is like W*I +b in matrix form imagine like that and it will click\n",
    "print(layer_outputs) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c534b1a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3]])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# batch of data\n",
    "inputs = [[1, 2, 3, 2.5], [2, 5, -1, 2], [-1.5, 2.7, 3.3, -0.8]] # each  individual list inside the list is a instance \n",
    "# we can define array with numpy\n",
    "np.array([[1,2,3]])# use of double bracket to say that this is not a list but a matrix with one row only\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1814115a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3]])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Or we can turn it into a 1D array and expand dimensions using one of the NumPy abilities:\n",
    "a = [1, 2, 3] \n",
    "np.expand_dims(np.array(a), axis=0)# bo.expand_dims adds a new dimension using one of the numpy abiliaties\n",
    "               "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7ab3ab10",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[20]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = [1, 2, 3] \n",
    "b = [2, 3, 4] \n",
    "a = np.array([a]) \n",
    "b = np.array([b]).T \n",
    "np.dot(a, b) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5db6f2b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n",
      "[[ 4.8    9.9   -0.09 ]\n",
      " [ 0.21  -1.81  -1.449]\n",
      " [ 3.885  2.7    0.026]]\n"
     ]
    }
   ],
   "source": [
    "# now lets combine all those things and get a full neural network\n",
    "inputs = [[1.0, 2.0, 3.0, 2.5], \n",
    " [2.0, 5.0, -1.0, 2.0], \n",
    "   [-1.5, 2.7, 3.3, -0.8]] \n",
    "weights = [[0.2, 0.8, -0.5, 1.0], \n",
    " [0.5, -0.91, 0.26, -0.5], \n",
    "   [-0.26, -0.27, 0.17, 0.87]] \n",
    "biases = [2.0, 3.0, 0.5]\n",
    "\n",
    "layer_outputs = np.dot(inputs, np.array(weights).T) + biases# we chose this not the below one ing of a list of layer outputs per each sample than a list of neurons and their outputs sample-wise. We want the resulting array to be sample-related and not neuron-related as we‚Äôll pass those samples further through the network, and the next layer will expect a batch of inputs\n",
    "layer_outputs1 = np.dot(weights, np.array(inputs).T) + biases\n",
    "\n",
    "print(layer_outputs) # biases get added row by row\n",
    "\n",
    "print(layer_outputs1) # both the layer_output and layer output1 are transpose of each other without aadding basises after that it changes a lot "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "26621b3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 4.8    1.21   2.385]\n",
      " [ 8.9   -1.81   0.2  ]\n",
      " [ 1.41   1.051  0.026]]\n",
      "[[ 0.5031  -1.04185 -2.03875]\n",
      " [ 0.2434  -2.7332  -5.7633 ]\n",
      " [-0.99314  1.41254 -0.35655]]\n",
      "[[-1.823206   2.2962155 -1.216877   0.772475 ]\n",
      " [-3.474662   3.452205  -1.853103   0.7789   ]\n",
      " [-1.4753446  2.7837363  1.0144873 -0.91322  ]]\n"
     ]
    }
   ],
   "source": [
    "inputs = [[1, 2, 3, 2.5], [2., 5., -1., 2], [-1.5, 2.7, 3.3, -0.8]] \n",
    "weights = [[0.2, 0.8, -0.5, 1], \n",
    "           [0.5, -0.91, 0.26, -0.5], \n",
    "           [-0.26, -0.27, 0.17, 0.87]] \n",
    "biases = [2, 3, 0.5] \n",
    "weights2 = [[0.1, -0.14, 0.5], \n",
    "            [-0.5, 0.12, -0.33], \n",
    "            [-0.44, 0.73, -0.13]] \n",
    "biases2 = [-1, 2, -0.5] \n",
    "\n",
    "weights3 = [[0.1, -0.14, 0.5], \n",
    "            [-0.5, 0.12, -0.33], \n",
    "            [-0.44, 0.73, -0.13],[0.1,-0.9,0.4]] \n",
    "biases3 = [-1, 2, -0.5,0.6]\n",
    "layer1_outputs = np.dot(inputs, np.array(weights).T) + biases \n",
    "print(layer1_outputs)\n",
    "layer2_outputs = np.dot(layer1_outputs, np.array(weights2).T) + biases2 # second layer computation\n",
    "layer3_output = np.dot(layer2_outputs, np.array(weights3).T) + biases3 # third layer computation\n",
    "print(layer2_outputs) \n",
    "print(layer3_output)\n",
    "# like this we can add any number of hidden layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c2f0e888",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# training data\n",
    "#You will typically not be generating training data from a function for your neural networks. You will have an actual dataset. Generating a dataset this way is purely for convenience at this stage\n",
    "import matplotlib.pyplot as plt \n",
    "from nnfs.datasets import spiral_data\n",
    "X,Y = spiral_data(samples=100, classes=3)\n",
    "plt.scatter(X[:,0],X[:,1])# X have both coordinates and Y have that point is in which class\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ded3225f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjgAAAGdCAYAAAAfTAk2AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy80BEi2AAAACXBIWXMAAA9hAAAPYQGoP6dpAADV3UlEQVR4nOydd3gUVRfG39nZFELvvYqgFEFAEBSUjqgUBelVEJUiqIh8CFhQULFSVBBERSkCCgoKiDSlSu8dCb0noaTs3vf747JJNtmd2U22pNzfPvMkmb1z75nJlDPnnqKRJBQKhUKhUCiyEJZgC6BQKBQKhULha5SCo1AoFAqFIsuhFByFQqFQKBRZDqXgKBQKhUKhyHIoBUehUCgUCkWWQyk4CoVCoVAoshxKwVEoFAqFQpHlUAqOQqFQKBSKLIc12AIEAyEEzp49i9y5c0PTtGCLo1AoFAqFwgNIIiYmBiVKlIDFYmyjyZYKztmzZ1G6dOlgi6FQKBQKhSINREZGolSpUoZtsqWCkzt3bgDyAOXJkyfI0igUCoVCofCE6OholC5dOvE5bkS2VHAc01J58uRRCo5CoVAoFJkMT9xLlJOxQqFQKBSKLIdScBQKhUKhUGQ5lIKjUCgUCoUiy6EUHIVCoVAoFFkOpeAoFAqFQqHIcvhVwVm3bh2efPJJlChRApqm4ZdffjHdZs2aNahVqxbCwsJQsWJFzJo1K1WbKVOmoFy5cggPD0e9evWwZcsW3wuvUCgUCoUi0+JXBefmzZuoUaMGpkyZ4lH7EydO4PHHH0fjxo2xc+dODB06FP369cPy5csT28ybNw8vv/wyxo4di+3bt6NGjRpo2bIlLl686K/dUCgUCoVCkcnQSDIgA2kafv75Z7Rr185tmxEjRmDp0qXYu3dv4rrOnTvj+vXr+OOPPwAA9erVwwMPPIDJkycDkGUXSpcujcGDB+P111/3SJbo6GjkzZsXUVFRKg+OQqFQKBSZBG+e3xnKB2fjxo1o1qyZ07qWLVti48aNAID4+Hhs27bNqY3FYkGzZs0S27giLi4O0dHRTotCofAvPHgQnD0bnDsXvHAh2OIoFIpsRoZScM6fP4+iRYs6rStatCiio6Nx+/ZtXL58GXa73WWb8+fPu+13/PjxyJs3b+Ki6lApFP6DkZFg0yZAlXuBnj2Arl2A0qXAfv3A27eDLZ5CocgmZCgFx1+MHDkSUVFRiUtkZGSwRVIosiS8fBl4+CFg/XrnL2w2YNY3QPv2oBDBEU6hUGQrMlQtqmLFiuFCClP2hQsXkCdPHuTIkQO6rkPXdZdtihUr5rbfsLAwhIWF+UVmhUKRjMmTgbNnAbs99XdCACuWA6tWAc2bB142hUKRrchQFpz69etj1apVTutWrlyJ+vXrAwBCQ0NRu3ZtpzZCCKxatSqxjUKhCCIzZ7hWbhzoOvDdt4GTR6FQZFv8quDcuHEDO3fuxM6dOwHIMPCdO3fi1KlTAOTUUc+ePRPbP//88zh+/Dhee+01HDx4EFOnTsX8+fMxbNiwxDYvv/wypk+fjm+//RYHDhzACy+8gJs3b6JPnz7+3BWFQuEJly4Zf2+3A2fOBkYWhUKRrfHrFNW///6Lxo0bJ/798ssvAwB69eqFWbNm4dy5c4nKDgCUL18eS5cuxbBhw/DZZ5+hVKlS+Prrr9GyZcvENp06dcKlS5cwZswYnD9/HjVr1sQff/yRyvFYoVAEgaJFgWTXdCqsVqCMcvJXKBT+J2B5cDISKg+OQuEfOG4c8OZY6W/jjr9WQ3v00UCJpFAoshCZNg+OQqHI5AwcCJQvLy01KbFYgLZtgUceCbxcCoUi26EUHIVC4TO0/PmBv/8BWrcGNC3pi/BwYPAQYN58aMnXKxQKhZ/IUGHiCoUi86MVLQr8shg8dQrYvh0ICQEefhha3rzBFi3dkJRh7j/9BNyIAe6uBPTtC61MmWCLplAoUqB8cJQPjkKh8ABevQo8+SSwcYOcgnPcOknggw+h3QmiUCgU/kP54CgUCoWv6dgR2LJZ/m6zyZB3u106VL/6CvjTT8GVT6FQOKEUHIVCoTCB//4LrP7LfRJDTQPGvQOHQZy7d4MvvQS2awv27Qv++SeyobFcoQgqygdHoVAozFiyRE5L2WyuvyeBPXuAyEhw4oeyZIWjvdUq63A1bgwuXgItV67Ayq5QZFOUBUehUCjMuH3bOSrMHVOnSuUGSFKGHD/XrQOe7esf+RQKRSqUgqNQKBRm1KgBJCQYt8mdW9bicofdDixYAJ444VvZFAqFS5SCo1AoFGZ06ADkzy+TFbpC14EnngQuXzbv6/fffSubQqFwiVJwFAqFwgQtPByYM1f606TM0myxANWrA8kKB7vvSAPi4vwjpEKhcEIpOAqFQuEBWosWwMZNwNNPJyk5xYoBY8YC69YDderIpIZGCAHUquV/YRUKhYqiUigUCk/R7r8fmDMXtNulJSZHjqTSE7lygV26AD/84DqcXNeBihWBRo0CK7RCkU1RFhyFQqHwEk3XoUVEpK6r9dHHUonRdef1ViuQKzcwd56qxaVQBAil4CgUCoWP0AoWBDZtBkaPAUqUkCvz5AEGPA/s2AGtRo3gCqhQZCNULSpVi0qhUPgJCgHNXeSVQqHwGlWLSqHIwtBuB69eBVU0ToZHKTcKRfBQV59CkUng5cvg8OFAoYJyyZ0L7NIZ3LMn2KIpMgg8ehQcMgQsWQIsWABs0hhcuFDVwVJkS9QUlZqiUmQCeOEC0KA+cOqUc4SOIy/L8hXQGjYMnoCKoMPVq4HHW8vSEI7yELouz5eevYCZM5VFSZHpUVNUCkVW47XhQGRk6vBjmw2Ijwe6dpGhy4psCW/cANq3k+dC8oKgjnPiu2+BmTODIptCESyUgqNQZHB49SowZ477StZCAGfOqBIA2ZkffgBiYuS54ApNAz79JLAyKRRBRik4CkVGZ/9+98qNA6sV2Ls3MPIoMh6bNrqvkwUAJLB/v7T0KBTZBKXgKBQZGN66BQx9ybyhEECOHP4XSJExsejSSmNGygSECkUWRik4CkVG5rXXgB07zNuRQJs2/pdHkTFp1szYymexAA8+CE0pwYpshFJwFIoMCqOigJkzpPJihMUCdOoErXz5wAimyHg89ZTMnOzOQiMEMPy1wMqkUAQZpeAoFBmVHTuA2FjzdlWqAF/P8L88igyLFhYGLF8BFCwop6oc01WOqudvvwOtffvgCahQBAFVTVyhyKh4WpTxhRehRUT4VxZFhkerWhU8dBj47jvg50XAzVvA/fcDzz8PrWbNYIunUAQclehPJfpTpAOePw/88Ye0tNSsCdSr57Nq0YyJAYoVBW7fNm64bz+0e+/1yZgKhUKRkVGJ/hQKP8PYWLB/f6B0KaBvH+DFF2Sm4Zo1fFY6QcudG3hugPvwX6sVaNZMKTcKhULhAqXgKBReQhLo2gX4ZmbqzML79wOPNAJPnPDNYOPHA02byd8dDqQOhefuu4HZP/hmHIVCochiKAVHofCWzZuBX35xnTXWbgdu3AA++MAnQ2nh4cCyZcBPC2Qo8N2VgAYNgGnTga3/QitSxHB73rgB7t4NHj6sCi4qFIpshfLBUT44Ci/hoEHAtK+M847kiABu3PCZP4638Pp1YNQoYNY3ST48d90FjHoDWu/eQZFJoVAo0kuG88GZMmUKypUrh/DwcNSrVw9btmxx2/bRRx+FpmmplscffzyxTe/evVN936pVq0DsikIBXL7kvuaPg9u3PAvx9gOMiQEaNZRKWHIH5ePHgb59wHfeCYpcCoVCEUj8ruDMmzcPL7/8MsaOHYvt27ejRo0aaNmyJS5evOiy/aJFi3Du3LnEZe/evdB1HR07dnRq16pVK6d2c+bM8feuKDIgtNnAH34AH2kEliwBVqsGvv++LFDpL0qVNq77AwD58gPh4f6TwYiPPwYOHEjtH+Qw1r45Fjx+PPByKRQKRQDxu4Lz8ccfo3///ujTpw+qVKmCL7/8EhEREZg5c6bL9gUKFECxYsUSl5UrVyIiIiKVghMWFubULn/+/P7eFUUGg3FxQOvHgB7dgX/+Ac6dA/bvA0b9D6hxn/8e4n36GE9P6TrQv39QpqdIAl9+kVq5SY7FAsxQiQEVCkXWxq8KTnx8PLZt24ZmzZolDWixoFmzZti4caNHfcyYMQOdO3dGzpw5ndavWbMGRYoUQeXKlfHCCy/gypUrbvuIi4tDdHS006LIAowdC/z1l/w9+ZSREMCFC8DTT/nHsTYhwbhoYd68wKuv+n5cT4iLk/tuBAkcOxoYeRQKhSJI+FXBuXz5Mux2O4oWLeq0vmjRojh//rzp9lu2bMHevXvRr18/p/WtWrXCd999h1WrVuH999/H2rVr8dhjj8Hu5q11/PjxyJs3b+JSunTptO+UIkPA27eBL75w7wtjswG7dgEbNvh+8JeHGdeHSkgAcuXy/bieEBoqFyMsFiBP3sDIo1AoFEEiQ4eJz5gxA9WrV0fdunWd1nfu3Blt2rRB9erV0a5dO/z222/YunUr1qxZ47KfkSNHIioqKnGJjIwMgPQKv7J/PxBjYonTdeDvv306LP/7D1izxtjJOCYGWLLEp+N6inan8GZiDSJX2GxA586BE0qhUCiCgF8VnEKFCkHXdVxIYTK/cOECihUrZrjtzZs3MXfuXDz77LOm41SoUAGFChXC0aOuze5hYWHIkyeP06LI5Hjq3+JrPxhPlGNdB/77z7fjesNrI6SC48oRWteBhg2Bxo0DL5dCoVAEEL8qOKGhoahduzZWrVqVuE4IgVWrVqF+/fqG2/7000+Ii4tD9+7dTcc5ffo0rly5guLFi6dbZkUmoWpVwMyx3G4HmjTx7biFCpm3EQIoXNi343qBVrWqrCztSAIYEpLkM9SiBbDk16Dl51FkTIQw9ptXKDIjfp+ievnllzF9+nR8++23OHDgAF544QXcvHkTffr0AQD07NkTI0eOTLXdjBkz0K5dOxQsWNBp/Y0bNzB8+HBs2rQJJ0+exKpVq9C2bVtUrFgRLVu29PfuKDIIWlgYMHiIewuN1Qo8WB9anTq+HbhyZeC++4wtQ6GhQPv2vh3XS7SGDYFTkcCin4HXRwLvjAN274G2dBm0vMr/RiFZtQpo1UrqwCEhQI0awMyZ5mmeFIpMAQPApEmTWKZMGYaGhrJu3brctGlT4nePPPIIe/Xq5dT+4MGDBMAVK1ak6uvWrVts0aIFCxcuzJCQEJYtW5b9+/fn+fPnPZYnKiqKABgVFZXmfVIEH5GQQNHhaQoNFFZd/rRocrm7IsXp0/4Z948/ksbRkHp5+23P+rHZKL75huKBOhQROSgKFqAYMIDiwAG/yK1QJGfqVBIgdV3+BEiLRf7s3p2024MtoUKRGm+e36pUg/LHydRQCOD334Hp04DDR4DChYBu3YFu3aClSC3g03F/+QV4fgBw8aL0dRECyJEDGPUGMHKk6RQQbTbgmY6yppVje0BanqxW4NffoDVt6jf5FdmbY8eASpWMLTU//AB07Ro4mRQKT/Dm+a0UHKXgKNIIExKAP/4ATpwAChYEnnwSmofnEz/5BHj1Fdfh5hYLkDs3cPqMX5U0RfZlxAjgo4/c54O0WIB69fyTZUGhSA/ePL8NYkkVCoURWkgI8OSTXm9HEvj8M/e5dIQAoqKAuXMBD6IIFQpv2b7dONm1EMCOHYGTJ6NBm01qd9euAXfdBa1atWCLpEgDGToPjkKRJbl2zTyM3GoFNm8OjDyKbEd4uHkGBbN8kVkVzpgBlC4FPPoI0L4dcF91sG5dcNu2YIum8BKl4CgUgcYoCV9yQkL8K4ci22JmeLRagx4IGBT4+edA/36py53s2A40agju2hUcwRRpQik4CkWA0fLkAeo8YFyR3GaT8bsKhR/o2hUoWtR1STWHZWfo0ICKFHQYFQW8PsL1l3Y7EB8vnZcc7e12OZWlyLAoBUehCAYjRrgPYdF1oGJFoHXrwMqkyDbkyiVz4DgSyuu61LctFiAsDPjpJ6BmzaCKGHh++kkWq3WH3Q6sXAHOnQM2bw6EhQKhIeD9NcFZs/xT2FeRLpSTsUIRBLSnnwbffQ8Y9T85H2CzJYWLlywJ/P4HNKOK5QqvuHBBlggrWVJG8yuAKlWAo0eBBQtkMGBCAvDAA0CfPjIoMNtx5oy8FhMS3LchpflL15NeUPbsAfr2AdatA2fMUFnCMxAqTFyFiSuCCPfvB6ZNA3bvBnLlBJ56GujUCZp6CvuEFSuAt95KCnfOkQPo3VuuC2I1DUUGhF9+CQx80X10oyfMmw+tY0ffCaVIhcqDY4JScBSK4GOzAZs2Adevyxm5e+7xbf8//AD06CF9SpLPBuo6ULasDFLzpLSYInvAy5eBkiWMLThG6DrQoAG0tet8K5jCCW+e38oHR6FQBJxvvgHKlJGFzZ98Erj3XqBBA8BXQSrR0UD//vJlPKWrk90uo/THjvXNWIqsgVaokMxE7gqjgAAHdnv2Th6UAVEKjkKhCCiffw707QucO+e8fssW4KGHpEtDepkzB4iNdf+93Q7MmgXcvp3+sRRZiNGjgfETpBd2csqWBerUMU8eFBbmP9kUXqMUHIVCETCuX3eKtHXCbpdKyeuve9/vtWvA1q3Avn3SYnPokHm6oVu3UitZiuyNpmnQRowAzp0HFiwEvp4BrF4DHDkK9O5jvHF2TR6UgVFRVAqFImB4Eon7++8y6qloUfP+Ll0CXntN+ts4XCfKlweqV/fMVzR3bs/kVmQvtJw5gaeeclrH7t2Bt98CrlxJXedC0+Ty0lCX/fHgQWDhQhnKd++9QMeO0CIikr6/cgWYMQP47Vd5gdStB7zwArQqVXy9a9kKpeAoFIqAcfq0Z5G4Z8+aKzhXrgD160t/muT51k6elPVPjbBYpM+PiqRSeIqWJw/45yqgRXPg/HnpVEzKJTxcRlBVreq0DW/dAnr3krH4jmRDCQnAkMHgjJnQOnQAt2wBWrWUjmMOh7EdO4CpU8BPPoU2ZEgQ9jZroBQchUIRMIoUcVZG3OGJ4jFhglRmUr5MJ7fcOFILpYRUTsYK79GqVQOPnwDmzwdWLJcnc916QO/e0AoUSL1Bzx7AL7/I3+32pJP1xg2gcydwya+yTUyM84nquEiGvgRWqwatSRO/7ldWRYWJqzBxhSJgXLwok+25U3IsFhlZtWaNcT92u0xGFxXlvo3FAlSqBBw8KK1GmibHzZED+PproEuXNO+GQmEK9+wBatznvoGuAxUqyGyL7h7DVivQvDm0pcv8I2QmxJvnt7LgKBSKgFGkCDByJPDOO6m/c5QKeO89836io42VG0AqNPXqSb+fhQvlNvfcA3TunP18b0hg3Tpg6VJZUqlWLaBjR5XV2a8sWJCUpdwVdjtw5IhxZJbNBvz5p3/kywYoBUehUASUt96S0bTvvScjmRyUKSP9LBs0MO8jZ07jZwcgnxv58gHVqsklu3L+vMw19O+/SZashARZTPOnn4CmTYMtYRYlKso8rBww94YXAiRVCYg0oMLEFQpFQNE0YNQo+eCdP19Wqli1Cjh2DPDU1SA0VAa5GIWC22xqGspmA5o3B3buTPrb4eAdFQU8/jiwd2/QxMuUcMcOsHcvsFAhMH8+sGULcOnS1MU2K1UydzgLCTFOIqjrwIMPKuUmjSgFR6FQBIUcuW1gx/mY278p+jQpizqWWvgUnyIKJnNPdxg1Sio4rp4Pui4f3nXr+ljoTMavv0oFxtVzVgg5S/LRR4GXK7PCefOAug8AP/4IXL0itcS//gKefAJ47TVnJadrV+PEf7oOdOsutXV3Cozd7jb0XGGOUnAUCkXAiUMcnsST6IROWIu1OIVT2IEdeBkv4z7ch1M4ZdrHffcBy5cnhZMnV3aefhqYN8+zGYKszMKF8jnqDptNHieFOTx9WkY82e3OGqMjMuqjiVKjvIOWLx8w9Qv5R0otXNeB0qWB998H5s2XJ29yc6TjnzZ0qDyZFWlCKTgKhSIgnDsnnYufeAKovmgslnMFAMCOpDhvgjiLs3gant3UGzUCTp0CFi+Wvj0TJ8qglHnzpJ9OdufGjdRh9Cm5fTupjNLKlcDhw4GRLdMxbZrrnAMOdB347FOnVVrv3sCvv0mvbgfh4UDfZ4FNm6EVLgztySeBnbuAZ/sBxYoBBQoAzZoDvy0FPvpYTU+lAxUmrsLEFQq/89NPQLdud6ZFQm4D54sBeaMNt9mCLXgADwRIwqzJ669Lpc+dkqNp0gKWO7cM6HFQv76sGVanTmDkNMOR/DE+HihVSrquBFyGFs3NI5py5IB285bLr3j6tMx3U7o0tJS1rhQeo6qJKzItJME//wQHDAC7dQXfeguMjAy2WIp0sGOHdPa12e48aKvvMVVudOhYi7WBETAL07+/sdEBkM7eR486r9uyReYj+vdf/8nmCSTw/fdA1apSsalQAShRAnjzTeNiqn7BrLgZAOju22ilSkG7916l3AQQpeAoMgy8ehV4+CGZCv2bmTLEZtw7QPly4CefBFs8RRr5+GNpKUi0FdPc5E743rB8DdcwC7PwMT7GL/gFCTCoF5FFuOsuYPx4+XtKNxCLJcnVI6Ud326X1pKhQ/0uoiFvvgn07CmTNTq4fFlOdbZuLWUMGC1aGjt1Wa1Ay5aBk0dhipqiUlNUGQY2bSKzkbmzp8//CVqHDoEVSpFu8ueXVcQTCYsFzhcF8hlbcf7Fv6iN2ukeX0DgTbyJD/AB4hAHHTrssKMQCmEapqE9sn4F6PnzgXffBXbvln8XLChD8n/6yXzbI0eAihX9K58r9u0zzl+kacDkycCLLwZGHl67BtxVQU4zubpHaRrw9z/Q6tcPjEDZFDVFpch0cOtWYPVqY2eBce+kzjWhyPCkKqwZFw5MGQTYXd9+rLCiHur5RLkBgDEYg3fwDuIgy5g7nJqv4AqextNYgRU+GSc58YjHdVyHgMn8UIB45hmZC+f8eVmc9Px5mVDRE/9VRw6dQDNtmvms0NSpgZEFALT8+YHf/wBy5XI2hzmKaE7/Wik3GQyl4CgyBkuWGN/NSPn6efZs4GRS+IQ6dVyEKr81FljWWv5uk19qdz5lUAYLsMAnY1/GZXyAD1x+RxAaNIzESJ+MBQB7sAdd0AU5kRP5kR8FURAjMAJXcdVnY6QVh0NxmTLyUitUyDyJLgCsDZIr1MGDxnnyyMBHfGn16gFHjwHvfwA0bgI89DAwdBhw6DC0vn0DK4zCFKXgKDIGt2979joZcM9CRXoZMsSFYS4hFGj3C9B+EbCyBcom3IW6qIvJmIxd2IVSKOWTsRdhEWxw/5QUENiO7TiCI27beMp6rEdd1MUCLEgc8zqu4yN8hHqoh0u4lO4xfMmTT3rW7pR5SiK/kCePcZJfIDipALSCBaG98gq0VaugrV8P7cMPod11V+AFUZiiFBxFmiEJmqUi95SaNV3MZaQgTx4ZSqHIVLRvD7zwgvw9uSXHatGhLW6Pr88uw8mQo9iETXgRLyIXfBdlchmXocMg090druBKusaxwYZO6IR4xKdSqOyw4wRO4DW8ZthHHOLwI35EN3RDB3TAeIzHBVxIl1xGeKJAaJpMtBsMOnQwjgCzWmXhVIXCHUrBUXgNt2wBO3QAwsOA0BDwnsrg5MmgmYJiRIcO0hvV3R1X14F+/aEZpT5XZEg0DZgyRSbfq1dP5jAJD5cWhPXrgWef9d/Y5VDO0IIDyKmx0iidrnGWYRnO4Zxbnxs77PgRP+I6rrv8/jAO427cjW7ohnmYh0VYhDfwBkqjNOZgTrpkc4emyUgkI8MpCbRq5ZfhTWnfXlZ/dzVzbbHI82jYsMDLpcg8qCgqFUXlFVy4EOjcSd4VHdYbxx2yUiWgYSNZwvmZZ6B5mSWMy5cDbZ6Ud9XkliGLBahRA1izFlru3L7ZEUW24BZuoTiKIxquI7Z06GiGZvgDf6RrnHfxLt7Em6bK1FZsRR04XxexiEUlVMJZnHXK6uzAAgs2YAPqoV66ZHTFunXAo4+69sXRdRltdfx48LJCnz0rFeHt250roRcqBPz8M/Dww8GRSxE8MlwU1ZQpU1CuXDmEh4ejXr162LJli9u2s2bNgqZpTkt4eLhTG5IYM2YMihcvjhw5cqBZs2Y4ciT9c+gKY3j1KtCju7QbJ1dASLkcOgTMnAF8+glQ9wGw9WPgjRse96+1bAls2ixrrzhe24oVA8a+Caxdp5QbhddEIAKTMAmAtNQkR4eOHMiBiZiY7nFyIIdHEVM5kCPVup/wEyIR6VK5AaSC4wsZHST3h2rUSEYrJc+Jo2lyKVBAlm4IZsmLEiVkssG1a4FXXwUGDwbmzAHOnFHKjcID6Gfmzp3L0NBQzpw5k/v27WP//v2ZL18+XrhwwWX7b775hnny5OG5c+cSl/Pnzzu1mTBhAvPmzctffvmFu3btYps2bVi+fHnevn3bI5mioqIIgFFRUenev+yE+PhjCotGocGzxapTtHkybWPZbBQ3blAI4eO9CDzXrpHTp5NvvUV+9RV55UqwJXJmK7dyIAeyHdtxAAdwAzdQMPMf9+Qs5EJWYiUi2acJm3AP9/ik/8M87NR3yo9GjeVYjnbaU237DJ+hhRbD7cMY5rVMsYzlj/yRL/AF9oh+ka2+ns9c+eMJkEWKkG+8kXQuHj9Ovv462aQJ2bo1+cUXZHR0eo+KQuF7vHl++13BqVu3LgcOHJj4t91uZ4kSJTh+/HiX7b/55hvmzZvXbX9CCBYrVowffvhh4rrr168zLCyMc+bM8UgmpeCkDdGrp1RaPFVwHMse3zxEMiOffEKGh5OaRoaEyJ+hoeS775LB1t0SmMAe7EEQtNLq9PMpPsU4xgVXQB8jKLiLu7iaq3mSJ33ef0d2pE7drZIygzNcbteWbQ2VGxC00OKV0rmN21iUReX/1G4l4kNkT5EliWq7CZC6TlaoQKZ4f1QoMjTePL/9OkUVHx+Pbdu2oVmzZonrLBYLmjVrho0bN7rd7saNGyhbtixKly6Ntm3bYt++fYnfnThxAufPn3fqM2/evKhXr55hnwofEBrmWSh3cnQdWLTIP/IkgzYb+PPPYKdOYPNm4AsvgNu2+X1cI776SjpBxsbKGbyEBPkzPh4YNQoIdvWJ0RiN2ZgNAIm+I46fv+AXvIyXgyabP9Cg4T7ch0fxKMqirM/7/wbfoCVkqn4rrNDvfDRoGIdx6AvXeVJqoqZhpJcFFlRDtVRTbO44h3Noiqa4jMsAAJvFBoTcCQAodh5Y3RgocAV2u0z6N2SIFzupUGQi/KrgXL58GXa7HUWLFnVaX7RoUZw/f97lNpUrV8bMmTOxePFizJ49G0IINGjQAKdPnwaAxO286TMuLg7R0dFOiyINPPGEceYtV1gswM2b/pHnDrxyBXiwHvD0U8CihcCqVcCMr4EH6oBDBgcl+3FCAjB6tHGbt9+W6X+CwQ3cwOf43G3NJwGBaZiW7vDp7ERO5MRv+A0bsREv4kV0QzeMxmicxEmMwii32/VDP8N+BQSGwHMt5Ct8hRjEuPbpsdqB/NeAZ2cAkP44CxfKzMbBJi4OuHrVfTJzhcJbMlyYeP369dGzZ0/UrFkTjzzyCBYtWoTChQvjq6++SnOf48ePR968eROX0qXTFxKabXn8cfdxm+5ISACqVPGfTADQpTOwa5f83XF3dChikycDn33m3/FdsH49cMkkr1tUFLBsWWDkSck6rMMt3DJsk4AErMKqAEmUNdCg4UE8iM/wGb7FtxiLsSiDMi7bEsRarMX3+B7t0R4aNFhhdeoLADqiI3qjt8cyzMd8tw7LsmMBdJqX+KfdDuzd63H3Pmf7dhlXEBEho7YKFwZefz1F/TKFIg34VcEpVKgQdF3HhQvOyaouXLiAYsWKedRHSEgI7r//fhw9ehQAErfzps+RI0ciKioqcYmMjPR2VxQANF0H/lgOVKggV6TKv59yA01mE+vY0W8ycfdu4M8/jV/7PvwADPBr4bVrnrUbOzbAFZHv4KjL5Kt23hCFKPyH/0wVrKyGDTYsxEK0R3vURm3kR348ikcxGqPxM34GQeRG7sTpqiqogmmYhjmY41GyQgc3YBK5aAGQy7lNyvRSBHERFw1z+6SXaERjzrozqNcwHkuWJCX1u3YNmDgRePBBadFRKNKKXxWc0NBQ1K5dG6tWJb0FCiGwatUq1PewKJndbseePXtQvHhxAED58uVRrFgxpz6jo6OxefNmt32GhYUhT548TosibWhlygB79gI/LQC6dJFJKgoWTJ2gz1GA7tvvoEVE+E+g5cvNFa1z54D9+/0ngws8zdy+b19gCwY6qImaHrW7H/f7bMxt2IYn8AQKoADKoRwKoACexbM4jdOp2kYhCqdx2i8KVjC4hmuoj/rogA5YgiXYju2IQhQAmQTQYXGJQQwqoAKiEY292Iv+6O+VcgMANVDDeJsEK7CzZuKf+fMDdevK3wliNmajOqqjKIqiBEqgDMrgQ3xomuPHUzZiIx7DY8jHfOjaqBRs5wvCNvEloODlxDZ2O3D0KPDGGz4ZUpFd8bfH89y5cxkWFsZZs2Zx//79fO6555gvX77E0O8ePXrw9ddfT2z/1ltvcfny5Tx27Bi3bdvGzp07Mzw8nPv27UtsM2HCBObLl4+LFy/m7t272bZtWxUmHkTElSsUr7xCkTePjJqyaBSPPUbx99/+H3vcOIoQq3kk17ZtfpclJTVqyKippERBrpcKFQIuGkmyJVu6jfqx0soGbOCzsVZzNUMZmmo8K60syqKJUU3/8B+2ZEtq1AiCOZmTgzmYl3jJZ7IEg9ZsbRhh5WnElScs5VLzER5ZnXj+vftu0rajOCoxrD1lmHtbtqWNtnQdh9/4G620pj4WCTpxtAJR6KLTtZEjBxkTk64hFVmMDBUmTpKTJk1imTJlGBoayrp163LTpk2J3z3yyCPs1atX4t9Dhw5NbFu0aFG2bt2a27dvd+pPCMHRo0ezaNGiDAsLY9OmTXno0CGP5VEKjn8Q8fEUZ89SBDCBhli+3Fy5yRlBEYS75ObNpMViruA4lJxx4wKbI+cUT7EIi7h8mBVgAR7hEZ+MY6ONJVnSba4XnTrbsR1/4S/Uqadqp1NneZbnBbrOnZXROciDHis2jpDwJmyS5vEEBfuxX2pFxXbn988GUbcKAuTzz5P2O6l5tnO7qWzf8bs0y3Wbt5mf+VOdb4mfeJ2Y9myqayMbZ5lQuCDDKTgZDaXgZB2E3U5xVwX3+XmsOsXgwUGT74knPFNwAKkMlSlDnjoVGNk2czPDGe5SwcnBHNzO7eadeMAyLjN9cLp96N35WGllX/b1iTyBZhInme5fyk8N1kjXmHba+SW/5N28O7HPsjfuZaNZM9i2neCQIeTOnc7bDOCAxDxI7hSvuqybZpl+5I/me347jMgd5XRdHD2arkOhyGJkmDw4CoW/0SwWYMFCIHduZ18cTZM+QPfXAt57L2jy9XWd+sQlQsjaO927+08eBwTRF30Rj3gQTPVdPOLRH/19MtYBHDD1I0kpQ0pssOEH/OC2plRGxgabxzlsAJlDpxIqpWtMCywYgAE4hEO4hEu4jMs4kXMf1vbqi19+1vDZZ7K8W3J2Y7ehn42AwH6k3ZdtP/YjBCHGjcLjgLL/AZCX8L33JsU0KBTeohQcRaZHq1kT2LkLGPKSjDENDQXurgRM/AhYuxZarlxBk+3JJ72LrLfZZAFEf4ftbsVW7MM+w+rX27ANu7Ar3WPlQi6fROLEIQ7/4b909xNoHsSDXu2/DTY8h+d8MrYGDYVQCAVR0FTJyoM8pm1yIu2FqXIip2fH4Ya8XklgzBjvc4sqFA6UgqPIEmhlykD76CNoFy5Ci42DdvAgtKFDoeVIXdwwkFitsmBhpTsv5GYBXw42bfKfTABwGId92s6IJ/EkLD661eRC8JTVtFIP9cwjm+6gQUM3dENTNE1cdw7n8B7eQ3d0xwAMwHIs90vodkd0NLSkWWFFF3RJc//t0d44P49dA/ZUgyWyHCwW4KOPgM6d0zycQqEUHIXC35QqBezeDfz6K9CypWfbeJNLMS3kgWepEjxtZ0RxFMdzeM6raZqUaNBQHdVRDuXSLU+g0aBhPuajIAoaKjmFUAjjMA7f4tvEY/UVvkIZlMFojMZczMVMzEQrtMIDeACXYJJJ0ku6oAvKoqxTskEHFlgQhjAMxuA0918ZldEBHdwfA514dO2bePstDadOAS9nrUohiiCgkUHIYx9koqOjkTdvXkRFRamcOIqAEhMDFC1qXKJB04CTJ4EyrhPg+oRbuIViKIYYxLhtkx/5cQ7nEIYwt208JQEJGIAB+AbfQIcOCyywww6C0KB5ZJFYiIV4Ck+lW5ZgcQEXMBmT8S2+xTVcQxmUQR/0QQM0QE7kRBVUcfJRWYZleByPu+zLCitqoRY2YVO6FMeUHMdxtEZrHMKhRFkSkIBCKIRf8AsewkPp6v8WbqErumIxFsMKKzRosMMOHTo+w2d4AS/4YjcUWRhvnt9KwVEKjiLAvPIK8OmnSZlbk6PrwFNPAfPn+1+O9/E+Xsfrbr//GB9jGIb5dMxDOIQ5mIMruIJyKIfSKI1O6GS4jQUWTMEUPI/nfSpLRuchPIRN2GSo/K3FWjRCI5+OKyCwHMuxAitggw0P4kF0QAePFd2buIkZmIGv8TXO4AyKoziexbPoh37IjdwAgB3YgfmYjyhEoSIqogd6oDAK+3Q/FFkTpeCYECwFhyBu4zZ06D55K1ZkTuLjgQ4d5JSV1Sodi3VdZm+tXx/44w9Z4cLfEMRojMYETABB6NBhhx0WWDD6zseX1gFXCAg8ikexARtS+Wdo0BCKUPyNv1EHdfwqR0bjGq6hAAoYtrHCiiEYgo/wUYCkMucKrqARGuEADgBAooUOAO7G3ViHdSiKokZdKBSGePP8Vj44AUBA4Et8iXtxL3IiJ8IRjkZohKVYGmzRFEEgNBT45RepyDz1lEyT/8QTsqrzunWBUW4AqUCMwzhEIhIf4kMMxVB8hI9wGqcxBmP8rtwA0jrzG35LnIqxwJLoo1EO5bABG7KdcgMAt2FeZl6DhljEBkAaz3GEpjvS2gBI/P04juNZPBtkCRXZCWXB8fPTRECgG7phLuZCg5Z40Tvelv0xDaBw5vRp4LvvgP/+AwoVkiW0qlULtlSKlBzGYSzDMsQhDrVQC03R1GfRV5kNG2wohmK4gitu22jQ8AW+wAAMCKBk7jmN0yiDMoaRWBo0HMVRVIBKbqNIG8qCk4GYgzmYi7kAnJOZOczxr+AVHMTBoMiW1SGBd94BypaV+TS++Qb44AOgenWZTC8YVbwV7qmEShiKoRiBEWiO5tlWuQHk9NOLeNHtMdCgIQIR6IquAZbMPVuwxTRhI0Fsgv9yIERFARMmABUrAjlzAuXLA+++KyuUK7If2fcOEiAmY7LhjdoCC6ZhWgAlyj589ZVUbISQ/i0JCdLfBQDmzAFeeilwsuzaJZWt//0PmDsXiEtWJPvSJWD1auCff5zXA8BVXMUETEBVVEUxFMPDeBjf43skIMHnMiYgAVdxNVU22wu4gK3YiuM47vMxFe55Ha+jLuqmun9YYYUFFvyAHxKddjMCnlY997Y6uqdcuADUqQOMGgUcOwbcuiWjEceMAWrXllnCFdkMP5aMyLAEshZVDuYwrb/yKB/1uxzZDZuNLF7cuPaTrpPnzvlXjuvXyZYtk8YLCZG/FyxILlhAdutGWq1JMuXPL6s72+3kUR5lCZZwKj7p+L0Jm/A2b/tExiM8wt7szVCGEgQjGMEX+SLXci2f4BNOdZTqsA5XcIVPxlWYc5M3+S7fZQmWSPz/t2VbbuIm840DzCVeYghDDO91OnWe5Vm/jP/kk87XUvLFaiVbtPDLsIoA483zW/ng+NkHpwAK4Brc20c1aHgMjymHYx+zdat03jVj+nSgXz//yEACjz4qLTP2FAlcLRZpWXL8TEn/54gtX92Pfdjnsj6QBRa8glfwAT5Il4y7sAuN0Ai3cMtpHB06BESqHDUWWEAQC7EQ7dE+XWMrPIcgbuImwhBmXs8piDyH5zADM1yGtuvQ0RVd8R2+8/m4//0np6PMnmZHjsjpK0XmRfngZCCewlMuM4M6IIh2aBc4gbIJN2+at7FYgBs3/CfD2rUyKiqlcgMkKTWulBsAmL53A3Zhl9vih47IvFu4lWb5CKIHeuAmbqYax5GEL+WDyvF3f/RHPJQTU6DQoCEXcmVo5QYAPsWneASPAEiainL8rI/6mIqpfhn333/NlRtAvvgosg9KwfEzwzAM2p1PSqywogRKZChHwaxC5cpSgTFCCP9GU82fn/aSC5aG/0ATxr4KMYhJV3XnrdiKPdhjXB/IBQRxBVfwG35L89iKrEkEIrASK/ELfkFrtEZN1EQrtMICLMBqrPZbLTFPrzN/l0BRZCyUguNnqqIqlmAJIhABDRp06IkWnRIogb/wV7oq9CpcU7y4rOTt7oZmsQDlygFNmnjf91ZsxTAMQy/0wpt4022F66goz94qXSHsmkfbpidXzW7sTvO2OnTldKxwiQ4dbdEWS7AEO7ADv+E3PI2nDS3Z6aVhQ5lfygirFXjkEe/7vn0b+Phj4O67ZULOvHmB55+X012KjI3SZwNAK7TCGZzB9/geW7AFVljRCq3QDu0QCpOrUpFmPv8c2LIFuHjReZpI14GQEGD2bHMrT3Ju4zY6ozOWYEnizZog3sbbeBNvpsr866ggnhb0dU1g140tK/mRH9WQdhNUBCLSvK0dduRDvjRvnxmg3Q5oGjRvThJFUChQQPrSffml62lfiwXo3RsoUsS7fm/eBJo1AzZvln+TQHQ0MGMG8P33wMqVQIMG6RZf4S/87vKcAQlkFJUiuJw5Q77wApkjR1IkU8eO5K5d3vfVjd2cIppSfqZxmlP7U6dIi8U4kstoqXKtPq20uhxLo8axHJuuY3OZl02jXtx9QhjCS7yUrvEzIkIIinnzKB58kMKiUegWisaPUvz2W7BFU5hw+zbZunXSdZ78Z/Pm5K1b3vc5fHhSHykXi4UsVoyMj/f9vijc483zWyk4imxBbKwMCb95M23bn+AJp3BpV5/SLE0bbU7bffihvBlqWuoQdV13rQDpOtm4MRlpP8NKrJSo0DjCbEGwIzsygQnpPi5DOMR0v1wpVyM4It1jZzSEEBTDhlJokIqNBrlYdfnzvfeCLaLCBLudXL6c7NyZfPhhslMn8vff5XpvuX2bzJPH/EVk4ULf74fCPd48v5XtNRvDqCjwxx/BL74A//wTdBfSkwUICwOKFQMi0jgr44lDbSQisQd7nNa9+qpMKnjPPUnrcuYEXnwR+PtvoEoVuU7Tkn526AAsWQKUspTADuzAdExHIzRCVVTFk3gSS7EU8zDPJz4NEzERPdADgHR6d3wAoCVaJjqFWmGFBg1WWDEcw/Ee3kv32BmOFStkmXfAeZ7DMb856n/g9u0BF0vhORYL0KKFvObWr5dJNVu18m4q2sGJE3I6yoiQEBnBpciYKB+cbAiFAN56C/jwAyA2Vj5VSaBMGfDrGdCaNQu2iBmOW7gFCyymEUeuiiR27gx06iSzqsbGytIRDkVr925g40Zg2zbpJNmypXR+dhCBCPS78/EHIQjBt/gWr+JVfI/vcREXUQql0Au9cDfuxi3cwiIswgmcQEEUxNN4OutWg546Jam8uyusVuCLqcD0rwMrlyIohIWZtyE9a6cIDirRX6BKN2cg+PrrwAfvp/7CYpHLmrXQlOecE7/jd7RGa8M2VlhxDudQCIUCJJXCAW024LffgCWLZY7+atWBZ5+FVry4532UKA6cP2/c6L77oO3clU5pFZkBUkZOHT9uHA25dassEaEIDCrRn8ItPHcO+Gii6y+FkFfyG6MCK1QmoAVaoDRKu60rZoUVz+AZpdwEAZ4+DdS4D3iqvQyNW7AAeOtNoGwZcMYMzzvy5FU8PDzNcioCw3/4D5/iU4zDOCzAgjQnpNQ0WTvOnXKj6zI8XSk3GRel4GQ35s83fh2x24E1a0BVmc4JHTrmYA7CEJaqWKAOHaVQCh/hoyBJl32h3Q60apmUlMRmS6quarMBz/UHV63yrLN27Y0zwVksQNt26ZZZ4R/iEIe+6IvyKI9X8ArewlvoiI4ogRJYhmVp6rNPH1m8E0g6NfQ7l3/16sDChT4QXOE3lIKT3bh0KekKNeLyZf/Lksl4CA9hK7aiEzolpszPgzx4CS9hK7aiGIoFWcLsAUlw40bw+++Bt98C9u937zdjsQATxnvW8aBB8immuUieaLEAuXMDzz6bdsEVfqUv+uJbfJtYYsRRfuQqrqIt2mIDNnjdp6YB48YB+/YBgwcDrVtLf7rFi+XUVOHCvt4LhS9RPjjZwAeHMTHAzz8DZ88CBw8A35kUu9M04MJFaIXUdIs74hGPm7iJPMiTyqKj8B/85x+gfz/g4EHvNrx1G5oH00v880+gfTvpx+NQdEggf35g2e/QPKngqgg4B3AAVVDF7fc6dDRBE6zAioDJdO2adOkqVEgpQr7Em+e3iqLK4nDKFGDEa/KGbRQh4kDXgSeeUMqNCaF3Pu64dg346Sfg3DlZNqJjR/mMVKQd/vsv0KwpkJDg/cbx8R75z2jNmoGRp4FvvwU2/CMtN42bAF27QsvlnzpKivQzD/OgQ3cb5WiHHSuxEldxFQVQwK+yHD4sp7V+/lnOlGoa0Ly5tAQ98IBfh1akQFlwsrAFhzNmyLddT9F1Gb+8aTO0e+/1n2BZGBL48ENgzBj5THXolKGhwNtvA8OHu54BUZjD5s2BNatdl2c3onRp4OR/0AJ84Ll+PTBpEvDP3/Laav04MHgwtKpVAypHduAlvIQv8AUSYKz8HsdxlEd5v8mxf78s3XDjRuryMLouUy2lpR6WIgkVRaUAExKAUf8zbpTyhv/QQ8A/G1IpN4yNBX/4AXzjDXDCBPDQIR9Lm3WYMgUYMQKIi5PKTkKC/BkXJ9dPnhxsCTMXJMHVq8F2bYFVf3qv3FgswKDBgVduxo8HHmkE/PKzNOOdPg3MnAHcXxNcsCCgsmQH7sJdpjmqwhHu9xxOL76YWrkBknzee/d2XStL4R+UBSeLWnD411/SnG/GBx8C1aoBFStCq1gxdT9LlgC9ewHXr8u0nY4IlY4dgW9mQUtrauAsSFycnI66ds19m/z55fNOJQczh0IALzwPTJ8uX3+9UW4cCk3TpsBvS6GZlZr2IYbXnqZJs96Ro9DKlAmYTFmdK7iCEijhNiTcCiv6oA+mYZrfZDh6VObNMWPVKqBJE7+JkeVRFhwFcPWqZ+2KFIHWqpVr5ebvv4GnnwKiouSKhISkh8zChUD37j4SNmuwerWxcgPI71evDow8mZ4pU6RyA3iu3Dhy8pcvD3z0ccCVGwDAZ5+6Dzcn5UvCV18FVKSsTkEUxCf4BACgwdlaZ4UVRVAEb+JNv8rgyFRgxuHDfhVDkQyl4GRVyns4z2zU7u235E9XRj4hgF9+Bnfv9l62LIqZcuPg+nW/ipEloBDAx17mFQoNBc5fAG7HQjt6DNrQoYFXbgBZZMzImd9uB9avC5w82YQX8SLmYR4qoVLiOius6IiO2IItKIESfh0/b17P2mXRSYMMiVJwsiq1asmpJ3dV5iwWoGJF4OGHXX7N69eBP018HqxWmThQAUAeTk+46y7/ypElOHUK+O8/77Z5+x1ohQpBC/b8nyf+Pmmp/qgw5Rk8gwM4gIM4iK3YivM4jx/xI0qipN/HrltXTlEbER4uc+koAkNArrIpU6agXLlyCA8PR7169bBlyxa3badPn46GDRsif/78yJ8/P5o1a5aqfe/evaFpmtPSqlUrf+9GpkLTNODLr6QSkjKxn6Pm1FfT3DtfmpXRlYMkTV8pUKcOULWqsU5ZpYpK7e4R3nhiFiwITJosQ9QyAi1amGdEbtY8cPJkMzRoqIzKqIM6KIiCARvXapWRkka88gqQL19AxFEgAArOvHnz8PLLL2Ps2LHYvn07atSogZYtW+LixYsu269ZswZdunTB6tWrsXHjRpQuXRotWrTAmTNnnNq1atUK586dS1zmzJnj713JdGgNGgBr18m4xeTUqwf8tRpa48buNy5aNKnktTvsds/NFhmMU6eAl18GihSRDr/33AN8+ilwO3UxcI/RNGDaNOmLnVKn1HW5fvp0FSaeEkZGgiNHgvfeA1aoAHbpLP9BxTzIDP3WW8CZs9AGDgx4pJRbXhrq3vKpaUBYONC/f0BFUgSGfv2AiRPltW6xJP20WOT9xkwBUvgY+pm6dety4MCBiX/b7XaWKFGC48eP92h7m83G3Llz89tvv01c16tXL7Zt2zbNMkVFRREAo6Ki0txHZkOcOEHxzz8Ux455vs2gQRRWnUKD6yUslOLyZT9K7R927CDz5SN1nZQORqSmyaVuXTImJn39b91KNmmS1Dcg/96yxSfiZynEmjUUOSOcz7MQq/zZpDGFRXN97ll1inJlKWy2YO9CKkRsLMUnH0vZHfvikDkiB8XKlcEWUeFnLl8mJ00ihw8nP/yQjIwMtkRZB2+e337NZBwfH49t27Zh5MiRiessFguaNWuGjRs3etTHrVu3kJCQgAIFnLNPrlmzBkWKFEH+/PnRpEkTjBs3DgULujZHxsXFIS4uLvHvaE+mX7IYWrlyQLly3m00ZgywbCkQGensNKlp8rn9+SRobo55RkUI4OmngZgY55dshx/1tm0yC+lnn6V9jDp1ZCjomTMyVXuxYkBJ/7sAZDp4/TrQ5kkgNtZ5Sspxrq1eLQ/e+fOpNxYCGPG6jOzzpLZaAOCuXcB77wKLFsmTK2dOGTeckADkyAG0egwYMACaOhmyPAULytJmiuDi1ymqy5cvw263o2hR5+RKRYsWxXlXNy0XjBgxAiVKlECzZs0S17Vq1QrfffcdVq1ahffffx9r167FY489Brsbs/D48eORN2/exKV06dJp36lshFa4MLBxE9Crl3PilurVgUU/Q3vuueAJl0b+/BM4ftz9DILdDsyYAdy8mf6xSpYEatdWyo1bvvtOZkVz529jsbhWbgCpkb74AlC4EDh0KOhpCJuf4Lp1wIP1kvLzA/Ik2rNHlkn59Tdob7/tsXLD06fBgwdBX5yI2QyCWId16Id+eByP4zk8hw3YACLbpXxT+NOUdObMGQLghg0bnNYPHz6cdevWNd1+/PjxzJ8/P3ft2mXY7tixYwTAP//80+X3sbGxjIqKSlwiIyOz3RRVehHR0RR79sipLiGCLU6aeecd0mp1nj5ytWzbFmxJsz6iw9Pup6C8Waw6xb33UFy7Fpz9SEigKFmCQre4l69nD8/6+vVXitq1krbNEU4xYADFxYt+3ousQSxj2ZZtCYJWWp1+PsNnGM/4YIuoSCfeTFH51YJTqFAh6LqOCxcuOK2/cOECipk4EE6cOBETJkzAihUrcN999xm2rVChAgoVKoSjR4+6/D4sLAx58uRxWhTeoeXODa1aNWjlymUcZ04DLl0CPvgA6NJFGqB++knOFISGuk7rk5KQEP/LmO3RNN94XNvtMsvau++mv6+08PvvwNmz7i1Rdjswdy5oknyTM2bIKbudO5NWxsYCM74G6j8IXr7sO5mzKK/gFfyKXwEANticfv6EnzASI91uq8h6+FXBCQ0NRe3atbFq1arEdUIIrFq1CvXr13e73QcffIB33nkHf/zxB+p4EFN7+vRpXLlyBcXNkhAosgXz5wOlSgEjR8rff/gBeOYZGSlVtap5UtwSJWQ4d1rZh314GS+jHdqhD/pgBVZAIPMWoOGOHeDzz4MPNQBbtQSnT0/z1AmFkP2tXQvUqOGZtukJdjvw9XRZgy3Q7NljHBYOSO362DG3X/PaNWDQQPlHSkXJbpc5gVQIjiFXcAXTMM3ttUYQUzAFUVCpLbIN/jYnzZ07l2FhYZw1axb379/P5557jvny5eP58+dJkj169ODrr7+e2H7ChAkMDQ3lggULeO7cucQl5k5oS0xMDF999VVu3LiRJ06c4J9//slatWrx7rvvZmxsrEcyZccoquzCpk2kxSIjolJOO1mtZLlyZOPGxtNUn3yStrEFBV/lqy7N4w3ZkNd53af76m+EEBSjRjlHNjmmlMqW8SoijyTFd99RlC/nPH0TYnU/tZOW5cwZPx0Ng/365BPP9mHvXvd9fP65+XRdrpwUHt7jsiM/8kfCg89iLg62qIp04M3z2+8KDklOmjSJZcqUYWhoKOvWrctNmzYlfvfII4+wV69eiX+XLVuWAFItY8eOJUneunWLLVq0YOHChRkSEsKyZcuyf//+iQqTJygFJ+vSoYO5j820aWStWvJ3R6i4Y5uBA8m0uhhN4iS3N1WdOp/gE77dWT8jZs829nupXMnjMG3x2Weu+3EoBskVBIcy1fox7xWcIFzT4tgxY+XEolFUKE9ht7vvY+BAitAQ8/37778A7lnmoimbeqTg/MSfgi2qIh148/xW1cSVP06mIj4e+OUXGaxy86accurfH6hQQaovYWFyNsAdui6nq777Dvj1V2DOHFmXtGJF2U/t2mmTyw47yqAMzuKsYbsDOIB7cE/aBgkgJIEa9wH79xtnFf71N2iPP27c15UrQMkS8p/nCl0H7r1X/h4bB9SrC7w4UCak7N0bmP29jKgykkPXgcaNoa1YabxjfoLdugLz5rmXccZMaH36uN9+1Cjgww+Ma1gBwJWr0PLnT4ekWZOLuIgSKAE7zIuyHsVR3AVVLyWz4s3z2695cBQKXxIZCTRrJqvx6rp0TVi2DHj/feCjj4CXXjJWbgD5/ImNlS4T7dvLxRfsxV5T5cYCC5ZhWaKCY4cdS7EUK7ACNthQF3XRGZ0RAZMM0oHg8mVg717jNiEhwMqVgImCgzlzjP8xdjtw8CBw+Qq0FDcsfvutTFw0dQqwdq1rJcmRl+mN0cZy+JPpXwM3bgK/Lknyx3EYDd8ZZ6jcAAA6dgTGv+f+e10HHm2slBs3rMRKj5SbOqijlJtshKr4psgUCAE89pjMYQMkOQrb7fIZ8vLL0iJTtapxYI6mSd9WXxMPN9aJZFhgQRxkwsljOIZ7cA/aoi2+wleYgRl4Fs+iJEpiDdb4XsAUcPdusH8/sGxZsEwZsFdPcOvWpAZmntgOzCwOAHDypLkTrs0GpIi2BGRNNa1tW2jLVwAXLwGOmnNWq1SwNE2WFJk7D1qjRp7J7Ae0iAhoixcDW7YCg4cA3bsDY8YCJ/+DNtI8ckerWRN4so37pIVCSA1e4RLHdWXGQAz0sySKjISaolJTVJmCP/6QCo47LBZZzbd3b+CFF9wH5+i6DEhJb/I9Eti8WRoVAKBOk2g8Xqco4rRYw+3+wB9oiIa4F/fiDM6keuu0wIIwhGEXduFu3J0+Id3J/v33QJ/e8qA5FBSrVSo1kyZDe/FFUAigfDlpNjPiu++hde8Obt8OfDEV2LJFZu1t0xbo1w9akSLg+PHAmNHmStPFS9AKFTKXf8cOmS34xg2p0XbuDC1XLo/2PSPDmzeBnj2BnxclWaWSExoKjJ8Abdiw4AiYgdmGbagD84jbkziJsigbAIkU/sKr57d/3YEyJsrJOPMxaBAZEmKeoO/yZbJt26TaUo71ui7/njkz/bKcPEnWrp3Ur8NRudDC/tSF7tbJuDzL0047v+bXhk6QVlo5kAPNBUkD4sAB44gfi0axdatsO3Gie+dZ3UJRqBDF7dsU48c7Owc7vs+bh2LTJorjx42dcK06RfNmftnfzIjo39/Y0dgXJ3EWpBZrUafr689KKx/n48EWUeEDMkyiP4XCV7jzT02JEMCCBcDkybIMECANFS1aAH/9BZi5QpgRHQ00agTs2iX/ttuTDBPXnvsAlsP3wJLisrLCighEYD7mwwILFmERNLifR7PBhvmYnz5B3TF1qjwg7tB1YNIk+ftLLwFt28rfk29jtQLh4dLb+6+/gP/dmYJJPl0lhLSwtG4NFC4MPPec67lDi0Wuf/uddO1WVoHXrgHffWvcaOwY0NMpxGzEbMxGXuSFNYVrqQ4dRVEUX+LLIEmmCBZKwVFkCmrXNnf3KFFCFrmzWoEXXwQOHZIOxfHx0hn50UfTL8c336SuPerAfiUfEh7YiMc2vo1SKAUAyIVc6I/+2IEdiSb0GMSY1sW5jdvpF9YVa9cYH0ibDVizGgCgWa3ATwuA72cDdesBuXPL4pcDBwG790B76CHgo4nGfiPXr8lMi5MmA0OGJPniOLYJCwMeeQTYvx+87ad9zkwsXWquzZ8+DST3l1IAAO7FvdiBHRiAAciJnACAfMiHl/EytmN74jWpyD4oBUeRKejaFciVy70DscUCDB6c2jgRFubbYtPff2+SfDcmNy68NAqRiIQddsQgBlMx1Sly4z7cl+otMzkWWFAF6UilbIQnpRGStdF0HVq3btA2bIAWFQ3t7Dlon3wCrUIF6aezdq2xb42mAav/gma1QvvkU+D0GeDV4Um1MOLigDVrgH7PAmXLgv/+m67d84bYWGD9elm0/MqVgA1rTFSUZ/+j6Gj/y5IJKYMymIzJiEEMbuM2ruIqPsAHKIIiwRZNEQSUgqPIFOTKJdOMWK3OATmOckZNm8pIKn/jyYPQ0cbi5vJ6Ak8k1sdxhYDAIAxKi3jmNG9hrPFZrUCLlqbd8OpVWenbLEaBdM4Nc/u2DPl2WCmESDbHdxVo0Ry8dMl0/PRgtwNvvimNUY0aAU2aAMWLy5plQS5KDlSq5Fn5iooV/S9LJkaDhnCEG04FK7I+SsFRZBoeewz4919ZQDNHDmmtuece6W+zdKkMMvE3d99trB/oepLvjyv2Yi+6oItb5UeDhvZoj67omk5J3fDCC0l+L64QQprC3MCFC8E6tYFCBYFSJaVCZGZxeOjhpN+nTpVWG1cJ8ex2aZn4+msPdiRtkNIP6+23pbHEQUKCnElr1Ei6DgWNpk2BMmXc+0npOtCkCbQKFQIrl0KRCVEKjiJTcd99MgvxrVvSXWT/fulvE6jq3wMGGM/I2O3Sn9YVBNEN3RCDGLcFAauiKuZjPnT4cF4tGVqFCsC8+VIxSa6pWa3yoTrrW2jVq7vclu+/D3Ts4FztOiHBvcXBYpE5anr2TFq3aKHxARRChkn7iQ0b3E8z2u3yfPoyiL6omsUCfDNL/m9SatJWq/SDmjQ5KLIp3EMC27YBixfLc8wo6bYicCgFR5Fp8cRVwde0awc8+aTrsTVNfteunettt2ALdmO3YcbVfdiHC0id8M6XaO3aAQcPAcNeBu6rAVSvLh2H9x+A1r27y2146BAw8nX5h7u7d3Krg65Lk9rPvzhn3/XEkdiPzsYzZxrnHBQC+Oorvw3vEVrjxsC69UDjxkkrdR1o1x7YvAWao6xFBoFHj4ITJoCjRoGzZqW50nxmZfVqoFo1oE4dee0/9BBw113AwoXBlkyhSjUoPIbXrgHffgts2ihvuE2bAZ07yyfG1atAnjzQIjJAmQE/ouvyxvX22zKa2jHNkS8fMGgQMGaM+ymsndhp2j9B7MVelEQ6MxGaoJUvD3zwgVw8Yfp0+X82isCKiJDOUmFhQPungIEDod2VIi1+rdrAxYvu+7Fa014QzANOnDCPxjPLbRgItHr1gBUrwYsXpVNXsWIZrkwDY2Olc/iPP8qT3mKRFr2XhoDTv4b2zDPBFtHvrFkjU1Ck1Pn/+w/o0AGYOxfo1CkooimgFByFh3DFCuCpp4Dbt5I8e+fMAQYPkld3XBxgsYDt2gOjR0PzRz2EDEJICPDOO8CoUcCBA3LdvffK1DBGhMOkgZftAsr+feaaQWwstOgY4zYDBwK//er+e5sNeP4F7+XzkCJFkuqYuaNAAb8N7zVakSJS6IxIv2flExxwTgh14wbQpTNYoAC0Zs2CJ5+fIaW7mhCpFRzHFOjgwfK2GagpdIUzaopKYQoPHwbatpHKjSMqxnEzu31bKjeAXL9kMVD/QXD9+uAJHCDCw4H775eLmXIDAC3R0tS3Jh/yoR7q+UhCH5Irl3GCQADI4YH1rkWLJCfmlFNaADB6DLQHH0ybjB7QrZuxcqPrstyHwhgeOiQtN66mK0n5AvTm2MALFkB275b1aI38bS5dAlasCJxMCmeUgqMw5/PPk6pammGzyRDg7t1UttUUFEMx9EEftxFUAPAqXs1QFhwKAa5cCcS6iXxyYLVKB2QTNE0DPv1MJg9MbuV74AFgwUJob73lA6nd07q19JFwNY2o69J6M2SIX0XIGixYYBxOKASwYQN47lzgZAowp0/7tp3C9ygFR2HOz4s8qxrtQAjpyLBypf9kyqRMwiS0QRsAsoSDBZbEpH8v4kWMhHnl6UDBEyeA6tWAli2A35e5b2ixyIfdy6941K+maTJ54LbtUnGKjYO2YSO0p57ykeTu0XWZ1bpdu6SZVofD+H33Af/8I/PjKEyIijK36AFZOiFh0aK+bafwPcoHR2GOYwrKG3Qd2LcPaNXK9/JkYsIRjkVYhM3YjNmYjYu4iNIojT7og2qoFmzxEuGNG0DjR4GzZ+UKV9Y4h9Nx7tzS+lLF++zLmp+SF9ntwPXrQM6cqacP8+SRBogTJ6QOnpAgDUgPPBCcyLxMyd13m7/0hIUBJf3rLB9MateW+RaPHXNv3M6XT90Cg4lScBTm1KolYyG9mXISQj5dFKnQoOHBO58My+zZ0grn7s6tadLk8eJAoHPnxOg5RkfLlNMnT8rCYM88A61U4GoAXbsGTJgATJsmFRyLBWjTBnjjjdTBWeXLu89ZBEgfi88/B5Yvl4fh0Ufl9FXduv7cg0xC587AsKEyIZUrrFagWzdouXIFVKxAomnAxx/LerSa5vpSef99z/zzFH4iANXNMxzelFtXkGLxYgoN3i26heL06WCLrkgj4tFHKCya8f+4Qnnnbb76iiIih9wuNESeA7qFYshgCpvN7zJfvkxWqkTqOikfN3LRdTIkhFy+3PO+vvuOtFhIqzWpH8fvn3+eNvlOniTfeot89lny9dfJPXvS1k9GQXz/vfxf6xbn8yLESlG6FMXZs8EWMSAsWkQWL+58zuXPT06bFmzJsibePL810hPP0axFdHQ08ubNi6ioKOTJkyfY4mR4SAIDX5QpXi0W8zSdFgvQ91lo06YFRsBswDZswwIsQAxiUBmV0R3dkR/+y4vCWvc7Zyx2RaFC0C7KulGcPx/o7Cbhh6YBw4ZBm/iRb4VMwYABwIwZrg2NmiYdiM+eNS/pcegQUKWK8Wm+ebPnlhwSGD0aeO895yoZNhvQo4eUObOGEXPZMlnY69871c1DQ2Vl3Hffg1a8eFBlCyQ2G/DXX9LoWaSIDBYMCwu2VFkTr57f/ta2MiLKguM9Qgj5xla7lnxLs2gUFcrLn1ZdvrWFWOV3XTpTxMYGW+QsQTSj2YqtCIJWWhnCEGrUGMYwzuIsv40renRP+n+6Wqw6RaOGsq0QFHdXNLb4hFgpLl70m7zR0WR4uPNbtKtl3jzzvl56ydlyk3KxWsnu3T2X7bPP3PelaeTAgWne7YAirl2j2L2b4tSp1N+dOkWxdy+Fuqcq/Iyy4JigLDjpgzYboGnQdB08c0ZmN3b4XHTt6raWkSIJgkhAAkIQYljx+HE8juVY7rK8gwYNy7AMreB7L0b+/TfQqKFxox/nQOvcGdy9G6hpkthR04Avv4LWv7/vhEzGnj3SJciMiAigfXvgpZekU7EratSQ/jdGlCrlWcbjhATpZ2tUIN1qBc6cybj5/BgZKct0zJ+f5Fj8wAPAO+OgtWgRXOEU2Q5vnt8qTFzhNZrVCu1ODgytZElo//sftGnToI0fr5QbEy7iIl7DayiIgghDGPIiL4ZgCCKR+mm5EzuxDMvc1q7SoOFtvO1zGXnhAvDWm+4baBrQ+nGgSBFw3TrAk1wnFotz+W4fkZAAHD4sq8x7wq1b0ge6Xj1Zl8oVnkRSeRpttWWLsXIDSJ3h99896y/QMDISqPuAs3IDyMqSj7UC580LnnAKhQlKwVEoAkQkIlELtfAxPsY1XAMAxCAGX+AL3I/7cQiHnNovwqLEHDmuEBDYiI24iIs+k5E3bwKPPiKL7LhC14EH6gKr/gSaNZVtn2pv3rHdLkOLfURCAjBunLSOVK4M9O3r+bY2m5wg6t8fOHgw9fctWxrnsLNapY+FJ3hSd1LTPGsXFEaMkLWwUoaEOxyUnusPuoukyoQQxGEcxi7sQgxMyo4oMjxKwVEoAsQADMAFXEhlkbHBhuu4jh7o4bT+Bm4YTl8lb+czvv9emkTcpQSw24GtW5xzI5lV/9Y0Of/SurVPRLTbgY4dZWFTM+uIERYL8MUXzuu2b5dOxkYZEYTwPNvxvfeaW3tIWY06o8Fr14AFP7nPd0MCMTEyqVAWYDZmo/KdT03URBEUwfN4HldxNdiiKdKIUnAUigBwEifxB/6ADa4fFnbYsRVbnSqOV0EVt+0d5EROlEAJ3wn67SzzNkZueymz21oscvl6BjQfhQotWAAsXmxWOYRAuRNA1b1ALtdv4jYbsHZt0t8//SQjo5YuNR5f12V+nKNHzWUtXVrqde4sQrouDVsNTdydgsKpU+bJ/EJCPDsQGZz38T56oAeOImlfYhGLr/E1GqABruN68IRTpBml4GRhGB0NbtsG7t2r6kIFmd3YDcLcn/8rfJX4e2d0RgQi3FpxdOjoi76+rV114aJnNcdcYbEAhQs7P80bNAD+XAXtiSfSJdatW8DXXwNNmwLPPmvSuP0iYFcN4EQFYG914FJhYFo/oFBqc4/1zgzg+fNJhTjNnukJCdKvvnZtYMcOc9knTwYKFUoay4Guy1Di2bMzaAblvHnN29jtnrXLwEQiEv/D/wAg1TVqhx1HcRQf4AOv+921Cxg5Enj+eeCDD+Q5llYc+TOnTZM5V80ydSju4PeYrgxIVg8TF9euUTz/PEWO8KQw3TKlKaZOpRAi2OJlS5ZzOeHhZwVXJG43j/NooYU6dac2OnVWZmVe4RWfyimaNJYh4N4mdnQsle6muHJFhhP7KNHjf/+RFSokhVUnhlkXvEQUPUdo9qR1z0+VR8imOR/VeJ04WkFukywB4KhRcoxOncxDzFMuui4TC3pySZ0+Tb7wApkjR1KoeZcu5L59PjlEfkPUrpU6kV/yxaJRnDwZbDHTxVt8K9X1lfJTgAVop92j/m7fJjt0SPo/h4QkJY38+GPvZLPbZXJIx3njWMqXJ1ev9n5fswLePL+VgpPFENHRFPdVd/+QGjEi2CL6BCHIDRvIvn3Jhg3J9u3Jn34i4+N9O85VXuUpnmIs05fX5yZvMhdzmSo3GjU2YAOnbVdzNZuwSWKb3MzNYRzGq7yaLplcIX78Me3KjUWjeOgh38ojyJo1U+SleWYusb1m0lGLLElMfoGY9zRh19wf3QSd+OSlREUpPJw8dYpcu9Z75Sb5smaN5/sTF0deuCAfgpkBsWyZ+/xGuoWiX79gi5huurO7qYIDgtd53bP+ukuFxt35Mnu257K9/rrrPiwWqTht3pzGnc7EKAXHhCyt4IwbZ/zGpYFi//5gi5ku7HaZ7j55+nxHev5atWTK/vTyF//iI3zEp0rFaI722IpzlqnT3F/jNUYyknGMS8+uGSLi4ykaP+r6HDIr3aCBwsf56detS3Fzf+PtOxYaS9LREnd+2j04stG5iJA45shB/vGHHKNZs7QrN5pGVq5MVq9OtmlDLlkiz0/DYyzIhATndVFR5CefSGWuVCmptH//ve8V9rQgZs+myJ1L/n8dJTgsGsWzz1LE+e9cDBSDOIhWWg3PGyutjKf5P+P48RRWRhfnS8WKnln9zp5NXXYkpQWxWTMfHIBMhlJwTMjSCk7pUsYPoBArxcsvB1vMdDFhgvFF36JF+vqfy7nUqLmdFkqrkmOjjdVZ3SMF5yAPpm8n0oG4eZNi0CDnKc4QK0XVKhT587m3Dt55oxdHjvhMljFjyFL6WVbGAeaostljBdHoM/DdMzxzRvYfE5M+603yaTPHw+ixx0hXibwPHZIWR8d0Q4kS5DvvkAcOyCkHTUvqy2EBePRR8tYtnx3ONCNu3KD45huKUaMoPvyQ4sQJ5+/j4ykWLqQYPVq+ZO3cGRxB08Df/NtUuenETh719cknxtYbx+LJ1KSnfZ0/n67dz3QoBceErKrgCLvds6mE9u2CLWqaiY8nCxUyv+j37k1b/1GMYgQjqNH1VIdOnS/xpTTLP53TTR/AYQxjDGPSPIavENeuUXTvllRQMTTE3D8nxEqRKyfFpk2p+xOCYt06iv79KO6vSVG4kOw3IgdFp2cotmxxbr9qFU+Ufiix7xc+s9ASbzAF5cFHExqjmHTdX7yYfgXH1fRByneIjRvJiIjUJSAsFrne3Zu6xUIOGeKXf6/PEOvXUxQvlmThcZwjLZpTXPX9NKqvERRsyqYup6kstDCMYdxJzxS2ceOMy3w4lhSnuktGjpTTUGZ9Zfaird6S4RScyZMns2zZsgwLC2PdunW52WTicP78+axcuTLDwsJYrVo1Ll261Ol7IQRHjx7NYsWKMTw8nE2bNuXhw4c9lierKjgk5cPF7AHUp3ewxUwz27Z59oD56KO09f8Vv3Kr3Dg+uZiLtymdKKIZzWmcxlf4CsdyLPfQ+G4TzWhGMMLwbfFZPps24X2M+PjjtPniWHWKkiUoYmIofviBYsQI+Wb/6CPup7qsulR25s6VYy9cSGHRaLckTZXd/286bTcJOluJVk77aLORBQr4XsmJiJD1sRxjlCxpPN1gtOTIkdRXRkPs30+RI4frKU2rTtGgAYXZnF0GIJrRbMM2iS8xIQwhCBZmYa7iKo/7WbTI/P9ptZJXPIgNmDLFeLoLkN9fupSOHc+EZCgFZ+7cuQwNDeXMmTO5b98+9u/fn/ny5eOFCxdctv/nn3+o6zo/+OAD7t+/n2+88QZDQkK4J5maOmHCBObNm5e//PILd+3axTZt2rB8+fK87aHnXpZWcJ57zrhIogaK5cuDLWaa2bzZ/Aai6+T776et/6EcmnhzM/oc53HO5VzmZE5q1BjCkMQ3wPZszxu84XaM2ZxNjRottDj1aaWVpVma53gujUfHd4i4OIqCBdLucKyBIlcyvw1P/Hcsmmx74gRFvrxO25wtBurxSPK38fZj14gEKzdwQ6p9feMNz5UPTfNs2gAgV915Lv76a/oVpnXrAnwCeIjo0ztL3W/2cz8ncALHciwXcIFHfjfJiY8nixZ1f45YrWS3bp71deUKGRpqfJ9r29b7fczsZCgFp27duhyYrFyu3W5niRIlOH78eJftn3nmGT7++ONO6+rVq8cBAwaQlNabYsWK8cMPP0z8/vr16wwLC+OcOXM8kilLKzhHjkiHQFdTCVadouHDmeKNyh2eVo32JrIlOW/wDVOHQxBcwAXU7nxSfqdTZzu2Mxznd/7OeqyXuE0Yw9iXfTOEckOS4q+/0qfcpHWxaBTt2qVa/+YYeOZE7O5zNT/bz/jV5b7GxEjn9JRKjuPvd94hf1kaz6HD4/nMM+Qjj3imlKxcKft/+23Ppi2MlvXrA/e/9xQhhLOfljuLcd++wRbVkJ3cyd7szYIsyNzMzcZszJ/5MwXTllJj5Uo5teTqfCpThjznxSU+caJ75SZXroyfZsAfZBgFJy4ujrqu8+eff3Za37NnT7Zp08blNqVLl+Ynn3zitG7MmDG87777SJLHjh0jAO7YscOpTaNGjTjEzWR1bGwso6KiEpfIyMgsq+CQpNiyhaJsmaQbjEPZeeJxiuvXgy1euhk40P0bt66T997rWZSCK3Zwh+GD0kILH+JDbMiGpqGlu7nbdLwzPMMDPMBoZqw5CLF4cXAUHDdL7a1pUGpsIG5EUOv9DQuVup3oWOyK6Gjp85A/f9K51LCR4Bs75/NBPpjYZ13W5Yf/zSMgDBWS0NCkaYj330/79BRA5sxJ3nBvEAwaIj7eM4W1w9PBFtUtC7iA1juf5C8oIDiQA9Os5GzdKqPqHJacnDnJwYNligBvmT6dLFbM+Zx46CFyt/ntJUvijYLj10zGly9fht1uR9GiRZ3WFy1aFOfdpHU8f/68YXvHT2/6HD9+PPLmzZu4lC5dOk37k1nQHngAOHYcWLoMGPsmMH4CsG8/tF9/g5bJs44CwIQJMouspjlngNV1IF8+mco/rZlha6ImHsNj0OE6tz5BDMEQrMd6t1W+AcAKKxbAvEZPCZTAPbgHuZE7cd0hHMJzeA55kAc6dFRGZXyOzxGHOIOefMy99wZuLA+IC0vLVhoweSDqHOyBDX+Fo4RBRYvcuYH33gMuXgTOngWuXQMeWvs/jKvxDLZgS2K7f/EvhpfphFI/vJ4qM7EDXQd69AAKFJB/P/64cW0rIywW4IUXgJw507a9P9FCQmS1UyMsFuCuioERyEvO4zy6oAvssDuVRHFc11MwBT/hp1TbxSIW3+N7DMEQvIpXsQqrcCqSGDdOZtkePlzu9uLFQFSUPJ+uXpXlPYoU8V7Ofv2AyEhg/Xrgt99kgdi//waqV0/zrmcf/KlpnTlzhgC4YYPzvPfw4cNZt25dl9uEhITwxx9/dFo3ZcoUFilShKT00QHAs2ed84R07NiRzzzzjMs+s5sFx1tEfDzF1asUKZNzZGBu3SI/+4ysUkU6YRYvLpNiGb2le0o0o9mKrQhKv5gQhlCjxnCG8zt+x+M8bmo9CGEIh3Ko12Ov4zrmYA6nN0rHVFhDNuQtpo4ZvsEbvMZraX7bdId49BH3UVOe+NT4cOn7NWiN98Bqk9xHR2iE0NiZnWmnnQd4gGM5lkM4hJ/yU16ie+/MdVxnOlaxTmucfC0cYd5166Z2Cn7sMWMrTtGi8qejP7OQ84yCR3m3vAgACSTjOC6VH1zyj07dZdLNAiyQeI1bxZ3rdOd9tJSJpNWaNB351FOZJ6FjZkJNUZmQlX1wvEEcPkzRuzdFWKi8EeXKKfOfnE2dZC6rc+KEVJAaNSKbNCE/+ID889q/HMmRHMRBnMzJvMZrJMlbvGUYCeVQSr7gF17JEMtYFmZhtzddCy38H/+X2P43/saH+XDi92VZlh/xI68dI90hDh6kKJA/tROpVafIGUHRq1fAFJx/a3k5PZXi04ANEh9aIQyhhRaGMISTOMnlvndiJ0NfLCutbBvXkRMnSiW7cGGyTh1y2jTXD7Vr1+S0gsPRVNOSHoSjRskpqOnTyYcfluUfHntMRuTYbD75Vzr/Xy9coFixQvpZpXPuS8TEUNSs4V4RfvNNH0ntex7n46bnTQhDEtsf4AGGM9z19RlvJQ5WIkLiEpVWi4Xs2TOIO5hFyTAKDimdjAcNGpT4t91uZ8mSJQ2djJ944gmndfXr10/lZDxx4sTE76OiopSTsZeIHTso8uRO/fAKsVKUKO6T+jLi+HGKYcNkf/nyUjz0kAwbvnWLYutWik2bKGKCn+9l9mz5xpz8DdtiIfPkIf/+2/U2L/JFQx+cHMzhcWp3B3M4x/SGm5/5Gcc4fspPEx/YKRWrJ/gEE+gba5w4doyiZw8Z3eRQbjp0oLiTaEh07Rowa86E1+68WSck22OR4mcaPjp1FmVRvsbXEjNI38W7TLcrz/JeHUu7nVyxQib7e+opcvhw8mAA8zmKK1counVzvuZz56IYOZIiHSmTRVQUxZAhzikqKt5FMXOmD6X3Pe3YzjQlRDjDE9s/x+fMAxCemetkmdM0WQ5E4TsylIIzd+5choWFcdasWdy/fz+fe+455suXj+fvpF/s0aMHX3/99cT2//zzD61WKydOnMgDBw5w7NixLsPE8+XLx8WLF3P37t1s27atChP3AiGEzErr7q0rxErxxOPmHRmN8c8/8oaXfAyHKdvxsNQgLQFDhqT7TTKt/Puv+5BOi4XMndt1nolLvMS7eFcqBcNCCzVq/I7feS3Lq3zVoxD1VVxleGPWqPErfuWDo5OEuHmT4r//UimkYvXqgE5V/d4SbP4HGBInp6warQErHza2pnmj6BRmYR7kQVZjNdP2VVjF5bGKiZGWnBdfJIcNI//6K+1O775CxMRQVKvq+pq3aBSdOqW7EK+4eZNizx4ZyRnsHfaAL/iF4XVkpZVt2TaxfR7mMT4jbBZifodUCs6UKcHbx6xIhlJwSHLSpEksU6YMQ0NDWbduXW5KluX0kUceYa9evZzaz58/n5UqVWJoaCirVq3qNtFf0aJFGRYWxqZNm/LQoUMey5PtFZx//jF/mFg0iv/+S1v/t29TFCpkPjeffMrjoQYUQXA26N7dOITXYpHTVa64xEt8kS8yB3Mk3uTqsz7/4B9pkmUQB3lU9O8FvmDYTqPG6qyejqPiOUIIigb101eBPA2L3QLarRaKgS+ytb2VTxQch5JTgzU4mqNN/xdVWTVVSY3ffpPhu5omQ4Ud51bt2sFNqS8+/ND8ekyWW0Hs3UsxYwbFt99SREYGT3A/Es1oFmRBt/9njRrXcm1ie09ePvB7y1RRnY6MJrt2kYMGkU2bSgvejz/K4qsK78hwCk5GI9srOF9+6dmD5Pff09b/d995/9CyaBRffunjPTWnYEHzMN3GjY37uMmbPMIjPM+0PcH2ci9bsIXpzVOjxnt5b6IDtNHHSmuaZEkL4vJlOf0YKAUnRzjFV19R3Eko8iW/9JmC4/gs4RLmZm5TJ9TczJ2Yxv/ff5P8a1ydR2XKBM/pVNxd0fiYhlgpevaUVrpGjZy/0y0UXbtQZNR0yungX/7LAizgZMnRqdNCSyoraFVWNZ7SircSHw1L9X9fulT6WTn8r5I7k99zj28CI7ITGSZMXJFBiYjwrF2OHGnrf9MmICTE++2mfZW28dKBzWbeJiHB+PsIRKAiKqIoiho3dME+7EN91McqrDJtSxAjMRK5kAsWGF+64Qj3Wpa0ohUsKGNY162Xcfrp6swkvl/XgecGQHvuOWjFigEACqBA+sZMKQI0nMRJLMdy5IX7tAp22HELt9AbvQEA77+f9FhzxalTwP33AzExPhXXM86cMf7eZgOOHAYaPgxs3OD8nRDA/PnAk0+AXsa78+JF8MMPwX79wJdfBjduBN0doCBQG7VxDMfwCT5BUzTFw3gYQzE0MVVDcgZioHFnITZgev/EPy0WoHRp4Px54N135TrH/UYI+fPoUaBNG/fnjCKd+F/fynhkewvOhQvm6dULFkjzlJEYPNjZz8bTJV9e3+6oB7RqZRy+q+vkiBH+G78FW5hOhTi+f5MyImU2Z5tab3oyOOEb4vPP0+d0/Pzz0lfEXW2j/PlSOcD3Z3+Ppva8+XxJaU3cyq0etd9i/9ejwoiaRnbtGoT/S+lSxsfdqlNUdXPcky9Llng+5tSpFCEhss8Qa9I9p1lTikx4741jHJuxWWqrnu2OVWfUO4n/Z6tVZlxft05GxZnVlFq71nx8hURZcBSGaEWKAAMGGL8tvz4SWliasqsBzZubmz1ckT9/2sZLB0OGmCdhGzDA8/72Yi8GYADKoRzKoAx6oie2YqvLtpGIxAqsMEwYCACP4TEcwAGMxVgAQAd0QAVUgBWpM81pdz6v4BXPhfYlL7wgM9u5ysJoRni4NIP8/Y/sA5CvwY5t77oLWLMWWtmyTpvdxm0fCS/RoKEFWgAA/sN/Hm3zGT9HwiMrAU0YtiOBefNk8reA0ruP8f/AbgcuX0oyLbhC14HvvvVoOC5eDAx8EbAlyD5ttiTzxZo1QOdOnsueQQhFKH7Db3gLb6EIkjL2VRHV0erbOcj12RsA5GF66ilg82agXDng8GFjC43VCvzxh5+Fz64EQOHKcGR3Cw55p5hit65J8+9WPclR9NVX0xUFIWw2OefvjeOpbqEYO9Z3O+gFr7/unFwtea6Sb7/1vJ8f+AMttDiFkjp+d5VvZT3Xm1oGdOp8n6krh57gCVZipcQxrLRSo8aczMnf+Ft6Dke6EQkJFFOmUNxTOen8at9ORu65sxxadYpkNetIUhw6RPHppxQffCBztrg5Jz/kh4a+Mt58dOrswA6Jff/O373r4WQZovEqU0uOhxktfIa4cEGma3B1/B016pKHebtb8uejCA+TubOaNKb41XV9L1HrfnNr0M6dgT0IPsRGGyMZyQu8kJhgMyGBvHjR2c/q2DFzq15ICPnqq0HakUyIcjI2QSk4SYidO6VC06snxahRFEeO+KbfI0ekWdyimU9ZWHWKokUoghhmsmwZ2bKlDAvPn19OI2zd6vn2h3nYdJpkMzc7bbOP+0wfmBo1Tud0l2PaaOOv/JXP83n2YR9O5VRGMWOd0yIhIVExESdPUpQp7fzgc/z+UIM0pwq4xEsMZWi6FRsQfJgPOx3DW7xlHh6c/GOzEHEhRL2Nhg+1H37wyeH1CnHsGMWD9ZyvPYtG8cwzFNHRFJUqmSs4ya9lxwvMmDHO45w+bd5PiDVoLzSBJD7es0CGefOCLWnmQSk4JigFJzCImBgZ7dK8mbyx9uhO8UCdpAeb4wZZvRqFF2H+3nLrFrlhg0za569/+TAOM818243dnLYRFKzGaoaRGaEM5WVe9o/QQUBcvSotMlWqSKW2Xl0ZjpzOFAGzOds0aVvKj4UWFmERNmVTdmEXLuVS2pg6dfB4jvdOXUrQiRXNDP1wjh1L1+6mC7F9u4yk/PprJ38mj0LJ3S3JQ8wPHzZvHxpCMXx4MHY/4IwZY5xrq0gRFS7uDUrBMUEpOMFF7N5N8fHH8kG3fr3fkoLFx5P/+5/MSOy4oYSHy1wUN2/6dqyarGn64CvBEqm2+5W/JtaacrXNKI7yraBZEBttfIpPuVUsB3IgrbTSQgt16omK6AN8wLAelQNBweEcTo1aYgixR4pOkfOpHmhWK/l4+nJo+g0RHU1R5V7vcxqFWCk6JE3riVu3PJvumjUriHsbOGJjycaNk2qVJT8XcuYk//kn2BJmLpSCY4JScLI+djvZrp3r6AVdlzV/1qyR9X42b05/ptlarGX6wCvJki63ncM5zMd8BOVUiUaNIQzhKI6infb0CZbFiWY0n+Ezhlaa3MzNIzzC9/geu7M7n+fzXMmVXh/bEzzBd/gO7+f9nik5Vfcmnn+Oh1vlyuSFC346GD5AXL4ssxonV3I8iYorVcq5n8GDjQu15s1D4eu3jAxMXBw5dSpZrRoZFkYWKEAOHEj6yCMgW+HN81sjs18EfnR0NPLmzYuoqCjkyZMn2OIo/MCyZUmBOJ5w993A5MlAixZpG+91vI6JmOg2IsoKK7qjO77BNy6/j0UslmAJTuAECqIg2qM9CqJg2oTJJuzCLjRHc1zCJcN2GjR8gS8wAF6EwxkwFVMxCINAuL916tTx3oyLmDOlAE6fBooXB559FujbF8idG7h6FZg1C/j7bxls1rgx0LMnEIjbEU+fBqZPB3bsAMJCgcefADp1gpYs7xXPnwe2b5chPq8NB3bvNu60QgVoR48lbX/9OvDwQ8ChQ85hiroud3jhImhPPunjPVNkB7x6fvtd3cqAKAtO1qddO+P8Nq78IiwW8o+0VVngCZ5gCEPcTjVp1LiDO1xua89CRpojPMIJnMCRHMlv+A1v0HPH4QQmcDd3czu3m24XwxgWZmGP8t9YaWVf9k3vriVyhVcMnZpTRmKlZMUKMiIiyarjWPLm9f90hZg1yzli0uFzU6okxYEDrrcZM8Z42sqqUwwblnq7a9coRoyQkVeOsZ58gmLjRv/upCJLo6aoTFAKTtanenXPlZvkSk6lSmmfrvqFvzCEIU4PXYfvxyw6+xtcuUKOHk0WLSrHzpNHztMPHy5r1Ny65YODEEBiGcvu7J74gHfU7cnN3JxH4xARO+38mB+zGIslHreczMmX+JJbRcesUGJKBWcAB/h0fz/hJ26Vm7zMy0N07TR/9Kj0A3M1deoo7nqnAoXPEevXu59usuoy6tFFLQlx5oz0qXHlgGzRZNj40aPux7XZKC5dylZTUtkVERNDMXEiReXKslJ9+XIUb71Fcdl3gRJKwTFBKTjGiOhoWWRvwgT5xpcJj1OTJu4jF8yWZLVgveY4j3M4h/M+3sdqrMbBHMwDdH4zPneOrFDB2MKUN29wQonTSjd2c+mX4nCgXsEVbrd9ns+7VRYe5IO8zdQP3bZs61XU1CIuStz2Nm/zAi8wju5DV86fl5F3+/Y5K7x22rmES/gEn2AxFktlyXmUj3I/97vtd9gw4/+7xUK+/bbJwU4jos2T5hnMv/vO9bZr11Lkye2c9kG3UETkSHPNOkXWQly+LLOQp1SidYtUntNYvDklSsExQSk47hGTJlHkjEiKjtAgb2Kffhps0bzim2/SptwA5MKF/pWtQwfPps80jfQiM37QOMRDhsqFhRbWZ32X227mZsNtNWr8nJ+n2s6TgqMOJakCKzCBCdzHfezMzolRVDmYg8/zeZ7m6cR+jx8n27Z1Vo4rVyZ/+omMZzzbsV1ivw75QBkht4EbTI9V+fLm//fatdP8r3CLEMJcudEtFJ07ue/j6lUZ/fjkExRPPE7x/vsUFy/6XlhFpkR06eJ+KjPEStGooU/GUQqOCUrBcY346ivjG+DUqcEW0WNu3yarVk2q3uvN8vff/pPrzBkXlqWw20SH+cTLE4ke3xK5oxIVnCpV0h/h5W/e5bse+cKcYeqyyf3YzzB/kEaNVVgl1XZjOMajMUuzNI/wCDdzMyMYkWosK60sxmI8yZM8cYIsVCi18umYTnp8+xturUZWWtmADUyPVcmS5udf1aq++K84I+x282goi0bxVHvfD67wOQkJ5PLl8kXu999lSoxgIs6d8yy9wJ496R5L1aJSeA3j44E3Rhk3Gv0GGBcXGIHSSXg48NdfwCOPyL81TZY1MkLTZO2Y+vX9J9eePSnK/XT/HjhXHPjpGeD9EcC3vYALRYHX3gdJ7N8PHDjgP3l8QRSiTKubA0A0olOtO4zDsMF9SXeCOIETqdb3R39oMK48/jyex0EcxF24C93RHXGISzWWDTZcwiUMwiD873/A9eupa5ORAMJisbT8JBCuI6dssGEDNmA7thvK9MADMjDJHVYrULeuYRdpQrNYgOrVjS8CTQNq1fb94AqfMn8+UKoU0LIl0KcP8NhjQMmSwA8/BFGoHTvMi/oBskBXAFEKjkKydi1w+bJxm6tXpdaQSShSBPjzT6lUfPYZ8PHHwJtvGm/z8cfmilB6CA1N9sfTC4DvewJ5r8u/rXZAA5AjFnj/deDViQDkYc/I3I27DZUUQBYqLImSqdYXREHoMC7EmRd5U60rhVKYhVmwwOJUdNTRV3/0x1RMRQQisBZrcQRH3Ibw22HHUi7F/A2nE+tBpqL6HiBflKGcOnSsxmrDNoMHw/0YkN918lcdyiEvmRfTfPZZPw2u8AULF8rz48IF5/WXLgHduwM//hgcuQy19rS08xXpthdlQtQUVWrE3LmeZS2dPTtpm5gYis8+o6henaJgAYqqVeUcfXR0EPfEnG+/lVMRyacFiheXfhb+5tYt6UAMzU4cK0/YDSZYYnISuWIYGel/udJDNKMZwQi3+2Gllb3Z2+W28znf1IdmON2n9N/MzezETszJnAxlKBuwAedybmIBRJKczMmeOSQ3Xel+6ugBY18hx+dpPs0YxhgeL0dxV1dO8Jomp1U/+SRN/wpDhN1O0bWLc3i4wz9Ct1AEugKowitsNrJ0aePpzWLF5PRVoBHR0dJX02wK9NSpdI+lfHBMUApOasTmzZ4pOHcSdYhLl2R16ORRFY7fK1eiyMjpWinnrJctI2fOlHPZgbwpvPQSiTpbPHpg3vfu3MAJlg6+5/fUqKWKpNKpsyRLuvS/IaXjbg3WcOmHo1NnfuZnJNOn4c3kTI+ONer/4/7hkeMmEZXbo35KsiQP8qChTIsWpVayUy4//piu3XaJsNtlZKSj2neOcIqOHSk2bzbfWBFU1q71zIdwhfuARb8iXn3VfS0zq07RpbNPxlEKjglKwUmNEELWoXF3guoWqbg4KkM//ZSxx/wTTwR5j4LPzZvkd9+Rb75JfvaZdDDet4/Ml49Eq9/NH5V2jaPPTQn2bpCU9Ziu87rLkG0Hy7iMdVgnUf5QhrIXe7lVbhxc5EU2YZNEpcah7NzFu7iLu9It+3meN3RkBsEiLMIGj8Qbpxb4YDgtwrxEg06dZVmW8XTv+Xn0qHkEXXpyMnmCv2rAKfzD/PmeKThuIv39joiLo2jfzjkC1/GMePghn1n2lYJjglJwXCPWrZNVflMqLrpFnrCrV8t2p055FpFx/HhQ9yeYzJ4tk7YBZEiInI6wWGRiP4uFROUDHlkDlnJpUPcjlrF8n++zFEslytSMzbiKq9xuc4qnuJd7GUXvrq+d3MkJnMBxHMcVXOG2VtQ1XuNaruXf/Ju36FlGxIEcaDhN9Tk/55o1MoLKVRI+TSP7DbrNZmzmmTWI4E90P+c5caJneZoOGhuCFAHkNm/zBE/wCq8EZfx//vFMwVnl/tL0O0IIij//lFOhDerLF+HFiylsNp+NoRQcE5SC4x6xYYPUtpMrKw3qUySLnRYLFng2neUPG3smYMkSD0PSNzxIJLgOddaosRiLMYFBmFC/Qyxj2YRNXE47adT4Db8JqDzRjOZzfI5hDEuUJQ/zcBRHGVpLSDKOcezBHgSlT5CV1sT9GMVRiT47S5aQBQvK/4/VKhUbXZeFERMSZDkJT6a8rLSyH/u5leettzxLYbB9u08PoSINXORFDuRAJz+zR/ko13CNU7tLl2SSxgoVpJW2Rg1ZYNNFcug0YbfLvl0p4A4lvFQp6auTlVEKjglKwTFHHDtG8fffLlOwi59/9kzBmWecoj8rIoSsGOzuJuS01NlC3ApPpeRowkKNGn/mz0Hdl4mcaFg1O4QhPM/zAZHlFm/xAT7gMveNRo0d2MHJsdgde7iHr/N1Pstn+Rbf4kmeTNUmLo5csIB85RWyaVOyYUOyTRvpr3XzplS0PFFw3DlWk7J/s/MjNJS8di09R02RXi7yIsuzfKrzTqdOCy1cwAUkyWPHyBIlnK1yjhpjdeuSvoq7WLo0qd+Uyo2mkT//7JtxMjJKwTFBKTjpQ1y8aJ4V1apT+KuoTgbm8GEPrTeOpfZWYm1Dp5tnuajq/INprPrpJVGM4lRO5bN8li/yRS7lUtpoo6BgWZY1fIhbaOF4jve5TAd5kIu5mKu5OtEyM5VTTSOhVnKlz2T49NMk643jAQLIKJbDRwQrsqKhPBo1fsEv3PYfF0cWLux+mspqJXu7148UAaIf+7lNKKlRY27m5k3eZO3a7i1yuk4+95zvZFq6lLzrLucxypcnFy/23RgZGaXgmKAUnPQj+vcz9pjv2TPYIgaFzZu9VHAcS/ljRKM1DK+1jzE3AuP8+St/ZU7mpEYtcdoGBO/lvablFxwKThd28Zk8e7iHD/EhpzEKszAncRLv432GCoWVVnamb6I0li51/3+yWuXD5HPbFLfyWGhhbuZmNI1f21eskP5ZKTMn67oc47yfjGMiNpYiK5Ww9xMxjGE4w02vgzHHZple32FhvrXGCUFu3CjLymzYIKevsgsqk7HC/3z6WVKaYF13/lm/PjBlSnDkCjJly6YxUeCJCsC6RxC/swqGDdVw+7bPRXNiJ3aiPdrjFm6BIGx3PgBwBEfwOB43zRRsgQU5kMMn8hzGYTyEh7AJm5zWX8IlDMZgHMIhEHS7vQ02HMdxn8jy/vtJp3KqcWzAiRNAscUD8AyeAQCnLM5WWBGCECzCIuRGbsNxmjcHNmwAHn886ZzJlQsYNAjYsgUoWtQnuwMA4M2b4LvvgqVKAjnCgRzhYNeu4K5dvhski3EapxGLWMM2IQjBP1cPmF7zcXHA7t2+k03TgAcfBJ56St5u/ZmcNDMT4LSCiqyCFhEBrlgJ/Por8M1MIDJS5g/v1Rto2xZaoDNWBpnISPmw0jSgWTNg1SrXmcs1DQgJAeLjXfcjBDBzJnDunDy0mrGO4ZaEBOCXX2Rm08uXgYoVgX79gAYNZJ8f4kMAcKk02GDDURxFLdTCLuxymwHYBhvaoV3aBEzBG3gDN3HT7VhxMC4RokFDYRROtxyxscC6dcZtrFZg+TIdPz71I9qgDSZjMvZgD8IRjg7ogJfwEu7BPR6NV6cOsHgxcPMmcOMGUKCAPD98CW/cABo/KtPpOzIZJyQAC34CFi0Ef/0NWvPmvh00C5ALuUzbCAjksOUC3eveiWSzW2LGIAAWpQyHmqJS+IorV8inn3Z2+rNYkkLDk5upLRbZbv58sk8fc0fktWvTJtOlS2TNmkljOqZWADmu3U5T07tOnc3YzK2TsZVWVmEV2pj+kI3rvG5aONOTTMTTOC3dssTEmE8n6jrZo0e6hwoY4tVX3ees0i0UBQpQ+CrUJ4tRi7UMHe1BcNnxA6bnTJ48Mou5Iv2oKSqFIgDcvg00aSItJcnf4ISQS968zhaYWrWA338HOnaUb+1Gb31WK/Ddd2mTq0sXWX/LIQuQVP9o1izg/Q9oahGxw46cyIkf8SPCEJZY88lR9+ke3IMVWGFaR8oTLuKiW8uNAx266ZTZWqxNtyw5c0prl5HlTAj/FMT0B4yLA6ZPc18IUQjg2lVZ5EiRirfwFgTc1+9qiIZoWb4SHnvM/bSmpgFDhgA5fDObq/ACpeAoFGnk+++BXbtcPzvsduDaNWDSJGDbNuD4cWDrVlkBmDQvoGmzpS6o5wl798oCo+6eZyTw8UcaKrGyocKgQ0dVVEUndMIZnMFETEQ3dMOzeBZLsRQ7sdNl8cy0UBAFTZUXAWHogwMA8zAPl2FSMNaEK1eAe+5xr3xqGhARAfToka5hAsfp00B06iruToSEADt3uv2aV66Ap05JZSmb8QSewEzMRBjCUn2nQcN6rEczNMMX39/AfffJ9Q5/GMeUVIcOwNixARJY4YRScBSKNDJjhvGbvsUCLFggLTflyyet1zSgWDHjvq1W6dLkLStWmDscXr4MtIscbNiGIPqjPwCpgAzDMMzCLHyJL9EarX1iuXFQAAVM+yTo5MzrChtsOIADaZbj4EGgWjVg2TLX3+u6/L/Mny+tc5mC8HDzNqTLdly1Cnz0EaBwIaBcWaBwIXDoUDCjl7f3MX3QByMxMtV6h8K9DuswvGBfbNoEzJkDtGolfauefhpYuRKYN0/53wQLpeAoFGnk7FnjaSYhgDNnXH/Xv797kzYgLTh9+3ovk83mmWNymwv90RzNUykNDiXjU3yKcijnvQBpZBzGGSo4TdDE1IIDwOWbticIAbRtKy04ws2MRPXq0hrXunWahggOJUpIwY20XpsNaNPGaRXnzAFaNAf++Sdp5Y0bwJTJwEMNspWSE494TMIkt9/bYccCLMCZ0BPo3BlYulRaa+fOlQEHaQ0UUKQfpeAoMi2MigJPnJBRIkGgZEnj54bF4t4KM3QoUKaM6zc7TZNTIA884L1MDz7ofnrKQY4cQPV7QvArfsUETEBplE78rj7q4zf8hsEwtvD4GsudjzuawzzKpwiK4H7cn6bx//oLOHw4yVfJFZGRMtx3xAgZkfbOO8CpU2kaLpHYWOCHH+T58NprUg5PInI8RdM0YNQb7rU2XQcaPQIt2cnG6Gi5g2Tqk8luB44eBd5+23dCZnC2Yzuu4Ippuz/wRwCkUXiFP72dr1y5wq5duzJ37tzMmzcv+/bty5iYGMP2gwYNYqVKlRgeHs7SpUtz8ODBvH79ulM7AKmWOXPmeCyXiqLK3IidOynatklKNBhipejejeLYsYDKMX26ecTN7Nnutz97VpYASB5NlSsXOWqUrHuUFoQg7703dfK45BFAgwal2IaCl3nZNDGdP3mUj5pGUpVhGcOIlo/4UZrHHzPGs9pQjog0qzWpMOebb6at6vfatUl1r0JCksavUYM8fTrNu+IS8eGHFBaLjKZyFM/VQFGvHsXly85tv/jCvJhu7lzZJvJqLdeaRvBZaOEn/CTYomYLMkwm41atWrFGjRrctGkT169fz4oVK7JLF/eZT/fs2cOnnnqKS5Ys4dGjR7lq1SrefffdfPrpp52FBvjNN9/w3LlzicttLy42peBkXsSGDRQ5wlOHvYZYKQrkpwhg+eXbt8latVwrE7pOPvigTMlvxqlT5K+/ysy2N26kX659++SDM7lcyeviGLxjBIXjPG76AHE8RHIwR2KYuiMDMwi2OTaUX3wp+MsvZGys9zKMHeu5guNq+cJ9VQaXHDxI5sjhulSD1UpWruzZueMNYvlyivr1KcqWoaheneKrr1xmNBYvvUQRGmJea+7ECd8KmEG5zMuJ55nRZx3XBVvUbEGGUHD2799PANy6dWviut9//52apvHMmTMe9zN//nyGhoYyIdkrLQD+nI6qYv5ScE7wBFdxFbdzu0eF/zIq4uZNik8+oahcSd7oCheiGDaM4r//giuXEBR3VzQuEdGkcUBlunaN7NzZ+UHlyJPiqwJ7aeHsWfJ//yNLliQjIsiqVclJkwKTiyOWsZzN2WzDNmzIhnyOz3Ert7ptv5qrPVJwQJmfpyu7sh/7sT3b87EDw5jvob1OCkL+/OTXX3sn89q1aVduHGN6U8X5uefMFSovjNKGCJuN4rn+SS8CVj3JgtO3D0UKc6EYNcq81pwGikuXfCNgJqAne7q1MOrUeS/vzdT3/MxEhlBwZsyYwXz58jmtS0hIoK7rXLRokcf9TJ8+nYUKFXJaB4AlSpRgwYIF+cADD3DGjBkUBjbi2NhYRkVFJS6RkZE+VXD2ci8bs7HTSX8X7+I8Zr5q2iI6mqJOHWmiTm6mtuoU+fJR7NwZPNnWrvWsirmLCuj+5swZ8ocfyBkzpEUmu3KGZ3gP70m0uDisLSD4Ml92+RDYxV0eKzggWJAFKSg4b56xgjBzpudyC0Hed1/6rDjevHPlzWvcl8UiE0j6AjFqlPspJ4tGMWKEc/tt24yvL91C0fBh3wiXSbjMy6zMyqmmSK20Mj/zczd3B1vEbEOGUHDeffddVqpUKdX6woULc+rUqR71cenSJZYpU4b/+9//nNa//fbb/Pvvv7l9+3ZOmDCBYWFh/Oyzz9z2M3bsWJd+O75QcPZxH3Mzt1vt/mt6+SoZZMTgwe6znlp1iop3Ba1Qn/jqK88UnKVLAyrX9u1k+/ZJVpycOcnBg8kLFwIqRiri48ljx8jIyLT5iHiLoGAd1jE057uqsC0oWJmVPcpW7PjE2G6xdGljJaFQIe+meU6cIMuUSZrOc0wXeargGMy+pyIkxLy/Fi08788dIjqaImeE8fWSI5wixb1QtH7M/X3AolGsWJF+4TIZ13mdb/NtlmRJgmBe5uVLfIkneTLYomUr/KrgjBgxwqWykHw5cOBAuhWcqKgo1q1bl61atWJ8fLxh29GjR7NUqVJuv/enBac1Wxs6R+ZkTt6gDxwrAoCIiaGIyGGuQATp5iZ++MEzBWdd4ObC16yRlYJdVYQuXVpadvxBbCz533+yVERKbt+WTrMOB1aAvOce8ttv/avo/M2/DZUSjRrLszztTK0gL+ZijxWccIZzzTq7R0rHsmXe7UN0NDl5MtmggTxmFSp4ruDcd5/n41Stalyqw2olhw71TnZXiCVLPLtmUpifRHS0VHIc01qhIVKxichBYeQ5n01Q01HBw6+lGl555RUcOHDAcKlQoQKKFSuGixcvOm1rs9lw9epVFDPJchYTE4NWrVohd+7c+PnnnxFiUn2uXr16OH36NOLcZNoMCwtDnjx5nBZfcB7n8Tt+N0wzfxM3sRCZJA36oUMwLWNttcokD8HgsceAMJM8J0WKyFjpAGCzAV27yrqFrqJpz50DXn3Vt2NeuQK89BJQsKCsXF6wINC4MbBmjfw+Pl4epnHjZFsHhw4BvXr5N6PqCqxILOXgCoI4ceeTkjZogx/wg2kFbius6IquuHzRs1tXiluQKblzAwMHyvQvgwbJDNSe4k2RzEGDjL+32WSupHTjaVn6FO203LmhLV0GbNsODH8NGPA8MHkKcPYctG7dfCBY5sYs87YiY+B1fsXChQujcGHzqr3169fH9evXsW3bNtSuXRsA8Ndff0EIgXr16rndLjo6Gi1btkRYWBiWLFmCcA8yce7cuRP58+dHmNnDz8dEIhIEDdtYYcV/+C9AEqWT0FDzNkL4vtyxh2j584NDhwEfvC9fdF0xegy0AMn3xx8y2Z87bDbgp5+Azz8HChVK/3hXrgD168uHbnKFav16oGlTmVjs0iVg7drUh8fx9zvvAJ06AVWrpl+elNhg8+jGb4PrZDNd0AXt0A71UR+7sTvVtaVDRzjC8Tpex1UPszyXLm3exhVCAO+/73l7XZd1yTylb1+Z5Xr1aucUNRaL/Putt4AqVTzvzy2O+gFmVK/ucrV2//3A/WnLLaRQBB1/mpJatWrF+++/n5s3b+bff//Nu+++2ylM/PTp06xcuTI3b95MUpqe6tWrx+rVq/Po0aNOYeC2OyEKS5Ys4fTp07lnzx4eOXKEU6dOZUREBMeMGeOxXL6KovIkvFWjxqn0zOco2IiEBIoSxc3N2bt961AnEhKkKf399ykmT6aIjHTf1majGDxImsutujSd63fye7zzjqGzua+ZMMEzH42NG30z3sCB7vPbaJr0/bnnHvOpjyFDfCNPSn7hL6bXQwQjeIbG83Y3eINd2IXanY9jCrgCKyRGYwlBVqrkOszacTxKlfIusik5Bw96PjWladKn5vhx78aIjSXfedPGR/PvYD1sZD5cZbVq5I8/pk1md4hHGhn71TWo79sBFQo/kiGcjEmZuK9Lly7MlSsX8+TJwz59+jgl+jtx4gQBcPXq1STJ1atXu/XrOXEn58Lvv//OmjVrMleuXMyZMydr1KjBL7/8knYvHF99GSZej/UMk49ZaeVFXkz3OIFCfPKJe8XGqlM0b+bb8VatSlKqrLpUXHQLRb9+FAYJTcTx4xTjxlEMGSKTmJ0751O5PGHSJPcP2OTLrl3pH+vmTZk3xexB604B8rXzqisSmMBSLGWasC+EIRzDMaZ+DCd4gpM5mRM5kX/yz1S+OytXJiXbS3kcNI1cvDjt+7J3r+fKjdVKLljgXf9CCKnMlyyReH3ZQ0Mp+vROlXgvvYgjR2Sqh5Sh3yFWioIFKA4d8s04jheViRMppk+nCLaXvSJLkmEUnIyKLxWcVVxFCy1uHSRHcqTHfV3nde7lXkbStQXjHM/xKI/yFv2XzEQIQTFoYNIN0KF4aKCoXdunN1+xdWuSBcZVKGr3bj4byx+cPGlsLdE0snx50hdBZ0eOmD9sQ0Kkw7NRG4uF7NAh/fK4Yyu3po4qFK4Vnbf4VrrH+/NP6bCbfB8rVSJ/+y19/d6+TebJY37Mn3ySTIt+IEaMcP8ScU9limvX0rcDKcc7dYpi4MCkiKqIHBQvvOCz3Fbi998pihV1vl+EWCleHpYqz45CkR6UgmOCrxP9LeESFmVRgkm5P8IYxtEc7TJiJCWRjGQP9mAIQxJv/g/yQa7kSpLkb/yND/CBxO9yMicHczCv8qpP5HeF+Pdfiuefp2jejKLTMxQ//+zzG5V48gn3pnPHsn+/T8f0NT17GltxvvnGN+OcO2f+sLVaydq1zafN5vk5PdNJnuRDm4cRCRa3yo0jGuo6r6d7PCHIHTtkNuitW30XKTZ8uHHJizJl0jYFJg4eND7nrTrF6NG+2YmUY9tsFFFRFGmdu3PV5/r1UplxlWvHolEMGuizsRQKpeCY4I9MxglM4BIu4af8lLM4y2PlI5KRLM7iqXKHOKxCz/N5J8XJ8dGp8x7e41clx5+IqCj3GYmTm9D9dKP3Fbdvy4RsDgXDUaPIYpE+Or6kXj3zKbFffzUuAXDvvb4vAZCSf/8lUXurqT8OCM5mxg05vnWLfPhh57w4DuUmb15y27a09SuGDzfPFFy0iE/3xZ+IRx8xflGxaBTZOfulwqf4NUxc4RorrHgST+IlvIRe6IX8yO/RdiMwApdwKVVkiYAMrfgSXzr97cAOO47gCN7Dez6QPghERbmvcOxA05xjnTMg4eEyGmbHDuDll4HevWWkUmSkrDrtS958Uz5iXaHrQNu2wBNPACtXyvBxQAa8OSqW33cf8OefngXLpYcpUwBLoWum7TRouAbzdsEiRw55vCZPlhFN4eEyC8GQIcCuXUCtWmns+OQJ85LvFy+CbtJeZCR4/rwM2zPaH00D5s0LnFAKxR28DhNX+I5ruIb5mO82bJZw8zS7gx12TMd0vIf3EILghG6nmcKF5RMjNtZ9G7sdKF8+cDKlg5o15eJPWrUCvvkGeP55IC5OKi6kDEd/7DFg9mzZ7qGHgNOngZ9/limLQkPl9w8/LJ81/mbTJkDEVTBtRxAVYN4umISFAS++KBefkb+A1Ehtrq97AFK78rcm6gs8eQHRdeDyZf/Lkowt2ILP8TnWYR106GiFVhiEQagKP+RHUGRYlAUniJzCKbfKjadEIQqXcMlHEgUOLTwc6NkrybzgCosF6NEjcEJlAnr1kgkEJ02Sis5rrwHbtwO//grkypXU7vBh4OBB+Xv58lL5CoRyA9zJxXj8LmBdQ8Cmu24kNBRDMbRAi8AIlZHo0sVYubFaga5dofnxH8a4ODAhwbjNtWvg8uXgH3+A7hSZkiWNr2FA7msAX1Q+w2eoh3qYh3mIRCRO4iS+xteogRqYB9eWJILYiI3oh35ogibohE5YjMWGSVwVmYAATJllOPxVTdxbTvCER34KZp8oBnc/0oo4c0aGybrzR/joo2CLmOm4cUPWxXL4ioSESP+RiAjf51dxx9ixd5xzq+8iYnISCSnCxm0WanYLf+WvgREogyGEoGjaxLXfilWnyJXTZ6HbTuPa7bKeW5V7k8Zr0pji99+d2928SfHC8xThYUntQkMonu1LER2dut/OnYx9inKEU1xPvzO5J2zgBsN7pZVWHuMxp21stLEP+yR+7/BxdAR7XOO1gMiu8AzlZGxCRlFwSPJ+3m+YR8foo1NnczYP9i6kCxEZSdGxo/PNvkJ5im+/DbZomZJ27YyTAQaijNiZMzLpoMVCouoeYlkrwp6URkHbXI/fn1nlf0EyMCImhqJDh6SUCI7zv3w5ii1bfD+e3U7RtUuS029yhUoDxZ1ixSIhgaLxo64DAKw6Rf0HU+WnEseOURQo4N7R2MPiyr6gMzsbFnvVqXM4hzttM47j3Kb50KmzDdsETH6FOd48vzXSndti1iU6Ohp58+ZFVFSUz+pSpZVlWIYn8IRLfxsLLCiO4jiP86lMpdqdzxqsQUM0DJS4foMXLwJHj8p5lmrVoFnU7Km37NljnJnfYgEaNJClHfzN+vXS4TkmRv7N4mehlYlEWHRhLP6kAlpkw5kpV/DoUWDZMumLVrMm0KyZX859zp4N9DSY7tU04OAh6S3fuZNxZzO/gda7t3P/R44Aw4YCv/+e5AlfoQLwzjhoXbqkS3ZvKIZiuIALhm3qoA62QtbTi0MciqO4qbP7YRzG3bjbZ3Iq0o43z2/lZBxkWqM1vsN3GIABuI3bsMIKcefTC73wIT5ET/TEMiyDFVZo0JCABORETszCrCyh3ACAVqSIDFFRpJmFC6U7hDv3DiGAv/+WBSj9fagbNgT++w/47jvgr78AsgQaNSqB3r2TIrwUgFaxogzL8jeTJyUVunKFxQJ89RWwZ7d0CnYXFWWxANOnyXDBZGh33w38thQ8fVoWS8uTB6hRw69+RK7wpBZa8jY7sMNUudGgYQVWKAUnE6IUnAxAd3RHO7TDfMzHURxFXuRFR3RMjDBZiqXYiZ34GT/jBm6gKqqiEzohJ3IGWXJFRiI62jNH4piYwOiS+fLJZ3cgnt8KE3bvNk7LYLcDO3fI8DujkG8hgFORbr/WSpUCSnlYCdUPNEdzzMEct8EbOnQ0R/PEvz0J8nC8VCoyH0rBySDkQi70RV+339e881Eo3HHPPcbBOQAQEQGUKBEYeRQZCLOUDJomQ9OLFZNTxe6UIU0DihX1j4w+4CX8v737jm+q6h84/rlJOijQltlSQKCADJmCgIiCUqWIguJPZTzKEnAg+ogi6AMKCOJ+1MeByFJUFGQpG0UQwTIERUCgUDYtyGjLbpLv749rQ0Ob0TaryXnzyov25uTe72nG/ebcM55iJjMLvE9Dw4iRwQy2bbuO6wgnnMtcdrhPK1Zu4AaPx6p4n+rooChBokcP/RzliNGoX1lwVkYJUvfc43w4twh0uwf69nM9AWc/+y9icuoU8uabSLubkObNkIGPIL/9VuyQi6IFLfiUTzFgwJTn+7sRI2GE8S3fcg3X2LaXoxz/4l8YKXg6AyNGGtOYtrT1euyK56lOxn7uZKwonvTFF/rUQZpmf54yGqFGDX0SvkqV/Bef4h+ybRu0bKE38V39kW80Qlwc7Nqt/9ymNWzfnv9SldEIderAxk1o/0y6JFu3QlJHOHPmygsutyPYmLFoo0Z5vW4F2clOPuADVrMaAwaSSeYxHqMmNfOVPcMZOtCBP/jDbrCHESPlKMfP/Ex96vswesWZwpy/VYKjEhwlyCxfDmPGwLp1+u+lSkG/fvpSD4VJbs6fh6++0vdjNELHjnDvvSVjgl0lP/n+e32E1IULemdhTdMTkerVYdlytPr6SVxOnoR+fWHRIvtk6I47YMZnaHH6JSq5cAFq1dRnM3bUb2fuPLR77vFqvTzhHOf4+J9/hzhEOcrRhz4MZSgJqGu6gUQlOC6oBEcJBRkZeofihAS9701h/PyzvrbV6dNXrmyYzXr/0aVL4To1432JJKdP60PbNm7QFyrrlAzdu6MVkLXK3r2wZo2e5LRrh3bttfb3f/YZ9O3j+GBGI7Rujbb2F09Xo8Tau1dvZc3I0CeBfughPb9U3KcSHBdUgqMojqWlQaNGep/Uq7tjGI36MO9du/RRUkrokr599LO1q4VDL14qMIEKJRaLPprwww/191DuiH2rFV54QV+g18cj6kuswpy/VSdjRVHs/O9/+mKeBfU1tVjgxAm9EUAJca46Ixe2XBAbNQo++kj/2WKBnBz9fxEYPx7eftu/8QUrleAEIAsW1rGOZSwjjTR/h6OEmNmznX8pF9EnFVRC3I1tnScvBgM0aaIvrBvCMjPhnXfy9+3Oa8IE/UuF4lkqwQkgf/M3AxhABSpwEzeRTDKJJNKRjuxil7/DU0LE+fOuy5w96/04lADXu7e+tIqjpSWsVvj3M76NKQAtW+Z8CiKAU6fgF9VVyeNUghMAcsjhaZ4mnnimMpVMMu3uX81qbuRGUkkt8jGsWMkiy62ZO5XQ1rSp3k/AEZNJXzZJKTzJzkZWrECWLtXXXyvBtOhomL8AIiLsXzC5Pw8eDA8/7J/gAoi7XwbUlwbPUwlOABjMYN7jvXwLauayYCGbbEYzutD7PslJnud5KlCBGGKIIoqHeIid7Cxu2EqQeuIJ55eozGZ47DHfxRMM5PJlZNgwfRbgTnfAnZ2hWlXkX//SRzaVUNqtt8K2P+HJoXDNNfoaIB07wsLv4MOPfL4WVSBq2NC9cg0aeDeOUKRGUfl5FNVOdtIQ994BJkz8zd/EEONW+eMcpw1tOMhBu+TJhIkwwviRH2lDmyLFrQQvEX3enBkz9JEduZ8QuSM/Ro2CsWP9G2NJIlYr3Ncdvvuu4GFpDRvCuvVopdXacsFIRB+VuGtXwV8cjEZo1w5++snnoZVIahRVCTKTmXZTijtjxkwGGW7v+xme4RCH8rUMmTFziUv0oAdW1AgHxZ6mwdSp8PHHkHfqk2bN4OuvVXJTaCtXwoIFjoel/fknfPqp7+NSCrSVrYxmNMMYxhSmcI5zDsseP66PgBoyRE/8t2/PX0bT9C8LERH5V8swmfSF1ydN8nAlFEC14Pi9BWcQg5jGNLdXtT3BCSpQwWXZk5wknniX+13KUjrRye14ldAioq9SbjTq/UmVwpMeD+rDzhxd99M0qN8AraCzo+IzWWTRgx4sYQkmTLZVxMtQhhnMoDvd7cq/9x4MG6bnrUaj/l4xm/U14aZP1xOavHbs0GcYz30phIVBr14wejQkJvquniWdasEpQapT3W79E0dy11NxJ7kB2MUul8mNESN/8Idb+3PlFKcYz3hqU5toomlIQ97hHc6ies6VZJoGMTEquSmWAwdcj7s/fMjx3YcOITNnIp99huzZ44UAA58grGc9AxlIMsn0oQ8rWenWZ6e7+/8//o/lLAf0Vu4ccgB9GYf7uZ+1rLWV//JLeOopPaGxWvV5bcz/fNx+843ev/pqDRvqLaBnzsDBg/r/06er5MarJARlZmYKIJmZmf4ORfbLftFEE1z8C5dw2Sgb3d7vb/Kby30axCDvy/vFrsNBOSg1pIYYxGDbt/bPv+vkOvlb/i72MRSlpLJ26yZWk1GsGo5vtWvnf9yZM2J94H6xGjT7ssmdxHrsmB9q4h85kiO9pbcgiElMdv/fIXfIOTlX7GOkSIrTz0qjGKWTdBIREatVpE4dEU0T0bPT/DdNE9m/v9hhKQUozPlbteD4WQ1qMIIRTstUoxrLWU5LWrq93yY0oSpVnZYRhLu4y+19OvIQD3GEI3b9eXI/Gf7iL4YwpNjHUJQS6+GHnbfgGAx6r+48JCdHX9xy7tz8M8T98AO0vwXJzvZCsIHnJV7iS74EsLVK5/6/kpU8RvGH9M1hjtO+kBYsLGc52WSzcyekpjqfuE/T9G5Xin+pBMeP9u7VV2tu+s14xmS/ne/yUwMa8BmfcYADtKd9ofZtxMiLvOj0/h70oCY1ixK6zQ52sJrVDi+HWbAwm9mkk16s4yj+JVu2IAP6I7VqIom1kMGDkW3b/B1WydC1K7RtW/DkQiaTvuri1ePuv/1WXxCzoMTIbNbPsFOmeCfeAHKOc7zLuw4vRVmx8gVfcJSjxTpOFlloOB/SLghnOevWfDUGg5rXJhCoBMcPjh2Dzp2hTh29k1mPBzXGlf83XQYeZemF1SxhCWmksYMdPMRDGIr4ND3Ko4xiFBoaxn/+5X5L6UxnPqX4IzdSSHFZxoKFzWwu9rEU/5BPPoGWLeDzz/X+JPv3w7Sp0LyZvqK04pRmMsHiJdC9e/4VFW+8EX5ei1a+vP326dMdzxCca+pUj8YZiNaz3ukoJtA/X1aysljHqU99h/OQ5YohhopUpHbt/KOhrmY2q3ltAoF745MVjzlzRp/z4MAB++1mM8ycGs6hvbewYoXzmWTdpaExlrH0pS/TmEYaaVSgAr3oRStaufzG4g4j7gXq7lB4JbDI1q3w2KNXhojkyv25fz/khhvQ1Ke5U1p0NHz9DXLgAKxapf/92rRBa9So4AccO+p8nScRyAj+VtHLXC5yOStWVrKSWcziNKepQx0GMID61M9X9iEe4nmed3g8I0YGMpAwwqhQAe6/X1+zzVxAw7WmQaVKcFfxr/4rxeWDPkEBx5+djCdOFDEYHHdOA5GFC30eVpEdkkN2nYsL+hcpkZIp/u/QrRSedUB/sYaZHHeODTOJdcgQf4cZdKx3dXHeMdmgibXF9f4O0+uOyBGXny8IskW22D3utJyWm+QmW4dkTTRbx+SRMlKsYs13rMky2Tb44uoOxvWlvpySU1fiOiJStaqIyWT/2W006tuWLfP2XyZ0qU7GAWzKFOdfzIxGfVKokqIa1XiABxy25Bgw8BiPEY1/5xtSiujHHwv+mprLbIZVP/ouHg8QEWTfPuSvvxBXqyD6S/8BzjsmAzwy0Dex+FECCdzDPQ4/X4wYaU1rmtHMbntvevMrvwJ6h2RBbP0EX+VVPuGTfPt6hEf4nu9pQQvbttKU5gmeYB3rKEe5K3ElwMaN8MgjUKqUvk3TIDlZXzTzjjuKU2vFY7yZaZ08eVJ69eolZcuWlZiYGOnfv79kZ2c7fUz79u0FsLsNHjzYrsyBAwfkzjvvlFKlSkmlSpXk2WeflZycHLfj8mcLTtmyzltvQKR1a5+HVSxZkiXtpJ3t207eYZx3y91yUS76O0SliKyJtZwPb9YQa6Pr/B2mW6xWq1inTBFrndpXYo+NEevw4WI9V/yhxp5kNZvFmpQkVqMh/9/bZBRr8+ZiPX/e32H6RLqkSx2pU2DLSpzESaqk2pX/U/502eJTQ2qIRSwOj3lMjkmqpMoFueAyvgsXRA4cEDlzpthVVdwQMC04vXv3Zvv27axYsYLvv/+eNWvWMGjQIJePGzhwIMeOHbPdXn/9ddt9FouFLl26cPnyZdatW8eMGTOYPn06o0cXfiFKf0hIcH6/0QjVq/smFk8pS1l+4ifmM5+udKU1relOd5axjPnMJ4II1ztRAtPtdzjvUWk06mVKglGj4JEBsG/flW2ZmfDWm3B7UkC15mhGIyxcCIMGQXj4lTtMJniwB/z4I1pu00GQiyOOjWzkZV6mGtUwYqQylXmO59jKVmpT2678Yha77Bt4gAP8xV8O748nntrUJpJIl/FFRurrjMa4t0Sg4kveyrJ27NghgGzceGVyuiVLloimaXLkyBGHj2vfvr089dRTDu9fvHixGAwGSU9Pt2376KOPJDo6Wi5duuRWbP5swXnjDdd9cBYt8nlYQcm6ebPeh6RZU7G2vVGsr70m1r/VpIOFYd22reBWhLx9cHbv9neYLln//NN5K5TRINZ33vF3mAWynjwp1kWLxPrdd2LN87mnFGycjLO1IDv7t1k2+ztUpQgCogVn/fr1xMbG0rLllcnpkpKSMBgMpKQ4H1r8xRdfULFiRRo1asTIkSM5f/683X4bN25MXFycbVunTp3IyspiewlYy2XQIH14eEFfig0G6NRJv46rFI+MH68Pbf7sM/j9d1i/Hl4YCdfWRX77zd/hlRhao0YwfYb+4sz7ojUa9d+/+BKtbl3/BeiuyZOdt0SJwIcf+i6eQtDKl0e78060u+5Cy/O5pxSsOc1dLlMTSSR1KQGvW6VYvDZ2Nz09ncqVK9sfzGSifPnypKc7Ht7Yq1cvatSoQUJCAn/88QfPP/88u3btYu7cubb9xl31Js/93dF+L126xKVLl2y/Z2VlFalOnhAdDT//rK9VsmDBldkww8Ohf399ZVpX018ozsnChTDqP/oveTvIWq36ypGdk5H9B0Kmib+4tH/9C2nZEj76CH78Qe9NmXQ7PP44Wp06/g7PPbt3Oe8sLQJp+xzfr5QYySRTjWoc5ajd7Oq5jBjpRz/KUtYP0Sm+VOgEZ8SIEbz22mtOy+zcubPIAeXto9O4cWOqVKlCx44d2bt3L7Vr13bySMdeffVVxowZU+SYPK1yZZg3Dw4dgg0b9C+WN98MV8/1pRTRm2/oLQwFjUKxWODECZg1K9/0+IpjWv368O67/g6j6GJjHb8mcpW2X1FUcnJg6VJ90qoKFeCuu9DKBtZJUS5f1lspP/4I9uzRv0H1/hc8+SRaVedLtQQrI0bmMIeOdOQSl+xacwwYaEhDJjDBI8eyWGD5cti1S1+QtmtX/fNdCRCFvf51/Phx2blzp9PbpUuXZMqUKRIbG2v32JycHDEajTJ37ly3j3f27FkBZOnSpSIiMmrUKGnatKldmX379gkgv/32W4H7uHjxomRmZtpuhw4dCpjFNhXPsubkuB71YzKK9V+9/R2qjfW338T66KNi7dBerN26inXmTLFeVCPPPMk6d67z10SYSayPPWZfPq7ylTlnNMRaOkqsEyeK1Zp/DhV/sF64INbbbr3Shyjv67tcObFu3ervEP1ql+ySgTJQoiRKEKSaVJNX5BXJkiyP7P/HH0WqVdP7Teb2qzSZRIYOFckd1Lthg0ivXiIVKojExop06SKyYoVHDh+yCtMHx+udjDdt2mTbtmzZMpedjK+2du1aAeT3338XkSudjDMyMmxlJk2aJNHR0XLRzZNCIK0mrniW9fJl9xKcXj39Hao+bHn48Csn2Lwnqvr1xFqI94ninDUnR+9sXtCkhUaDWMuUFuvu3fpz8t13elJz9SreubcJE/xdHRERsY4c6bgDuMmoD/G3OB4KHUrMYvbo/jZuFAkPL3jAiKaJDBwoMnWq/nPeyQCNRv3/MWM8Gk5ICYgER0QkOTlZmjdvLikpKbJ27VqpW7eu9Ox55cRy+PBhqVevnqSkpIiISGpqqowdO1Y2bdokaWlpsmDBAklMTJRbbrnF9hiz2SyNGjWSO+64Q7Zu3SpLly6VSpUqyciRI92OSyU4wc16fXPnI38Mmlj/9z9/hynWTz5x3qLQskXAtBYEA2tGhljbtbvy9w0P03+uEi/WSZPEek8357M2595KRYrVz5OeWC9e1OfwcRXr4sV+jTNY3XnnlWTF0c3VaNkffvB3LUqmgElwTp48KT179pQyZcpIdHS09OvXz26iv7S0NAFk1apVIiJy8OBBueWWW6R8+fISEREhderUkeeeey5fRfbv3y+dO3eWUqVKScWKFWXYsGElZqI/xfusn3/ufDhwdFmx+vm5t1qtYq2d6PoE9fPPfo2zqKy7d+uX2r76SqzHjvk7HBur1SrWlBSxvvSS3gLy7bdi/WyGnvS6k9zk3mbM8G89tm93HWOYSayjRvk1zmB06pTeMuMsedE05wmOySTSrZu/a1IyFeb87dUVEMuXL8+XX37p8P6aNWsiucOIgOrVq7N69WqX+61RowaLFy/2SIyhJocclrCEVFKJJZaudKUiFf0dlmf17g0pv8IHH9h3LDUaISwc5s3XFz/0pwMH7CecK4jJpHdybdfONzF5gBw9Cv36wooVVzYajUjvf8EHH6CVLu232AA0TYNWrfQbIBkZcE31/IuJOmM06h3V/cnd1XhdLXutFNrp01dGvzqSm8o4YjbrSzoo3qVe/SFkEYvoT3+OcxwjRixYCCOMf/NvJjDB7ZXBA52mach770PnO/Uk57fN+nSj93aHIUPQEhP9HaJ7J1NNc/+kGwDkzBm4uZ0+PDAviwW+mAkH9iMrf9Bn6Q0UU6e6XvPpahaLPnWtP9WpA9WqweHDjsuYzWpRJC+oXBnCwiAnp3j7Ubmn96kZV0LET/xEN7pxAv2bpwX9Qz2HHN7gDYYz3J/heZymafrkaIsWoR1LR0vbj/b224GR3IB+gnQ1L0BODqSnIxMmIIsXI4U9Efvaxx/rLVMFJWUWC6xeDYHW8rp1S+EfExsLd9/t8VAKQzMa4dnnHBcwmfRWqtatfRdUiChTBnr2dN6IZjA4n8/MZILOnT0fm2JPJTgh4kVetM1RfjVBeJd3OcYxP0QWmrTwcHj8CdezOs78HF5+Ce7qAom1kF9/9U2ARTF1ij6ZoiNGI8yY7rNw3BIRobeUuSO33Pv/Q4t0vUaR1z35JDz2mP5zbnNA7uupdm2YO0+/JKd43NixUK6c4yTnP//RG40dvb1F4KmnvBefolMJTgg4xCHWsa7AWT1zCcJsZvswKoUXX4QOHfQTp6MTkdV6pUXkyBFI6ojs2uWzEAslI8P5/RaLXgcvE7MZmTsXefJJZMgQZNYsfUK8gtx1t/uXqGrXhjnfovXu7blgi0HTNLQPPoS1v+iT+7Vpo6/1MuMz2LIVzdXKvkqR1agBKSn6sjp537rVq8O0aTBmDHz/ff4kJ3eFk5kzoWlT38cdatRVwBBwkpMuyxgxulVO8RwtIgJZvASmT4cPP9Bnoo2IgDNnCn6A1QoXLsDIETB3ni9DdU+VBMh2knwZjV7vuyI7dkCXO/VLZSaTfvb58AOIi0O+X4TWooX9A+65B2rV0vsNFXRpTdPgjTegfQe4/vqAbBHR2raFtm39HUbISUzUk5gjR/S3btmy0Lz5lYTm1lshLQ0+/RSWLdPz6Jtv1pfpqVnTr6GHDE3EVX/w4JOVlUVMTAyZmZlE+3s0jQ+c4ATxxDttwdHQ+JRP6U9/H0amXE0GDtQv47jqXPzGG2jDnvVJTO6SN9+EEc87v0y1aDGalzofyOnT0KA+nDyZv1XGaIQyZWH79nwtG7JvH9yepJ+NjEY9fk3Tf578KdrDD3sl3mByjnMsZzlnOEMd6tCOdmgEXjKolHyFOX+rS1QhoBKV6EpXp6OkIonkfu4H4CIXeZd3qU99wgmnIhV5iqfYz34fRRzCzpx275LJc88hy5Z5P57CGDwY6tYteHiIwaC353fq5L3jT50Kf//teA2ys9l6R+iraImJsPMv+GoW9OgB3bvDy2PgwEGV3LggCK/zOvHE053u9Kc/t3AL13Ita1jj7/CUEKdacEKgBQdgN7tpTWuyybaNoAK95UYQPuETBjKQc5wjiSRSSAGwdUo2YSKKKH7gB1rS0i91CAXy/PPw9luukxyjETrcipZ3vpkAICdOwKODYf78KxOBRETAgAHw5lte7ZwrbVrrq9c6U7cu2q7dXosh1IxhDC/zcr7tBgyYMPEzP9OKVr4PLMBJdrZ+bSsmBq1KFX+HU6KoFhwln2u5lvWsJ4kku6bj2tRmFrMYyEAARjOajWzMN+LKjJlznOM+7rNLkBQPGzDAvRYciwVW/UigfT/RKlVC+3Yu7D8A386FBQvhyFG0/33g/ZFHZ8+6LnPunHdjCCEnOMErvFLgfVasWLAwkpE+jiqwyZEjSL9+UKkiNGwAVROQm9shP/7o79CCkmrBCZEWnLwOcYh97COWWJrQxJbwXOACccSRTbbTx3/P93Shiy9CDUny4ovw6gTXBTUNcsxoroaahwjp1RPmzHHcf8lohI4d0ZYG2KW9EuoDPmAoQ5327QM4whESUCO65PBhaNMajh+3f43mvn+//gbtvvv8E1wJolpwFKeqU532tKcpTe1ac1JJdZncmDCxAReXAZTieeUVuO//nJcxGOCGViq5yevRx5x3zrZY9LmHFI/IIMOt2c+Pc9wH0ZQAI0fkT25A79QuAo8MQC5c8E9sQUp9Oio2YYS5LCMI4YT7IJrQpWkaTJoEpaKcz4/z9NM+jSvg3XwzDB2q/5z375b788N9/D4DcTCpSlXMOB/tp6FRBdXHRM6cga+/dpyAi0BmJnz7rU/jCnYqwVFsruVaqlPdaRkLFjqj5hj3Nq18ef3DLizMflRS7tSpTz4JDz7on+AClKZp8M5/4dMpUK/elTtq1YL3/wdTpwbkPDYl1QM84PTLjhEjySQTR5wPowpQjpYwySssDHarDvCepBIcxcaAgRGMcHi/ESPtac/1XO/DqEKXlpwMf2yDwY9C1apQsSIk3Q7fL4L/vqtO1gXQNA2tf3/YvgMyjkN6BuxJRXviCXU5z8PKUY7xjC/wPiNGwglnIhN9HFWAcqevp8XiXjnFbaqTsXpB2RGEZ3mWt3kbEybMmG0rjzelKStYQSUq+TtMRVECxCQmMZrRdn1tWtCCSUyiBS2cPDK0yPXN4Y8/HE+EqWmwdx+amubYqcKcv1WCoxKcAm1lK5OZzG52U45y9KAHXemKSa3uoSjKVXLIYS1rySST2tSmMY39HVLAke+/h64O+oAZDPDQw2jTpvk2qBJIJTguqARHURRF8TWZMQMefwwuXtT71lmt+qWphx6CTyajRUT4O8SAV5jzt/o6riiKoig+oPXpg9x7rz6iKjUVYmLggQfQ6tTxd2hBSSU4iuJnOeSwgx1YsFCf+kQR5e+QFEXxEi06GgYO9HcYIUENK1BCjiCsYx2f8RkLWcgF/DO5lgULE5lIVarSjGa0oAVxxPEsz/otJkVRlGChWnCUEs2MmQMcwICBGtTA4CJnX8taBjCA3VyZbyKaaEYzmmd4xm5mZ28ShAEM4DM+s1vz6yxneYd32MhGVrBCTaqoKIpSRKoFRymRzJh5lVepRjXqUIdEEqlFLd7jPYdr42xgAx3pSCqpdtuzyOJZnnU4p4c3rGY1M5hhl9zksmJlDWv4jM98Fo+iKEqwUQmOUuJYsHA/9/MiL5JBhm37QQ7yFE/xKI8WmDiMYAQWLA4ToLGM5SQnvRZ3XpOZ7HTIvQEDH/OxT2JRFEUJRirBUUqcOcxhPvMLTGJATx5+4ie7bUc4wipWYcHicL9mzHzDN27FsIlNPMMzPMzDjGIUe9nrdvwAu9ntdB0fK1b2sa9Q+1QURVGuUH1wlBLnIz6yza5cEBMmJjGJW7nVts2dFY1NmEgn3WmZi1ykN72Zy1xMmGxJ1iu8wkhGMp7xbvXjqUAFDBgctiYBxBDjcj+K4iki8PPPMH06HD0KVapAnz7Qvr3jNV8VJZCpBEcpcf7iL5ctMTvYYbftIhdd7jeHHKpS1WmZIQxhPvNtx8nrVV6lClV4kiddHqsXvVjGMof3GzDwMA+73I8S/A5xiB/4AQsW2tCG67jO48e4fBl69oS5c/X558xm/f/p0/UF2GfPBjUHnVLSqEtUSomyn/0ukxUNjVhi7bZ9wRcuW1ZMmLif+x3ef4QjTGOa01aX8Yx3eukp1wM8QAMaFNgPx4iRClTgcR53uR8leGWRRQ96UIMa9KMfj/AIjWhEe9pzkIMePdbw4TB/vv5z7qLXuf8vWgT//rdHD6coPqESHKXE2MpWmtGMbLJdlu1BD7vfv+ALh312cl3HdZSjnMP7F7HIaXIDkEEGm9nsMr5IIvmRH7mRGwE9qTFiBKAudVnDGuKIc7kfJTiZMXMndzKHOflet+tYRzvacYpTHjnWmTPw8ceO14C0WuHTT+Gkb/rfK4rHqARHKREE4QEe4CxnnSYZJkxUoxoP8ZDd9rOcdXmMeOKd3n+BCy7n2ckt54544lnDGjazmQlMYCxjWcUqdrCD+tR3ax9KcPqe7/mFXwq8FGvGzBGOeGyU3Zo1cOmS8zI5ObBqlUcOpyg+o/rgKCXCKlaxhz0uy9WmNotZTFnK2m1PJJE97HHYimPCRF3qOt13E5q4bMExYCh0cnL9P/8UJdcMZjjtSG/FyhSm8AIvFPtYly97tpyiBArVgqOUCL/xm+0SjjNTmEIiifm2u+rPYsbMQJyvD9OBDtShjsM4TJi4l3tdtgQpiivHOOa0Iz3ACU545FjXu5lbt2jhkcMpis+oBEcpEcIJd9mHBvS+LQUZzGDa0CbfJabcjsfDGU4Tmjjdt4bGl3xJJJH5OgcbMVKFKrzHey5jVBRXalLTaUKvoVGNaoXa56FDMGYM9OoFgwfDypX60PDEREhO1kdNFcRkgttug3r1CnU4RfE7leAoJUJnOru8PFSZyjSlaYH3RRLJSlYyjGFEE23bXoMaTGISE5noVhw3cAOb2EQvehFGGKCvZfUUT7GJTSSQ4GaNFMWx/vR32YIziEFu7++dd6BmTRg3Dr75BqZOhdtvh7Zt4dQpvRNxQgIYr8qpjEaIj4dp04pQCUXxM68mOKdOnaJ3795ER0cTGxvLgAEDOHvWcWfP/fv3o2lagbfZs2fbyhV0/6xZs7xZFcXP6lKXe7nX6bfa53ne6fIHUUTxOq+TQQZ/8ie72c1e9jKIQYVaZLM+9ZnBDDLJZAlLmMlMHuVRKlO5UHVSFEeSSKIrXQvs1G7ESEMa8giPuLWvOXPgmWf00VAWi37LHQK+cSPcdx9UrQq//QYvvngl0alSBUaO1Ldfc40na6covqGJiOt2/yLq3Lkzx44dY9KkSeTk5NCvXz9uuOEGvvzyywLLWywWTpywv678ySef8MYbb3Ds2DHKlCmjB61pTJs2jeTkZFu52NhYIiMLvjxxtaysLGJiYsjMzCQ6Otr1A5SAkEUWd3M3a1iDCRNmzLb/n+RJ3uVdn60GPoUpjGIUxzhm23YzN/MhH9KIRj6JQQlul7nMC7zAh3xoG5lnwsQDPMD7vE95yrvchwg0awbbtuk/O7JhA9xwg4cCVxQvKsz522sJzs6dO2nYsCEbN26kZcuWACxdupQ777yTw4cPk5DgXlN+8+bNuf7665kyZcqVoDWNefPmcc899xQpNpXglFxWrKxkJV/yJSc5SS1qMYABDi9NecM7vMMzPJNvuxEjUUSxgQ1qmLfiMdlk8yu/YsbM9VxfqPmRjhyBai666phMMGKEfvlKUQJdQCQ4U6dOZdiwYZw+fdq2zWw2ExkZyezZs7n33ntd7mPz5s20bNmSX375hbZt214JWtNISEjg0qVLJCYm8uijj9KvXz80BwumXLp0iUt5JnrIysqievXqKsFRCu0Up6hCFS5T8JhZI0bu5m7mMc/HkSkA5zlPCilc5jJNaEIVqvg7JL9KTYW6zmc/ICwMnnoK3njDNzEpSnEUJsHx2jw46enpVK5s3yfBZDJRvnx50tOdL2iYa8qUKTRo0MAuuQEYO3Yst912G1FRUSxfvpzHH3+cs2fPMnTo0AL38+qrrzJmzJiiVURR8viar8khx+H9FiwsZCEnOUkFKvgwstBmwcJYxvJf/ksWWYCebN7HfbzP+yHbP6p6dYiOhqwsx2VycqBJngGEZjMsWQJ790JsLHTtCuVdXw1TlIBT6E7GI0aMcNgROPf2119/FTuwCxcu8OWXXzJgwIB8940aNYqbbrqJ5s2b8/zzzzN8+HDecPL1Y+TIkWRmZtpuhw4dKnZ8Smg6xCGnHZlBv4yWt2+O4l2CMIABjGOcLbkBPemZy1xu4ibOcMZ/AfpRRAQMGpR/dFQuTdOTmPv/WYJt0SL9klbXrjBsGPTrp3c2/s9/HC/loCiBqtAtOMOGDaNv375OyyQmJhIfH8/x48fttpvNZk6dOkV8vOuJ0ObMmcP58+d5+GHXKyq3bt2acePGcenSJSIKWPI2IiKiwO2Kkpcg/MRPfM7npJNONarRj360oY2t83IlKrkcvgtQkYreDlf5x0Y2MoMZBd5nxsw+9vE+7zOKUT6OLDCMHg0//ghbtuTvaCwCNWro61H99Rd063Ylkcn9//JlGD9eb9mZ6N5sCooSELzeyXjTpk20+GcKzOXLl5OcnOxWJ+MOHTpQsWJF5syZ4/JY48eP56233uLUKfcWn1OdjJWrXeAC/8f/sZjF+UZo9aQnM5hBGGEc5SjXcI3DJMeIkVu5lRWs8HENQtejPMoUpjhdxb0qVTnMYR9GFVjS0+G66/Q5b65mMumT/cXEwObNjltqTCY4fBji1Bqwih8V5vzttXlwGjRoQHJyMgMHDmTDhg388ssvDBkyhB49etiSmyNHjlC/fn02bNhg99jU1FTWrFnDI4/kn+fhu+++49NPP+XPP/8kNTWVjz76iAkTJvDkk096qypKCBjKUJayFMB2osz9fxazbN/+E0jgWZ4tcB+Gf/69wis+iFjJdZCDTpMbIOQvGc6ZA3nGe9gxm2H3bn1OHGeXoSwWfV6c9eudDzlXlIAhXnTy5Enp2bOnlClTRqKjo6Vfv36SnZ1tuz8tLU0AWbVqld3jRo4cKdWrVxeLxZJvn0uWLJFmzZpJmTJlpHTp0tK0aVP5+OOPCyzrSGZmpgCSmZlZ5LopwSNd0sUkJsHJvyiJkmzRX7sWschL8pJESqRdmZpSU1bJKv9WJgT1kT4un7+KUtHt/V2QC7JUlspsmS2/y+9ejNx3WrYU0TQRPTUp/q1OHZGlS/1dKyUUFeb87dWJ/gKVukSl5PUlX9Kb3i7LLWEJyVyZXDKTTBazmCyyqEtdOtChwJlnFe9ayUpu53aH9xsx8gzP8DqvO92PILzDO4xjnF2n5Ja0ZDKTaUYzD0XsezVqwMGDntufpum3pUv1JR8UxVcC4hKVopQUjua0cVUuhhh60pPBDOY2blPJjZ90pCO3c3uBy3gYMVKe8jzN0y73M5axDGNYvhFXW9hCO9qxgx0eitj3EhPB4OTlaTRCxYqOR1tdLfdr8dNPq8tVSuBSn8hKyGtBC5dlNDSa09wH0SiFpaExn/n0pGe+JLMpTVnLWpeLoKaT7rDvlAULF7lYokdhDRzoun/Niy9C6dLuJzlWK+zYoY/OUpRApBIcJeQ1pjFtaetwfhsTJu7mbqpT3ceRKe6KIorP+ZyDHGQqU/mYj9nIRjazmWu51uXjv+Irp6vVW7Awn/mcxkFP3QB3//3QoUPBrTgGAyQlwZAhegfiDh0Kt++jRz0RoaJ4nkpwFAX4nM+pQIV8lzmMGKlOdT7mYz9FphRGVarSj34MZjAtaclqVtOVrkQRRSSR3MZtfMd3+R53lKNOV6oHfQLH4xx3WiZQhYXB4sV6ElOq1JXtUVEwdCh8950+DLxhQ1i5EtLS3F+bqkpor4ahBDDVyVh1Mlb+kU46/+W/TGUqJzlJPPEMZCBDGerWys1KYPkf/+NJnrTNZwR6wmrBwkhGMoEJtrJv8RbDGe60FceAgROcKPGvhaws/bKSpkHz5lC2bMHlLl3SkxdHw8s1Da69Fnbu1H9WFF8IiMU2A5lKcBQluG1nO41pjOD4420FK0giCdBbcKpT3WGCY8RIF7qwgAVeiTdQTZ+uL9dwtdyEZtEi6NzZpyEpIU6NolIUJaR9xEdOLzmZMPEe79l+TyCBEYwosKwRI+GEMw43r9kEkb599STnqnWTqV4dFi5UyY0S2Ly2mrii+FMaaXzN15zkJDWpSS96UY5y/g4rH7MZ5s2DadP0eUqqVNG/Mf/f/0F4uL+jK7nWsc7p7MZmzPzKr3bbXuEVylCGV3mVbLJt2xvQgKlMpQlNrt5NSOjTB3r1glWr4PhxfTHOW25xPuxcUQKBukSlLlEFFTNmnuRJJjHJtnSCGTPhhPMWb/EET/g7RJvz5+HOO2H1an1orsWinzSsVmjVCpYv19cHUgqvLW1Zz3qnZRytT3WOc/zAD2SRxbVcyw3cYFtsVVEU/yrM+Vu14ChBZRjDmMQkBMHyzz+AS1xiCEMoRzl60QuA7dth8mR9HZ7YWHjwQejSRR9N4gtPPw0//6z/bPln7c7cuUo2b4bBg2HWLN/EEmy60IUUUhz2qTFhoitdC7yvNKUd3qcoSsmhWnBUC07QyCCDalRzemkikUT2SCqj/qMxYYKezJjNV1pQrr8eli3TZ3X1pr//hoQEyMlxXMZggAMH9EsCSuFkkEFd6nKOc/mSHA0NI0b+4A8a0MBPESqKUhSqk7ESkhawwNZi48g+9jFm3h9M+GeEsPmfXCi3BeWPP/T+L962fr3z5Ab01pw1a7wfS6ARgVOnIDOz6PuII46lLKUMZewuLxkwEE443/KtSm4UJcipBEcJGtlku7Ue1OSvsxzO22E2631iNm3ycHBXcbfd1Nn0+sHGaoWPPtLnVqlQQb9s2KwZfPVV0dY7aktb9rOft3mbLnShM50Zy1j2s19dglKUEKD64ChB41quddmCo4nGsZ9r42R6FEwmfdbXli09HGAerVpduSzmiKZB27beiyGQWK36aJ2ZM+0njdu2TR/B8+efMH584fdbjnI8/c8/RVFCi2rBUYJGZzoTT7zDES9GjNxytgscc77woqbBZfcWGC+y+Hh44AHHCxuaTHqH58RE78YRKObP15MbsG+tyW3BmjABNm70eVghKy0NRoyAm2/W16Z67TU4ccLfUSlK4agERwkaJkxMZzrGf/5dfV8ssXwY9q7Dqelz5eRAC9cLjBfbhx9Co0Z6QpXbapH7c506MGWK92MIFP/7n/NVrE0m/fKV4n0zZ0LduvDmm7B2rX7J9oUX9GQ7d9SfopQEKsFRgkonOrGa1XSgg22bCRMP8ACb2ETDyEQGDXJ8MjUa9cn27r7b+7HGxuqdjT/8UB+9FRcHTZvCu+/qrRVXzx4bzP74w/nlOrMZfv/dd/GEqs2b9UuFFov982G1Xpm3SbXkKCWFGiauhokHrROc4BSnqEIVornyPJ87Bx076kmEyJVLIiYTREToqym3aeOnoENUjRr6TM6OaJp+uWT1at/FFIoeekife8nsYKYFg0HvCzWi4FUtFMXr1DBxRQEqUYl61LNLbgBKl9annX/jDahdW//Qjo6GRx7RV1lWyU3x7N0Lw4bplzmuuUafQNHVpQ1n/ZFy+WL4fqhbssRxcgN6S87Spb6LR1GKQ7XgqBYcRfGYZcugWzf9JJl7iSN3MsWXX4aXXir4cQcOwHXXwYUL+YfGG436xIu7dvl+6Yrjx2HOHH1ixho14L77oEwZ38bgS+XKwZkzzsvcdJPeN0dR/EG14CiK4nMnTsC99+oj0PL238htEXj5ZX34fUFq1NCTo3L/rIdqMl1ZMqNqVb3FzZfJjdUKI0fqxx4yBF55RV9ZOz4epk71TQyHD+t/s27d9FawmTPh4kXvHvPGG523pBmNeoKjKCWBasFRLTiK4hGvvaaPtnE0OaHRCLfeCitWON7HhQvwzTewbp1+6bBjR/0EHxbmnZgdGTVKT2oc+eYbuP9+7x1/6lQYNEj/Oe8irNdco/cRq1vXO8ddvhw6dXJ8v9Gor90WKtMXKIGnMOdvleCoBEdRPKJLF8ctNLkiIvQkZtkymDQJdu7UR5P16qWP3gmE1dNPn9ZbahzNhaRp+gl+zx4czohdHKtX64lgQZ/MRqO+Ntnu3RAe7vljw5XkLu9ElCaTnmBNn653RFYUf1GXqBRF8Tl3T/Z9+kDnzvDdd3q/mg0b9JXVGzWCffu8GqJbFi50PtGjiN6ReutW7xz/9df1FpuCWCx6f6W5c71zbIBx4/QE9I479ISzQgX9EtmGDSq5UUoWleAoiuIRt93mPMkxGqFmzSszFue2DuQO1T92TJ9/SARSUvQ+L02b6n0+3npLX4DTF06fdpxg5OWqM25R5I5ScjYnkNEIixZ5/th53XGH3hp35ozewXrmTN9MfqkonqQSHEUJIfvYxxd8wVd8xRGOeHTfffvqQ/CdtT6cOOF44UyLBXbs0FsJ2rSBL77QJwBctw6GD4d69fS1qbytdm33FjmtVcvzx7ZYXB9bBC5d8vyxFSXYqARHUULAcY5zF3dRhzr8i3/Ri15cwzX0pCdZZHnkGOXL65edIiPtR+LkjoZ67jnXrTAGg57YgP18LFar3rKSnOz9dcKSk/VZpB21RhmNeufnmjU9f+ywMGjQwPXlPtWaoiiuqQRHUYJcNtncwi0sYxmSZxl1K1ZmM5s7uIMccjxyrA4d9A6wL76oX16qX19vkdm0SZ9I0RWr1fHJ3WKBo0fh2289EqpDYWHw6ad6HFe3RhmNEBWlL6fhLU895fg+TdMTxv79vXd8RQkWKsFRlCA3jWnsZjdm8k9Ra8FCCinMY57Hjle1KowZo3fC3blTH/LcooU+8sid9bWcjes0meDHHz0WqkN3360PZ7/hhivbNE1v3UlJ0Scl9JZHHoHu3fWf8yZYJpP+++efQ6VK3ju+ogQLleAoSpCbgvNlyY0YmcY0r8dhMumtE45aaNzp2Avu9Y/xhNtug19/hbQ0fd2yo0fh++/1S0jeZDTC11/rrUiNGul/l8hIPelZt05f1kJRFNdM/g5AURTvOsYxu0tTV7Ng4TCHfRLL8OH6itVz59rPs2Iw6MORy5bVEwpHrThms+9n0q1Z0zv9bZwxGmHAAP0m4p35dhQl2KkWHEUJctWohobjM6QRI9dwjU9iMZlg9my9H02HDvqkddddp08st2OHvjyCo+TGYNCXcujRwyehBgyV3ChK0XgtwRk/fjxt27YlKiqK2NhYtx4jIowePZoqVapQqlQpkpKS2LNnj12ZU6dO0bt3b6Kjo4mNjWXAgAGcPXvWCzVQlODwCM5791qwMIABPopGT1S6d9eXHDh0CP78U09sKla80moB9iOxjEYoVUofpRUV5bNQFUUpwbyW4Fy+fJn777+fxx57zO3HvP7667z33nt8/PHHpKSkULp0aTp16sTFPCvM9e7dm+3bt7NixQq+//571qxZw6DcRVsURcmnL31pRCOM5F9F0YCB9rSnK139EFl+mgaTJ8OCBfpQ7Lg4/fLQM8/A9u1qoUdFUdzn9bWopk+fztNPP80ZF9N+iggJCQkMGzaMZ599FoDMzEzi4uKYPn06PXr0YOfOnTRs2JCNGzfSsmVLAJYuXcqdd97J4cOHSUhIcCsmtRaVEmpOc5rHeZzZzMaC3vEljDD60pf/8l+iUM0iiqIEvhK5FlVaWhrp6ekkJSXZtsXExNC6dWvWr18PwPr164mNjbUlNwBJSUkYDAZSUlIc7vvSpUtkZWXZ3RQllJSjHF/xFYc4xHzms5CFHOMYn/CJSm4URQlKATOKKj09HYC4uDi77XFxcbb70tPTqXzVRBomk4ny5cvbyhTk1VdfZcyYMR6OWFFKnipUoRvd/B2GoiiK1xWqBWfEiBFomub09tdff3kr1iIbOXIkmZmZttuhQ4f8HZKiKIqiKF5UqBacYcOG0bdvX6dlEhMTixRIfHw8ABkZGVSpUsW2PSMjg2bNmtnKHD9+3O5xZrOZU6dO2R5fkIiICCIiIooUl6IoiqIoJU+hEpxKlSpRyUtzhNeqVYv4+Hh++OEHW0KTlZVFSkqKbSTWjTfeyJkzZ9i8eTMt/llt7scff8RqtdK6dWuvxKUoiqIoSsnjtU7GBw8eZOvWrRw8eBCLxcLWrVvZunWr3Zw19evXZ948fQ0cTdN4+umneeWVV1i4cCHbtm3j4YcfJiEhgXvuuQeABg0akJyczMCBA9mwYQO//PILQ4YMoUePHm6PoFIURVEUJfh5rZPx6NGjmTFjhu335s2bA7Bq1So6dOgAwK5du8jMzLSVGT58OOfOnWPQoEGcOXOGdu3asXTpUiIjI21lvvjiC4YMGULHjh0xGAzcd999vPfee96qhqIoiqIoJZDX58EJRGoeHEVRFEUpeUrkPDiKoiiKoiieohIcRVEURVGCjkpwFEVRFEUJOgEzk7Ev5XY7Uks2KIqiKErJkXvedqf7cEgmONnZ2QBUr17dz5EoiqIoilJY2dnZxMTEOC0TkqOorFYrR48epWzZsmiaZndfVlYW1atX59ChQ0E9wipU6gmhU9dQqSeougajUKknhE5dvVFPESE7O5uEhAQMBue9bEKyBcdgMFCtWjWnZaKjo4P6hZcrVOoJoVPXUKknqLoGo1CpJ4ROXT1dT1ctN7lUJ2NFURRFUYKOSnAURVEURQk6KsG5SkREBC+99FLQrz4eKvWE0KlrqNQTVF2DUajUE0Knrv6uZ0h2MlYURVEUJbipFhxFURRFUYKOSnAURVEURQk6KsFRFEVRFCXoqARHURRFUZSgE3IJzvjx42nbti1RUVHExsa69RgRYfTo0VSpUoVSpUqRlJTEnj177MqcOnWK3r17Ex0dTWxsLAMGDODs2bNeqIH7ChvT/v370TStwNvs2bNt5Qq6f9asWb6oUoGK8rfv0KFDvjo8+uijdmUOHjxIly5diIqKonLlyjz33HOYzWZvVsWlwtb11KlTPPnkk9SrV49SpUpxzTXXMHToUDIzM+3KBcJz+sEHH1CzZk0iIyNp3bo1GzZscFp+9uzZ1K9fn8jISBo3bszixYvt7nfnfesPhann5MmTufnmmylXrhzlypUjKSkpX/m+ffvme+6Sk5O9XQ23FKau06dPz1ePyMhIuzLB8JwW9NmjaRpdunSxlQnU53TNmjXcfffdJCQkoGka8+fPd/mYn376ieuvv56IiAjq1KnD9OnT85Up7HvfbRJiRo8eLW+//bY888wzEhMT49ZjJk6cKDExMTJ//nz5/fffpWvXrlKrVi25cOGCrUxycrI0bdpUfv31V/n555+lTp060rNnTy/Vwj2FjclsNsuxY8fsbmPGjJEyZcpIdna2rRwg06ZNsyuX92/ha0X527dv314GDhxoV4fMzEzb/WazWRo1aiRJSUmyZcsWWbx4sVSsWFFGjhzp7eo4Vdi6btu2Tbp37y4LFy6U1NRU+eGHH6Ru3bpy33332ZXz93M6a9YsCQ8Pl6lTp8r27dtl4MCBEhsbKxkZGQWW/+WXX8RoNMrrr78uO3bskP/85z8SFhYm27Zts5Vx533ra4WtZ69eveSDDz6QLVu2yM6dO6Vv374SExMjhw8ftpXp06ePJCcn2z13p06d8lWVHCpsXadNmybR0dF29UhPT7crEwzP6cmTJ+3q+Oeff4rRaJRp06bZygTqc7p48WJ58cUXZe7cuQLIvHnznJbft2+fREVFyTPPPCM7duyQ999/X4xGoyxdutRWprB/v8IIuQQn17Rp09xKcKxWq8THx8sbb7xh23bmzBmJiIiQr776SkREduzYIYBs3LjRVmbJkiWiaZocOXLE47G7w1MxNWvWTPr372+3zZ0Xtq8UtZ7t27eXp556yuH9ixcvFoPBYPcB+9FHH0l0dLRcunTJI7EXlqee02+++UbCw8MlJyfHts3fz2mrVq3kiSeesP1usVgkISFBXn311QLLP/DAA9KlSxe7ba1bt5bBgweLiHvvW38obD2vZjabpWzZsjJjxgzbtj59+ki3bt08HWqxFbaurj6Tg/U5feedd6Rs2bJy9uxZ27ZAfU7zcuczY/jw4XLdddfZbXvwwQelU6dOtt+L+/dzJuQuURVWWloa6enpJCUl2bbFxMTQunVr1q9fD8D69euJjY2lZcuWtjJJSUkYDAZSUlJ8HrOnYtq8eTNbt25lwIAB+e574oknqFixIq1atWLq1KluLV3vDcWp5xdffEHFihVp1KgRI0eO5Pz583b7bdy4MXFxcbZtnTp1Iisri+3bt3u+Im7w1OssMzOT6OhoTCb7pej89ZxevnyZzZs3273HDAYDSUlJtvfY1davX29XHvTnJ7e8O+9bXytKPa92/vx5cnJyKF++vN32n376icqVK1OvXj0ee+wxTp486dHYC6uodT179iw1atSgevXqdOvWze69FqzP6ZQpU+jRowelS5e22x5oz2lRuHqfeuLv50xILrZZGOnp6QB2J7rc33PvS09Pp3Llynb3m0wmypcvbyvja56IacqUKTRo0IC2bdvabR87diy33XYbUVFRLF++nMcff5yzZ88ydOhQj8XvrqLWs1evXtSoUYOEhAT++OMPnn/+eXbt2sXcuXNt+y3oOc+9zx888Zz+/fffjBs3jkGDBtlt9+dz+vfff2OxWAr8e//1118FPsbR85P3PZm7zVEZXytKPa/2/PPPk5CQYHdCSE5Opnv37tSqVYu9e/fywgsv0LlzZ9avX4/RaPRoHdxVlLrWq1ePqVOn0qRJEzIzM3nzzTdp27Yt27dvp1q1akH5nG7YsIE///yTKVOm2G0PxOe0KBy9T7Oysrhw4QKnT58u9nvCmaBIcEaMGMFrr73mtMzOnTupX7++jyLyHnfrWlwXLlzgyy+/ZNSoUfnuy7utefPmnDt3jjfeeMOjJ0Nv1zPvCb5x48ZUqVKFjh07snfvXmrXrl3k/RaFr57TrKwsunTpQsOGDXn55Zft7vPFc6oUz8SJE5k1axY//fSTXefbHj162H5u3LgxTZo0oXbt2vz000907NjRH6EWyY033siNN95o+71t27Y0aNCASZMmMW7cOD9G5j1TpkyhcePGtGrVym57sDyn/hYUCc6wYcPo27ev0zKJiYlF2nd8fDwAGRkZVKlSxbY9IyODZs2a2cocP37c7nFms5lTp07ZHu8p7ta1uDHNmTOH8+fP8/DDD7ss27p1a8aNG8elS5c8tuaIr+qZq3Xr1gCkpqZSu3Zt4uPj8/Xkz8jIACiRz2l2djbJycmULVuWefPmERYW5rS8N55TRypWrIjRaLT9fXNlZGQ4rFd8fLzT8u68b32tKPXM9eabbzJx4kRWrlxJkyZNnJZNTEykYsWKpKam+u1kWJy65goLC6N58+akpqYCwfecnjt3jlmzZjF27FiXxwmE57QoHL1Po6OjKVWqFEajsdivE6eK3YunhCpsJ+M333zTti0zM7PATsabNm2ylVm2bFlAdDIuakzt27fPN9LGkVdeeUXKlStX5FiLw1N/+7Vr1wogv//+u4hc6WSctyf/pEmTJDo6Wi5evOi5ChRCUeuamZkpbdq0kfbt28u5c+fcOpavn9NWrVrJkCFDbL9bLBapWrWq007Gd911l922G2+8MV8nY2fvW38obD1FRF577TWJjo6W9evXu3WMQ4cOiaZpsmDBgmLHWxxFqWteZrNZ6tWrJ//+979FJLieUxH9HBQRESF///23y2MEynOaF252Mm7UqJHdtp49e+brZFyc14nTGIu9hxLmwIEDsmXLFtvw5y1btsiWLVvshkHXq1dP5s6da/t94sSJEhsbKwsWLJA//vhDunXrVuAw8ebNm0tKSoqsXbtW6tatGxDDxJ3FdPjwYalXr56kpKTYPW7Pnj2iaZosWbIk3z4XLlwokydPlm3btsmePXvkww8/lKioKBk9erTX6+NIYeuZmpoqY8eOlU2bNklaWposWLBAEhMT5ZZbbrE9JneY+B133CFbt26VpUuXSqVKlQJimHhh6pqZmSmtW7eWxo0bS2pqqt2wU7PZLCKB8ZzOmjVLIiIiZPr06bJjxw4ZNGiQxMbG2kaxPfTQQzJixAhb+V9++UVMJpO8+eabsnPnTnnppZcKHCbu6n3ra4Wt58SJEyU8PFzmzJlj99zlfl5lZ2fLs88+K+vXr5e0tDRZuXKlXH/99VK3bl2/JeK5ClvXMWPGyLJly2Tv3r2yefNm6dGjh0RGRsr27dttZYLhOc3Vrl07efDBB/NtD+TnNDs723bOBOTtt9+WLVu2yIEDB0REZMSIEfLQQw/ZyucOE3/uuedk586d8sEHHxQ4TNzZ3684Qi7B6dOnjwD5bqtWrbKV4Z85QXJZrVYZNWqUxMXFSUREhHTs2FF27dplt9+TJ09Kz549pUyZMhIdHS39+vWzS5r8wVVMaWlp+eouIjJy5EipXr26WCyWfPtcsmSJNGvWTMqUKSOlS5eWpk2byscff1xgWV8pbD0PHjwot9xyi5QvX14iIiKkTp068txzz9nNgyMisn//funcubOUKlVKKlasKMOGDbMbWu0Pha3rqlWrCny9A5KWliYigfOcvv/++3LNNddIeHi4tGrVSn799Vfbfe3bt5c+ffrYlf/mm2/k2muvlfDwcLnuuutk0aJFdve78771h8LUs0aNGgU+dy+99JKIiJw/f17uuOMOqVSpkoSFhUmNGjVk4MCBHjk5eEJh6vr000/bysbFxcmdd94pv/32m93+guE5FRH566+/BJDly5fn21cgP6eOPk9y69enTx9p3759vsc0a9ZMwsPDJTEx0e7cmsvZ3684NBE/je9VFEVRFEXxEjUPjqIoiqIoQUclOIqiKIqiBB2V4CiKoiiKEnRUgqMoiqIoStBRCY6iKIqiKEFHJTiKoiiKogQdleAoiqIoihJ0VIKjKIqiKErQUQmOoiiKoihBRyU4iqIoiqIEHZXgKIqiKIoSdFSCoyiKoihK0Pl/iibQOnvz3T8AAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#this dataset have three classes , differentiating them by neural network is a difficult task\n",
    "plt.scatter(X[:, 0], X[:, 1], c=Y, cmap='brg') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "7b6a81cc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00000000e+00  0.00000000e+00  0.00000000e+00]\n",
      " [-2.94633794e-05 -1.27362933e-04 -1.00922574e-04]\n",
      " [ 1.62553071e-04  7.17707621e-05 -6.49185381e-05]\n",
      " [ 2.84437928e-04  2.23957271e-04 -1.66560225e-05]\n",
      " [ 3.69679966e-04  2.66171194e-04 -4.61879485e-05]]\n",
      "[[0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [0.00000000e+00 0.00000000e+00 0.00000000e+00]\n",
      " [1.62553071e-04 7.17707621e-05 0.00000000e+00]\n",
      " [2.84437928e-04 2.23957271e-04 0.00000000e+00]\n",
      " [3.69679966e-04 2.66171194e-04 0.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# Dense Layer class\n",
    "class Layer_Dense:\n",
    "  def __init__(self,n_inputs, n_neurons):\n",
    "    #initialize weights and biases\n",
    "    self.weights = 0.01*np.random.randn(n_inputs, n_neurons)# this np.random.randn produces gaussian with a mean of 0 and varianve of 1 \n",
    "    # multiplied by 0.01 to reduce the training time bigger number takes more time to calculate na that's why\n",
    "\n",
    "    self.biases = np.zeros((1,n_neurons))\n",
    " \n",
    "  #forward pass\n",
    "  def forward(self,inputs):\n",
    "    # calculate output values from inputs, weights and biases\n",
    "    self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "class Activation_ReLU:\n",
    "# Forward pass \n",
    "    def forward(self, inputs):\n",
    "\n",
    "    # Calculate output values from input \n",
    "      self.output = np.maximum(0, inputs) \n",
    "# when we pass data through a model from begining to end that called forward pass\n",
    "X,Y = spiral_data(samples=100, classes=3) \n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values \n",
    "dense1 = Layer_Dense(2, 3) \n",
    "\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Perform a forward pass of our training data through this layer \n",
    "dense1.forward(X) \n",
    "# Let's see output of the first few samples: \n",
    "\n",
    "print(dense1.output[:5])\n",
    "\n",
    "activation1.forward(dense1.output)\n",
    "\n",
    "print(activation1.output[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "bed49fe7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.38389653e-87 3.72007598e-44 1.00000000e+00]]\n"
     ]
    }
   ],
   "source": [
    "# now lets define activation functions and what does it do\n",
    "# we dont mostly use sigmoid or any other activation function because it takes a lot of time to calculate them also so \n",
    "# we use relu easy to calculate and also add complexity in the system\n",
    "# but to classify we need final output in a a range so we can compare and classify things for this in output layer we use softmax funtion\n",
    "#  To address this lack of context, the \n",
    "#softmax activation on the output data can take in non-normalized, or uncalibrated, inputs and produce a normalized distribution of probabilities for our classes\n",
    "# lets look at an soft_max function example and its effect on output\n",
    "# Softmax activation \n",
    "class Activation_Softmax: \n",
    "  def forward(self,inputs):\n",
    "    exp_values = np.exp(inputs-np.max(inputs,axis=1, keepdims=True))\n",
    "    probabilities = exp_values/np.sum(exp_values,axis=1, keepdims=True)# keepdim = True ensure that the dimension after summation doesnt change because it can happen as row summed up this output removes the bracket on the final sum so we get an list but to ensure we get matrix we dp keepdims = True its important\n",
    "    self.output = probabilities\n",
    "# to make the exponential in control such that we dont get overflow error we subtract by maximum value of input to ensure that exponential is in control\n",
    "softmax = Activation_Softmax()\n",
    "a = [100,200,300]\n",
    "softmax.forward([a])\n",
    "print(softmax.output)\n",
    "\n",
    "\n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "65171ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# do you get money for these sessions\n",
    "# if i believe that krishna is not a god can i understand the essence of bhagvadgita"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e64b168c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333246 0.33333352 0.33333402]\n",
      " [0.33333158 0.33333371 0.3333347 ]\n",
      " [0.3333333  0.33333334 0.33333336]\n",
      " [0.33333097 0.33333385 0.33333518]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Dense Layer class\n",
    "class Layer_Dense:\n",
    "  def __init__(self,n_inputs, n_neurons):\n",
    "    #initialize weights and biases\n",
    "    self.weights = 0.01*np.random.randn(n_inputs, n_neurons)# this np.random.randn produces gaussian with a mean of 0 and varianve of 1 \n",
    "    # multiplied by 0.01 to reduce the training time bigger number takes more time to calculate na that's why\n",
    "\n",
    "    self.biases = np.zeros((1,n_neurons))\n",
    " \n",
    "  #forward pass\n",
    "  def forward(self,inputs):\n",
    "    # calculate output values from inputs, weights and biases\n",
    "    self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "class Activation_ReLU:\n",
    "# Forward pass \n",
    "    def forward(self, inputs):\n",
    "\n",
    "    # Calculate output values from input \n",
    "      self.output = np.maximum(0, inputs) \n",
    "class Activation_Softmax:\n",
    "  def forward(self, inputs):\n",
    "    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "    probabilities = exp_values/np.sum(exp_values, axis=1, keepdims=True)\n",
    "    self.output = probabilities\n",
    "\n",
    "# when we pass data through a model from begining to end that called forward pass\n",
    "X,Y = spiral_data(samples=100, classes=3) \n",
    "\n",
    "# Create Dense layer with 2 input features and 3 output values \n",
    "dense1 = Layer_Dense(2, 3) \n",
    "dense2 = Layer_Dense(3,3)\n",
    "activation2 = Activation_Softmax()\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Perform a forward pass of our training data through this layer \n",
    "dense1.forward(X) \n",
    "# Let's see output of the first few samples: \n",
    "\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "\n",
    "print(activation2.output[:5])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ae85213c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here the model confidence score is equal for a input array this is because weights are randomly generated from a gaussian distribution \n",
    "# now we will learn how to correct our neural network via loss functions\n",
    "# you will wonder why we do not calculate the error of a model based on the argmax accuracy.revall out earlier example of confidenceit shows that the result for different inputs with different confidence score would be same which is not good \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "b06b852a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# one of the loss function is mean squared error used in linear regression\n",
    "# but here we are doing classification so we need a different loss function,\n",
    "#Categorical cross-entropy is explicitly used to compare a ‚Äúground-truth‚Äù probability (y ‚Äã or ‚Äã ‚Äútargets‚Äã‚Äù) and some predicted distribution (y-hat ‚Äã or ‚Äúpredictions‚Äã‚Äù), so it makes sense to use cross-entropy here. It is also one of the most commonly used loss functions with a softmax activation on the output layer. \n",
    "# Arrays or vectors like this are called one-hot,‚Äã meaning one of the values is ‚Äúhot‚Äù (on), with a value of 1, and the rest are ‚Äúcold‚Äù (off), with values of 0.\n",
    "#  When comparing the model‚Äôs results to a one-hot vector using cross-entropy, the other parts of the equation zero out, and the target probability‚Äôs log loss is multiplied by 1, making the cross-entropy calculation relatively simple. This is also a special case of the cross-entropy calculation, called categorical cross-entropy\n",
    "# lets code it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c930e3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.3025850929940455\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "softmax_output = [0.1,0.1,0.2]\n",
    "target_output = [1,0,0]# this is like we had three classification and the targeted one is first class for this given input while the neural network output is above \n",
    "loss = -(math.log(softmax_output[0])*target_output[0] #only term conti is this \n",
    "         + math.log(softmax_output[1])*target_output[1] # this is zero becuase target_output[1] = 0 because of hot vector so we can ingnore this \n",
    "         + math.log(softmax_output[2])*target_output[2]) # this is zero becuase target_output[2] = 0 because of hot vector so we can ingnore this\n",
    "# the log used is natural log\n",
    "print(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "519f3761",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.7, 0.1, 0.2], [0.1, 0.5, 0.4], [0.02, 0.9, 0.08]]\n",
      "0.7\n",
      "0.5\n",
      "0.9\n"
     ]
    }
   ],
   "source": [
    "softmax_outputs = [[0.7, 0.1, 0.2], \n",
    "                   [0.1, 0.5, 0.4], \n",
    "                  [0.02, 0.9, 0.08]] \n",
    "print(softmax_outputs)# i dont get why we need to use np.array here isnt the output is same may be its because of difference in datatypes\n",
    "class_targets = [0, 1, 1]\n",
    "\n",
    "for targ_idx, distribution in zip(class_targets, softmax_outputs):\n",
    "  print(distribution[targ_idx])# this doesnot gives a list\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "647c85c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.4\n",
      "[0.7 0.5 0.9]\n",
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "#we can remove the zip function loop with the help of np.array\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2], \n",
    "                   [0.1, 0.5, 0.4], \n",
    "                   [0.02, 0.9, 0.08]])\n",
    "print(softmax_outputs[1][2])\n",
    "class_targets = [0,1,1]\n",
    "print(softmax_outputs[range(len(softmax_outputs)), class_targets])\n",
    "# using len range is samrt cause We can achieve that by using a list containing numbers from 0 \n",
    "#through all of the indices. We know we‚Äôre going to have as many indices as distributions in our \n",
    "#entire batch, so we can use a range() instead of typing each value ourselves: \n",
    "neg_log = -np.log(softmax_outputs[range(len(softmax_outputs)), class_targets])\n",
    "# this is out loss but we want an average loss per batch to have an idea about how our model is doing\n",
    "average_loss = np.mean(neg_log)\n",
    "print(average_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "53a6ab0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "# in case of not sparse idx_target is given but hot encoded vectors are given in that case we use matrix multiplication \n",
    " \n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2], \n",
    "                            [0.1, 0.5, 0.4], \n",
    "                            [0.02, 0.9, 0.08]]) \n",
    "class_targets = np.array([[1, 0, 0], \n",
    "                          [0, 1, 0], \n",
    "                          [0, 1, 0]]) \n",
    "if(len(class_targets)==1):\n",
    "  correct_confidence = softmax_outputs[range(len(softmax_outputs)), class_targets]\n",
    "elif(len(class_targets.shape) == 2):\n",
    "  correct_confidence = np.sum(softmax_outputs*class_targets, axis =1)# what we are doing is element wise multiplication\n",
    "neg_log = -np.log(correct_confidence)\n",
    "average_loss = np.mean(neg_log)\n",
    "print(average_loss)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "0fdf8564",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[-1.00000005e-07 -1.61180957e+01 -6.93147181e-01]\n"
     ]
    }
   ],
   "source": [
    "# look there are problem if confidence in the target index is 0 because then log of 0 is -inf. so loss will become inf. on which python gives error, so we use clipping on both end of the confidence before giving it to log\n",
    "y_pred = [1,0,0.5]\n",
    "y_pred_clipped = np.clip(y_pred, 1e-7, 1-1e-7)\n",
    "print(np.log(y_pred_clipped))# look log is giving sensible values now "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "99c4fd2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#In the later chapters, we‚Äôll be adding more loss functions and some of the operations that we‚Äôll be \n",
    "#performing are common for all of them. One of these operations is how we calculate the overall \n",
    "#loss ‚Äî no matter which loss function we‚Äôll use, the overall loss is always a mean value of all \n",
    "#sample losses. Let‚Äôs create the Loss class containing the calculate method that will call our \n",
    "#loss object‚Äôs forward method and calculate the mean value of the returned sample losses:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "88c4ac50",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.38506088005216804\n"
     ]
    }
   ],
   "source": [
    "# Common loss class \n",
    "class Loss: \n",
    " \n",
    "    # Calculates the data and regularization losses \n",
    "    # given model output and ground truth values \n",
    "    def calculate(self, output, y): \n",
    " \n",
    "        # Calculate sample losses \n",
    "        sample_losses = self.forward(output, y) \n",
    " \n",
    "        # Calculate mean loss \n",
    "        data_loss = np.mean(sample_losses) \n",
    " \n",
    "        # Return loss \n",
    "        return data_loss \n",
    "# and\n",
    "# Cross-entropy loss \n",
    "class Loss_CategoricalCrossentropy(Loss): \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, y_pred, y_true): \n",
    " \n",
    "        # Number of samples in a batch \n",
    "        samples = len(y_pred) \n",
    " \n",
    "        # Clip data to prevent division by 0 \n",
    "        # Clip both sides to not drag mean towards any value \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7) \n",
    " \n",
    "        # Probabilities for target values - \n",
    "        # only if categorical labels \n",
    "        if len(y_true.shape) == 1: \n",
    "            correct_confidences = y_pred_clipped[ \n",
    "                range(samples), \n",
    "                y_true \n",
    "            ] \n",
    " \n",
    "        # Mask values - only for one-hot encoded labels \n",
    "        elif len(y_true.shape) == 2: \n",
    "            correct_confidences = np.sum( \n",
    "                y_pred_clipped * y_true, \n",
    "                axis=1 \n",
    "            ) \n",
    " \n",
    "        # Losses \n",
    "        negative_log_likelihoods = -np.log(correct_confidences) \n",
    "     \n",
    "        return negative_log_likelihoods\n",
    "softmax_outputs = np.array([[0.7, 0.1, 0.2], \n",
    "                            [0.1, 0.5, 0.4], \n",
    "                            [0.02, 0.9, 0.08]]) \n",
    "class_targets = np.array([[1, 0, 0], \n",
    "                          [0, 1, 0], \n",
    "                          [0, 1, 0]])                 \n",
    "loss_function = Loss_CategoricalCrossentropy() \n",
    "loss = loss_function.calculate(softmax_outputs, class_targets) \n",
    "print(loss) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "55fbe6fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333333 0.33333333 0.33333333]\n",
      " [0.33333333 0.33333333 0.33333333]\n",
      " [0.33333327 0.33333352 0.33333322]\n",
      " [0.33333323 0.33333362 0.33333315]\n",
      " [0.3333331  0.33333398 0.33333292]]\n",
      "1.0986157441961195\n",
      "[0 0 1 1 1]\n",
      "acc: 0.2733333333333333\n"
     ]
    }
   ],
   "source": [
    "# whole code upto here \n",
    "class Layer_Dense:\n",
    "  def __init__(self,n_inputs, n_neurons):\n",
    "    #initialize weights and biases\n",
    "    self.weights = 0.01*np.random.randn(n_inputs, n_neurons)# this np.random.randn produces gaussian with a mean of 0 and varianve of 1 \n",
    "    # multiplied by 0.01 to reduce the training time bigger number takes more time to calculate na that's why\n",
    "\n",
    "    self.biases = np.zeros((1,n_neurons))\n",
    " \n",
    "  #forward pass\n",
    "  def forward(self,inputs):\n",
    "    # calculate output values from inputs, weights and biases\n",
    "    self.output = np.dot(inputs, self.weights) + self.biases\n",
    "\n",
    "class Activation_ReLU:\n",
    "# Forward pass \n",
    "    def forward(self, inputs):\n",
    "\n",
    "    # Calculate output values from input \n",
    "      self.output = np.maximum(0, inputs) \n",
    "class Activation_Softmax:\n",
    "  def forward(self, inputs):\n",
    "    exp_values = np.exp(inputs - np.max(inputs, axis=1, keepdims=True))\n",
    "    probabilities = exp_values/np.sum(exp_values, axis=1, keepdims=True)\n",
    "    self.output = probabilities\n",
    "# Common loss class \n",
    "class Loss: \n",
    " \n",
    "    # Calculates the data and regularization losses \n",
    "    # given model output and ground truth values \n",
    "    def calculate(self, output, y): \n",
    " \n",
    "        # Calculate sample losses \n",
    "        sample_losses = self.forward(output, y) \n",
    " \n",
    "        # Calculate mean loss \n",
    "        data_loss = np.mean(sample_losses) \n",
    " \n",
    "        # Return loss \n",
    "        return data_loss \n",
    "# and\n",
    "# Cross-entropy loss \n",
    "class Loss_CategoricalCrossentropy(Loss): \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, y_pred, y_true): \n",
    " \n",
    "        # Number of samples in a batch \n",
    "        samples = len(y_pred) \n",
    " \n",
    "        # Clip data to prevent division by 0 \n",
    "        # Clip both sides to not drag mean towards any value \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7) \n",
    " \n",
    "        # Probabilities for target values - \n",
    "        # only if categorical labels \n",
    "        if len(y_true.shape) == 1: \n",
    "            correct_confidences = y_pred_clipped[ \n",
    "                range(samples), \n",
    "                y_true \n",
    "            ] \n",
    " \n",
    "        # Mask values - only for one-hot encoded labels \n",
    "        elif len(y_true.shape) == 2: \n",
    "            correct_confidences = np.sum( \n",
    "                y_pred_clipped * y_true, \n",
    "                axis=1 \n",
    "            ) \n",
    " \n",
    "        # Losses \n",
    "        negative_log_likelihoods = -np.log(correct_confidences) \n",
    "     \n",
    "        return negative_log_likelihoods\n",
    "# calculating accuracy \n",
    "class accuracy:\n",
    "   def cal(self, input, correct):\n",
    "      predictions = np.argmax(input, axis = 1)\n",
    "      if(len(correct.shape)==1):\n",
    "        Accuracy = np.mean(predictions == correct)\n",
    "      elif(len(correct.shape)==2):\n",
    "        corrected = np.argmax(correct, axis = 1)\n",
    "        Accuracy = np.mean(predictions == corrected)\n",
    "      self.out = Accuracy\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "           \n",
    "           \n",
    "# when we pass data through a model from begining to end that called forward pass\n",
    "X,Y = spiral_data(samples=100, classes=3) \n",
    "class_targets=Y\n",
    "# Create Dense layer with 2 input features and 3 output values \n",
    "dense1 = Layer_Dense(2, 3) \n",
    "dense2 = Layer_Dense(3,3)\n",
    "activation2 = Activation_Softmax()\n",
    "activation1 = Activation_ReLU()\n",
    "\n",
    "# Perform a forward pass of our training data through this layer \n",
    "dense1.forward(X) \n",
    "# Let's see output of the first few samples: \n",
    "\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "activation2.forward(dense2.output)\n",
    "softmax_outputs= activation2.output\n",
    "loss_function = Loss_CategoricalCrossentropy() \n",
    "loss = loss_function.calculate(softmax_outputs, class_targets) \n",
    "print(activation2.output[:5]) \n",
    "print(loss) \n",
    "print(np.argmax(activation2.output[:5], axis =1))\n",
    "acc = accuracy()\n",
    "acc.cal(softmax_outputs,class_targets)\n",
    "print('acc:',acc.out)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "4834a13d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 0.6666666666666666\n"
     ]
    }
   ],
   "source": [
    "#accuracy soltuion i have added the class for it above \n",
    "softmax_outputs = np.array([[0.7, 0.2, 0.1], \n",
    "                            [0.5, 0.1, 0.4], \n",
    "                            [0.02, 0.9, 0.08]]) \n",
    "# Target (ground-truth) labels for 3 samples \n",
    "class_targets = np.array([0, 1, 1]) \n",
    " \n",
    "# Calculate values along second axis (axis of index 1) \n",
    "predictions = np.argmax(softmax_outputs, axis=1) \n",
    "# If targets are one-hot encoded - convert them \n",
    "if len(class_targets.shape) == 2: \n",
    "    class_targets = np.argmax(class_targets, axis=1) \n",
    "# True evaluates to 1; False to 0 \n",
    "accuracy = np.mean(predictions==class_targets) \n",
    " \n",
    " \n",
    "print('acc:', accuracy) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4a7b892a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-0.01306527  0.01658131 -0.00118164]\n",
      " [-0.00680178  0.00666383 -0.0046072 ]]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'dense1_biases' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[34], line 29\u001b[0m\n\u001b[0;32m     27\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m iteration \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m100000\u001b[39m):\n\u001b[0;32m     28\u001b[0m   dense1\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[1;32m---> 29\u001b[0m   \u001b[43mdense1_biases\u001b[49m \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     30\u001b[0m   dense2\u001b[38;5;241m.\u001b[39mweights \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n\u001b[0;32m     31\u001b[0m   dense2_biases \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.05\u001b[39m\u001b[38;5;241m*\u001b[39mnp\u001b[38;5;241m.\u001b[39mrandom\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m3\u001b[39m)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'dense1_biases' is not defined"
     ]
    }
   ],
   "source": [
    "# now we will do optimization\n",
    "# to find the best method by which we can decrease the loss is the main difficulty of neural networks\n",
    "# randomly changing the weights and biases to check the loss is a lost case because it will take more time and is not effecient \n",
    "# but lets see once how its done\n",
    "import matplotlib.pyplot as plt \n",
    "import nnfs \n",
    "from nnfs.datasets import vertical_data \n",
    "nnfs.init()\n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    "plt.scatter(X[:, 0], X[:, 1], c=y, s=40, cmap='brg') \n",
    "plt.show()\n",
    "# create model\n",
    "dense1 = Layer_Dense(2,3)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 =Layer_Dense(3,3)\n",
    "activation2 = Activation_Softmax()\n",
    "loss_function = Loss_CategoricalCrossentropy()\n",
    "\n",
    "# helper variable\n",
    "Lowest_loss = 9999999\n",
    "best_dense1_weights = dense1.weights.copy()\n",
    "best_dense1_biases = dense1.biases.copy()\n",
    "best_dense2_weights = dense2.weights.copy()\n",
    "best_dense2_biases = dense2.biases.copy()\n",
    "print(best_dense1_weights.copy())\n",
    "\n",
    "for iteration in range(100000):\n",
    "  dense1.weights += 0.05*np.random.randn(2,3)\n",
    "  dense1_biases += 0.05*np.random.randn(1,3)\n",
    "  dense2.weights += 0.05*np.random.randn(3,3)\n",
    "  dense2_biases += 0.05*np.random.randn(1,3)\n",
    "  if(iteration == 1):\n",
    "    print(\"These are dense1 weishts in first iteration\",dense1.weights)\n",
    "\n",
    "  dense1.forward(X)\n",
    "  activation1.forward(dense1.output)\n",
    "  dense2.forward(activation1.output)\n",
    "  activation2.forward(dense2.output)\n",
    "\n",
    "  loss = loss_function.calculate(activation2.output,y)\n",
    "\n",
    "  predictions = np.argmax(activation2.output, axis = 1)\n",
    "  accuracy = np.mean(predictions == y)\n",
    "\n",
    "  if loss< Lowest_loss:\n",
    "    print('new set of weights flund, iteration:', iteration , 'loss', loss, 'acc:', accuracy)\n",
    "    Lowest_loss = loss\n",
    "    best_dense1_weights = dense1.weights.copy()\n",
    "    best_dense1_biases = dense1.biases.copy()\n",
    "    best_dense2_weights = dense2.weights.copy()\n",
    "    best_dense2_biases = dense2.biases.copy()\n",
    "    if(iteration==1):\n",
    "      print(\"this is the copy of dense1 weights\", dense1.weights.copy())\n",
    "      print(\"this is the best of dense1 weights\", best_dense1_weights)\n",
    "  else:\n",
    "    dense1.weights = best_dense1_weights.copy()\n",
    "    dense1.biases = best_dense1_biases.copy()\n",
    "    dense2.weights = best_dense2_weights.copy()\n",
    "    dense2.biases = best_dense2_biases.copy()\n",
    "    if(iteration==1):\n",
    "      print(\"this is the copy of dense1 weightsss\", dense1.weights.copy())\n",
    "      print(\"this is the best of dense1 weights\", best_dense1_weights)\n",
    "    #adding copy function is essential \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "514e288d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 2 4 6 8]\n",
      "slope 2.0\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#using that if the loss is not less the least_loss the weights of the newest loss will be copied to desn1 and dens2 and then we will perturb these weights instead of starting again completely randomly we will perturb it \n",
    "# so  i have used a else and chaged the = to += in the update weights section \n",
    "# for spiral dataset it felf miserable(42% accuracy) while for vertical dataset it acheived 91 % acuuracy\n",
    "# now we will see derivatives approach in optimization\n",
    "def f(x):\n",
    "  return 2*x\n",
    "x = np.array(range(5))\n",
    "y = f(x)\n",
    "print(y)\n",
    "plt.plot(x,y)\n",
    "plt.show\n",
    "print(\"slope\",(y[1]-y[0])/(x[1]-x[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "c5568da9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 2.0001) (8, 8.000800020000002)\n",
      "Approximate derivative for f(x) where x = 2 is 8.000199999998785\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def f(x):\n",
    "  return 2*x**2\n",
    "x = np.arange(0,5,0.001)\n",
    "y = f(x)\n",
    "plt.plot(x,y)\n",
    "p2_delta = 0.0001\n",
    "x1 = 2\n",
    "x2 = x1+p2_delta\n",
    "y1 = f(x1)\n",
    "y2 = f(x2)\n",
    "print((x1,x2),(y1,y2))\n",
    "approximate_derivative = (y2-y1)/(x2-x1)\n",
    "b = y2 - approximate_derivative*x2\n",
    "def tangent_line(x):\n",
    "  return approximate_derivative*x + b\n",
    "to_plot = [x1-0.9, x1, x1+0.9] \n",
    "plt.plot(to_plot, [tangent_line(i) for i in to_plot]) \n",
    "print('Approximate derivative for f(x)', \n",
    "f'where x = {x1} is {approximate_derivative}') \n",
    "plt.show()\n",
    "# using single derivative for each weight and biases it will take a lot of time to do differnetiation for each weights then updating the values so we use multivariate calculus instead\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "a0da3987",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3 2 6\n",
      "6.0\n",
      "6.0\n"
     ]
    }
   ],
   "source": [
    "# chapter 7 was simply based of derivatives defintion and how to do derivative of a function \n",
    "# chapter 8 was all maths about gradients partial derivatives and alsllllllllllll'100-pandas-puzzles\n",
    "x = [1,-2,3]\n",
    "w = [-3,-1,2]\n",
    "b = 1.0\n",
    "\n",
    "xw0 = x[0]*w[0]\n",
    "xw1 = x[1]*w[1]\n",
    "xw2 = x[2]*w[2]\n",
    "print(xw0,xw1,xw2)\n",
    "z = xw0 + xw1 + xw2 +b\n",
    "print(z)\n",
    "y = max(z,0)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "a965181c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n"
     ]
    }
   ],
   "source": [
    "drelu_dz = (1. if z>0 else 0.)\n",
    "print(drelu_dz)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "77f70c94",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0 1.0 1.0 1.0\n"
     ]
    }
   ],
   "source": [
    "dsum_dxw0 = 1\n",
    "dsum_dxw1 = 1\n",
    "dsum_dxw2 = 1\n",
    "dsum_db = 1\n",
    "drelu_dxw0 = drelu_dz*dsum_dxw0\n",
    "drelu_dxw1 = drelu_dz*dsum_dxw1\n",
    "drelu_dxw2 = drelu_dz*dsum_dxw2\n",
    "drelu_db = drelu_dz*dsum_db\n",
    "print(drelu_dxw0,drelu_dxw1,drelu_dxw2,drelu_db)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "dd3fc616",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-3.0 1.0 -1.0 -2.0 2.0 3.0\n"
     ]
    }
   ],
   "source": [
    "dmul_dx0 = w[0] \n",
    "dmul_dx1 = w[1] \n",
    "dmul_dx2 = w[2] \n",
    "dmul_dw0 = x[0] \n",
    "dmul_dw1 = x[1] \n",
    "dmul_dw2 = x[2] \n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0 \n",
    "drelu_dw0 = drelu_dxw0 * dmul_dw0 \n",
    "drelu_dx1 = drelu_dxw1 * dmul_dx1 \n",
    "drelu_dw1 = drelu_dxw1 * dmul_dw1 \n",
    "drelu_dx2 = drelu_dxw2 * dmul_dx2 \n",
    "drelu_dw2 = drelu_dxw2 * dmul_dw2 \n",
    "print(drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "defae805",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0\n",
      "1.0 1.0 1.0 1.0\n",
      "-3.0 1.0 -1.0 -2.0 2.0 3.0\n"
     ]
    }
   ],
   "source": [
    "x = [1.0, -2.0, 3.0]  # input values \n",
    "w = [-3.0, -1.0, 2.0]  # weights \n",
    "b = 1.0  # bias \n",
    "# Multiplying inputs by weights \n",
    "xw0 = x[0] * w[0] \n",
    "xw1 = x[1] * w[1] \n",
    "xw2 = x[2] * w[2] \n",
    "# Adding weighted inputs and a bias \n",
    "z = xw0 + xw1 + xw2 + b \n",
    "# ReLU activation function \n",
    "y = max(z, 0) \n",
    "# Backward pass \n",
    "# The derivative from the next layer \n",
    "dvalue = 1.0 \n",
    "\n",
    "# Derivative of ReLU and the chain rule \n",
    "drelu_dz = dvalue * (1. if z > 0 else 0.) \n",
    "print(drelu_dz) \n",
    "# Partial derivatives of the multiplication, the chain rule \n",
    "dsum_dxw0 = 1 \n",
    "dsum_dxw1 = 1 \n",
    "dsum_dxw2 = 1 \n",
    "dsum_db = 1 \n",
    "drelu_dxw0 = drelu_dz * dsum_dxw0 \n",
    "drelu_dxw1 = drelu_dz * dsum_dxw1 \n",
    "drelu_dxw2 = drelu_dz * dsum_dxw2 \n",
    "drelu_db = drelu_dz * dsum_db \n",
    "print(drelu_dxw0, drelu_dxw1, drelu_dxw2, drelu_db) \n",
    "# Partial derivatives of the multiplication, the chain rule \n",
    "dmul_dx0 = w[0] \n",
    "dmul_dx1 = w[1] \n",
    "dmul_dx2 = w[2] \n",
    "dmul_dw0 = x[0] \n",
    "dmul_dw1 = x[1] \n",
    "dmul_dw2 = x[2] \n",
    "drelu_dx0 = drelu_dxw0 * dmul_dx0 \n",
    "drelu_dw0 = drelu_dxw0 * dmul_dw0 \n",
    "drelu_dx1 = drelu_dxw1 * dmul_dx1 \n",
    "drelu_dw1 = drelu_dxw1 * dmul_dw1 \n",
    "drelu_dx2 = drelu_dxw2 * dmul_dx2 \n",
    "drelu_dw2 = drelu_dxw2 * dmul_dw2 \n",
    "print(drelu_dx0, drelu_dw0, drelu_dx1, drelu_dw1, drelu_dx2, drelu_dw2)\n",
    "# this is the whole code at one place remeber this derivative calculation is with respet to x0 input to connect various layers with each other \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "9649da57",
   "metadata": {},
   "outputs": [],
   "source": [
    "#we have developed all main theory for back propagation also now we will see the derivative of softmax function and cross entropy loss function\n",
    "# beofre that we should make corresponding classes \n",
    "def backward(self,dvalues):\n",
    "  self.dweights = np.dot(self.input.T, dvalues)\n",
    "  self.dbiases = np.sum(dvalues, axis=0, keepdims=True)\n",
    "  self.dinputs = np.dot(dvalues, self.weights.T)\n",
    " # Forward pass \n",
    "def forward(self, inputs): \n",
    "    # Remember input values \n",
    "    self.inputs = inputs \n",
    "    self.output = np.maximum(0, inputs) \n",
    "\n",
    "# Backward pass \n",
    "def backward(self, dvalues): \n",
    "    # Since we need to modify the original variable, \n",
    "    # let's make a copy of the values first \n",
    "    self.dinputs = dvalues.copy() \n",
    "\n",
    "    # Zero gradient where input values were negative \n",
    "    self.dinputs[self.inputs <= 0] = 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "039a9d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets do cateforical cross-entrpy loss derivative\n",
    "def backward(self, dvalues, y_true):\n",
    "  samples = len(dvalues)\n",
    "  labels = len(dvalues[0])\n",
    "  if len(y_true.shape) == 1:\n",
    "    y_true = np.eye(labels)[y_true]# one-hot encoded vectors from sparse data\n",
    "  self.dinputs = -y_true/dvalues\n",
    "  self.dinputs = self.dinputs/samples# this is gradient normalization to make the learning rate between a range\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "e57c9e6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# lets see soft max derivatice\n",
    "def backward(self, dvalues):\n",
    "  self.dinputs = np.empty_like(dvalues)\n",
    "  for index, (single_output, single_dvalues) in enumerate(zip(self.output,dvalues)):\n",
    "    single_output = single_output.reshape(-1,1)\n",
    "    jacobian_matrix = np.diagflat(single_output)- np.dot(single_output,single_output.T)\n",
    "    self.dinputs[index] = np.dot(jacobian_matrix, single_dvalues)\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "2aa4b1fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.33333334 0.33333334 0.33333334]\n",
      " [0.3333332  0.3333332  0.33333364]\n",
      " [0.3333329  0.33333293 0.3333342 ]\n",
      " [0.3333326  0.33333263 0.33333477]\n",
      " [0.33333233 0.3333324  0.33333528]]\n",
      "loss: 1.0986104\n",
      "acc: 0.34\n",
      "[[ 1.5766357e-04  7.8368583e-05  4.7324400e-05]\n",
      " [ 1.8161038e-04  1.1045573e-05 -3.3096312e-05]]\n",
      "[[-3.60553473e-04  9.66117223e-05 -1.03671395e-04]]\n",
      "[[ 5.44109462e-05  1.07411419e-04 -1.61822361e-04]\n",
      " [-4.07913431e-05 -7.16780924e-05  1.12469446e-04]\n",
      " [-5.30112993e-05  8.58172934e-05 -3.28059905e-05]]\n",
      "[[-1.0729185e-05 -9.4610732e-06  2.0027859e-05]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import nnfs \n",
    "from nnfs.datasets import spiral_data \n",
    " \n",
    "nnfs.init() \n",
    " \n",
    " \n",
    "# Dense layer \n",
    "class Layer_Dense: \n",
    " \n",
    "    # Layer initialization \n",
    "    def __init__(self, n_inputs, n_neurons): \n",
    "        # Initialize weights and biases \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons) \n",
    "        self.biases = np.zeros((1, n_neurons)) \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs): \n",
    "        # Remember input values \n",
    "        self.inputs = inputs \n",
    "        # Calculate output values from inputs, weights and biases \n",
    "        self.output = np.dot(inputs, self.weights) + self.biases \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues): \n",
    "        # Gradients on parameters \n",
    "        self.dweights = np.dot(self.inputs.T, dvalues) \n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True) \n",
    "        # Gradient on values \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T) \n",
    " \n",
    " \n",
    "# ReLU activation \n",
    "class Activation_ReLU: \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs):\n",
    " # Remember input values \n",
    "        self.inputs = inputs \n",
    "        # Calculate output values from inputs \n",
    "        self.output = np.maximum(0, inputs) \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues): \n",
    "        # Since we need to modify original variable, \n",
    "        # let's make a copy of values first \n",
    "        self.dinputs = dvalues.copy() \n",
    " \n",
    "        # Zero gradient where input values were negative \n",
    "        self.dinputs[self.inputs <= 0] = 0 \n",
    " \n",
    " \n",
    "# Softmax activation \n",
    "class Activation_Softmax: \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs): \n",
    "        # Remember input values \n",
    "        self.inputs = inputs \n",
    " \n",
    "        # Get unnormalized probabilities \n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, \n",
    "                                            keepdims=True)) \n",
    "        # Normalize them for each sample \n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, \n",
    "                                            keepdims=True) \n",
    " \n",
    "        self.output = probabilities \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues): \n",
    " \n",
    "        # Create uninitialized array \n",
    "        self.dinputs = np.empty_like(dvalues) \n",
    " \n",
    "        # Enumerate outputs and gradients \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)): \n",
    "            # Flatten output array \n",
    "            single_output = single_output.reshape(-1, 1) \n",
    "            # Calculate Jacobian matrix of the output and \n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T) \n",
    " # Calculate sample-wise gradient \n",
    "            # and add it to the array of sample gradients \n",
    "            self.dinputs[index] = np.dot(jacobian_matrix, \n",
    "                                         single_dvalues) \n",
    " \n",
    " \n",
    "# Common loss class \n",
    "class Loss: \n",
    " \n",
    "    # Calculates the data and regularization losses \n",
    "    # given model output and ground truth values \n",
    "    def calculate(self, output, y): \n",
    " \n",
    "        # Calculate sample losses \n",
    "        sample_losses = self.forward(output, y) \n",
    " \n",
    "        # Calculate mean loss \n",
    "        data_loss = np.mean(sample_losses) \n",
    " \n",
    "        # Return loss \n",
    "        return data_loss \n",
    " \n",
    " \n",
    "# Cross-entropy loss \n",
    "class Loss_CategoricalCrossentropy(Loss): \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, y_pred, y_true): \n",
    " \n",
    "        # Number of samples in a batch \n",
    "        samples = len(y_pred) \n",
    " \n",
    "        # Clip data to prevent division by 0 \n",
    "        # Clip both sides to not drag mean towards any value \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7) \n",
    " \n",
    "        # Probabilities for target values - \n",
    "        # only if categorical labels \n",
    "        if len(y_true.shape) == 1: \n",
    "            correct_confidences = y_pred_clipped[ \n",
    "                range(samples), \n",
    "                y_true \n",
    "            ] \n",
    " # Mask values - only for one-hot encoded labels \n",
    "        elif len(y_true.shape) == 2: \n",
    "            correct_confidences = np.sum( \n",
    "                y_pred_clipped * y_true, \n",
    "                axis=1 \n",
    "            ) \n",
    " \n",
    "        # Losses \n",
    "        negative_log_likelihoods = -np.log(correct_confidences) \n",
    "        return negative_log_likelihoods \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues, y_true): \n",
    " \n",
    "        # Number of samples \n",
    "        samples = len(dvalues) \n",
    "        # Number of labels in every sample \n",
    "        # We'll use the first sample to count them \n",
    "        labels = len(dvalues[0]) \n",
    " \n",
    "        # If labels are sparse, turn them into one-hot vector \n",
    "        if len(y_true.shape) == 1: \n",
    "            y_true = np.eye(labels)[y_true] \n",
    " \n",
    "        # Calculate gradient \n",
    "        self.dinputs = -y_true / dvalues \n",
    "        # Normalize gradient \n",
    "        self.dinputs = self.dinputs / samples \n",
    " \n",
    " \n",
    "# Softmax classifier - combined Softmax activation \n",
    "# and cross-entropy loss for faster backward step \n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy(): \n",
    " \n",
    "    # Creates activation and loss function objects \n",
    "    def __init__(self): \n",
    "        self.activation = Activation_Softmax() \n",
    "        self.loss = Loss_CategoricalCrossentropy() \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs, y_true): \n",
    "        # Output layer's activation function \n",
    "        self.activation.forward(inputs) \n",
    "        # Set the output \n",
    "        self.output = self.activation.output \n",
    "        # Calculate and return loss value \n",
    "        return self.loss.calculate(self.output, y_true) \n",
    "\n",
    "  # Backward pass \n",
    "    def backward(self, dvalues, y_true): \n",
    " \n",
    "        # Number of samples \n",
    "        samples = len(dvalues) \n",
    " \n",
    "        # If labels are one-hot encoded, \n",
    "        # turn them into discrete values \n",
    "        if len(y_true.shape) == 2: \n",
    "            y_true = np.argmax(y_true, axis=1) \n",
    " \n",
    "        # Copy so we can safely modify \n",
    "        self.dinputs = dvalues.copy() \n",
    "        # Calculate gradient \n",
    "        self.dinputs[range(samples), y_true] -= 1 \n",
    "        # Normalize gradient \n",
    "        self.dinputs = self.dinputs / samples \n",
    " \n",
    " \n",
    "# Create dataset \n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    " \n",
    "# Create Dense layer with 2 input features and 3 output values \n",
    "dense1 = Layer_Dense(2, 3) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    " \n",
    "# Create second Dense layer with 3 input features (as we take output \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(3, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Perform a forward pass of our training data through this layer \n",
    "dense1.forward(X) \n",
    " \n",
    "# Perform a forward pass through activation function \n",
    "# takes the output of first dense layer here \n",
    "activation1.forward(dense1.output) \n",
    " \n",
    "# Perform a forward pass through second Dense layer \n",
    "# takes outputs of activation function of first layer as inputs \n",
    "dense2.forward(activation1.output) \n",
    " \n",
    "# Perform a forward pass through the activation/loss function \n",
    "# takes the output of second dense layer here and returns loss \n",
    "loss = loss_activation.forward(dense2.output, y)\n",
    "# Let's see output of the first few samples: \n",
    "print(loss_activation.output[:5]) \n",
    "# Print loss value \n",
    "print('loss:', loss) \n",
    "# Calculate accuracy from output of activation2 and targets \n",
    "# calculate values along first axis \n",
    "predictions = np.argmax(loss_activation.output, axis=1) \n",
    "if len(y.shape) == 2: \n",
    "   y = np.argmax(y, axis=1) \n",
    "accuracy = np.mean(predictions==y) \n",
    "# Print accuracy \n",
    "print('acc:', accuracy) \n",
    "# Backward pass \n",
    "loss_activation.backward(loss_activation.output, y) \n",
    "dense2.backward(loss_activation.dinputs) \n",
    "activation1.backward(dense2.dinputs) \n",
    "dense1.backward(activation1.dinputs) \n",
    "# Print gradients \n",
    "print(dense1.dweights) \n",
    "print(dense1.dbiases) \n",
    "print(dense2.dweights) \n",
    "print(dense2.dbiases) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "fcc386f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will start chapter 10 optimezers\n",
    "# stochastic gradient descent(SGD) is the most common way to decrease the loss\n",
    "# lets create a SGD class code\n",
    "class Optimizer_SGD:\n",
    "  def __init__(self, learning_rate=3):\n",
    "    self.learning_rate= learning_rate\n",
    "  def update_params(self, Layer):\n",
    "    Layer.weights += -self.learning_rate*Layer.dweights# currently they are not specified but later we will use them\n",
    "    Layer.biases += -self.learning_rate*Layer.dbiases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "2d4d79e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.347, loss: 1.099\n",
      "epoch: 100, acc: 0.403, loss: 1.080\n",
      "epoch: 200, acc: 0.387, loss: 1.073\n",
      "epoch: 300, acc: 0.403, loss: 1.072\n",
      "epoch: 400, acc: 0.410, loss: 1.071\n",
      "epoch: 500, acc: 0.417, loss: 1.070\n",
      "epoch: 600, acc: 0.417, loss: 1.069\n",
      "epoch: 700, acc: 0.420, loss: 1.067\n",
      "epoch: 800, acc: 0.417, loss: 1.065\n",
      "epoch: 900, acc: 0.450, loss: 1.060\n",
      "epoch: 1000, acc: 0.483, loss: 1.050\n",
      "epoch: 1100, acc: 0.457, loss: 1.039\n",
      "epoch: 1200, acc: 0.490, loss: 1.026\n",
      "epoch: 1300, acc: 0.497, loss: 1.018\n",
      "epoch: 1400, acc: 0.517, loss: 1.008\n",
      "epoch: 1500, acc: 0.523, loss: 1.001\n",
      "epoch: 1600, acc: 0.543, loss: 0.993\n",
      "epoch: 1700, acc: 0.550, loss: 0.985\n",
      "epoch: 1800, acc: 0.573, loss: 0.977\n",
      "epoch: 1900, acc: 0.567, loss: 0.973\n",
      "epoch: 2000, acc: 0.567, loss: 0.967\n",
      "epoch: 2100, acc: 0.617, loss: 0.941\n",
      "epoch: 2200, acc: 0.580, loss: 0.970\n",
      "epoch: 2300, acc: 0.633, loss: 0.929\n",
      "epoch: 2400, acc: 0.627, loss: 0.925\n",
      "epoch: 2500, acc: 0.643, loss: 0.920\n",
      "epoch: 2600, acc: 0.633, loss: 0.929\n",
      "epoch: 2700, acc: 0.610, loss: 0.936\n",
      "epoch: 2800, acc: 0.597, loss: 0.954\n",
      "epoch: 2900, acc: 0.627, loss: 0.925\n",
      "epoch: 3000, acc: 0.637, loss: 0.912\n",
      "epoch: 3100, acc: 0.627, loss: 0.919\n",
      "epoch: 3200, acc: 0.637, loss: 0.914\n",
      "epoch: 3300, acc: 0.637, loss: 0.917\n",
      "epoch: 3400, acc: 0.660, loss: 0.893\n",
      "epoch: 3500, acc: 0.640, loss: 0.909\n",
      "epoch: 3600, acc: 0.637, loss: 0.903\n",
      "epoch: 3700, acc: 0.640, loss: 0.905\n",
      "epoch: 3800, acc: 0.627, loss: 0.909\n",
      "epoch: 3900, acc: 0.660, loss: 0.893\n",
      "epoch: 4000, acc: 0.640, loss: 0.910\n",
      "epoch: 4100, acc: 0.667, loss: 0.883\n",
      "epoch: 4200, acc: 0.660, loss: 0.885\n",
      "epoch: 4300, acc: 0.663, loss: 0.888\n",
      "epoch: 4400, acc: 0.653, loss: 0.891\n",
      "epoch: 4500, acc: 0.647, loss: 0.901\n",
      "epoch: 4600, acc: 0.653, loss: 0.895\n",
      "epoch: 4700, acc: 0.683, loss: 0.869\n",
      "epoch: 4800, acc: 0.673, loss: 0.877\n",
      "epoch: 4900, acc: 0.673, loss: 0.875\n",
      "epoch: 5000, acc: 0.677, loss: 0.883\n",
      "epoch: 5100, acc: 0.667, loss: 0.880\n",
      "epoch: 5200, acc: 0.667, loss: 0.880\n",
      "epoch: 5300, acc: 0.673, loss: 0.876\n",
      "epoch: 5400, acc: 0.643, loss: 0.914\n",
      "epoch: 5500, acc: 0.670, loss: 0.882\n",
      "epoch: 5600, acc: 0.687, loss: 0.869\n",
      "epoch: 5700, acc: 0.673, loss: 0.877\n",
      "epoch: 5800, acc: 0.677, loss: 0.876\n",
      "epoch: 5900, acc: 0.677, loss: 0.875\n",
      "epoch: 6000, acc: 0.673, loss: 0.874\n",
      "epoch: 6100, acc: 0.683, loss: 0.866\n",
      "epoch: 6200, acc: 0.687, loss: 0.866\n",
      "epoch: 6300, acc: 0.680, loss: 0.874\n",
      "epoch: 6400, acc: 0.683, loss: 0.866\n",
      "epoch: 6500, acc: 0.690, loss: 0.867\n",
      "epoch: 6600, acc: 0.690, loss: 0.866\n",
      "epoch: 6700, acc: 0.683, loss: 0.866\n",
      "epoch: 6800, acc: 0.677, loss: 0.876\n",
      "epoch: 6900, acc: 0.683, loss: 0.870\n",
      "epoch: 7000, acc: 0.687, loss: 0.865\n",
      "epoch: 7100, acc: 0.693, loss: 0.862\n",
      "epoch: 7200, acc: 0.690, loss: 0.866\n",
      "epoch: 7300, acc: 0.693, loss: 0.862\n",
      "epoch: 7400, acc: 0.673, loss: 0.881\n",
      "epoch: 7500, acc: 0.690, loss: 0.862\n",
      "epoch: 7600, acc: 0.693, loss: 0.863\n",
      "epoch: 7700, acc: 0.693, loss: 0.857\n",
      "epoch: 7800, acc: 0.697, loss: 0.858\n",
      "epoch: 7900, acc: 0.683, loss: 0.871\n",
      "epoch: 8000, acc: 0.693, loss: 0.860\n",
      "epoch: 8100, acc: 0.697, loss: 0.857\n",
      "epoch: 8200, acc: 0.687, loss: 0.865\n",
      "epoch: 8300, acc: 0.697, loss: 0.860\n",
      "epoch: 8400, acc: 0.693, loss: 0.855\n",
      "epoch: 8500, acc: 0.697, loss: 0.857\n",
      "epoch: 8600, acc: 0.687, loss: 0.868\n",
      "epoch: 8700, acc: 0.693, loss: 0.856\n",
      "epoch: 8800, acc: 0.687, loss: 0.864\n",
      "epoch: 8900, acc: 0.687, loss: 0.861\n",
      "epoch: 9000, acc: 0.693, loss: 0.857\n",
      "epoch: 9100, acc: 0.687, loss: 0.867\n",
      "epoch: 9200, acc: 0.687, loss: 0.866\n",
      "epoch: 9300, acc: 0.700, loss: 0.851\n",
      "epoch: 9400, acc: 0.700, loss: 0.851\n",
      "epoch: 9500, acc: 0.670, loss: 0.879\n",
      "epoch: 9600, acc: 0.690, loss: 0.861\n",
      "epoch: 9700, acc: 0.683, loss: 0.868\n",
      "epoch: 9800, acc: 0.697, loss: 0.853\n",
      "epoch: 9900, acc: 0.693, loss: 0.855\n",
      "epoch: 10000, acc: 0.693, loss: 0.855\n"
     ]
    }
   ],
   "source": [
    "# final model\n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    " \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    "activation2 = Activation_Softmax()\n",
    "# Create second Dense layer with 64 input features (as we take output  \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "optimizer = Optimizer_SGD() \n",
    " \n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    "    activation2.forward(dense2.output)\n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(activation2.output, y) \n",
    " \n",
    "  \n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}') \n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    activation2.backward(loss_activation.dinputs)\n",
    "    dense2.backward(activation2.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " # i noticed that softmax was not added in this so i made activation 2 and also applied it here but now its taking a lot more time, a lot more time.\n",
    "    # Update weights and biases \n",
    "    optimizer.update_params(dense1) \n",
    "    optimizer.update_params(dense2) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "40782eda",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we learnt how setting up differnt learning rate effect the accuracy and loss overall so read book for more , so now we are using a decay learnign rate approach,\n",
    "# this approach in starting have high L.R. but then we slowly decrease it to get a fix value and minimum value , not causing overshooting and stagnation both.\n",
    "\n",
    "class Optimizer_SGD: \n",
    " \n",
    "    # Initialize optimizer - set settings, \n",
    "    # learning rate of 1. is default for this optimizer \n",
    "    def __init__(self, learning_rate=2., decay=0.0001): \n",
    "        self.learning_rate = learning_rate \n",
    "        self.current_learning_rate = learning_rate \n",
    "        self.decay = decay \n",
    "        self.iterations = 0 \n",
    " \n",
    "    # Call once before any parameter updates \n",
    "    def pre_update_params(self): \n",
    "        if self.decay: \n",
    "            self.current_learning_rate = self.learning_rate *   (1. / (1. + self.decay * self.iterations)) \n",
    " \n",
    "    # Update parameters \n",
    "    def update_params(self, layer): \n",
    "        layer.weights += -self.current_learning_rate * layer.dweights \n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases \n",
    " \n",
    "    # Call once after any parameter updates \n",
    "    def post_update_params(self): \n",
    "        self.iterations += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "e5e39fd4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.367, loss: 1.099, currect_learning_rate: 2.0\n",
      "epoch: 100, acc: 0.407, loss: 1.093, currect_learning_rate: 1.9803940984255866\n",
      "epoch: 200, acc: 0.417, loss: 1.073, currect_learning_rate: 1.9609765663300323\n",
      "epoch: 300, acc: 0.413, loss: 1.070, currect_learning_rate: 1.941936110301971\n",
      "epoch: 400, acc: 0.410, loss: 1.069, currect_learning_rate: 1.9232618521011635\n",
      "epoch: 500, acc: 0.417, loss: 1.069, currect_learning_rate: 1.9049433279359937\n",
      "epoch: 600, acc: 0.420, loss: 1.068, currect_learning_rate: 1.8869704689121614\n",
      "epoch: 700, acc: 0.420, loss: 1.068, currect_learning_rate: 1.8693335825778108\n",
      "epoch: 800, acc: 0.417, loss: 1.067, currect_learning_rate: 1.852023335494027\n",
      "epoch: 900, acc: 0.413, loss: 1.066, currect_learning_rate: 1.8350307367648406\n",
      "epoch: 1000, acc: 0.410, loss: 1.065, currect_learning_rate: 1.8183471224656784\n",
      "epoch: 1100, acc: 0.410, loss: 1.064, currect_learning_rate: 1.8019641409135956\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[49], line 55\u001b[0m\n\u001b[0;32m     53\u001b[0m \u001b[38;5;66;03m# Backward pass \u001b[39;00m\n\u001b[0;32m     54\u001b[0m loss_activation\u001b[38;5;241m.\u001b[39mbackward(loss_activation\u001b[38;5;241m.\u001b[39moutput, y)\n\u001b[1;32m---> 55\u001b[0m \u001b[43mactivation2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_activation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdinputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     56\u001b[0m dense2\u001b[38;5;241m.\u001b[39mbackward(activation2\u001b[38;5;241m.\u001b[39mdinputs) \n\u001b[0;32m     57\u001b[0m activation1\u001b[38;5;241m.\u001b[39mbackward(dense2\u001b[38;5;241m.\u001b[39mdinputs) \n",
      "Cell \u001b[1;32mIn[45], line 81\u001b[0m, in \u001b[0;36mActivation_Softmax.backward\u001b[1;34m(self, dvalues)\u001b[0m\n\u001b[0;32m     79\u001b[0m            single_output \u001b[38;5;241m=\u001b[39m single_output\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1\u001b[39m) \n\u001b[0;32m     80\u001b[0m            \u001b[38;5;66;03m# Calculate Jacobian matrix of the output and \u001b[39;00m\n\u001b[1;32m---> 81\u001b[0m            jacobian_matrix \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdiagflat\u001b[49m\u001b[43m(\u001b[49m\u001b[43msingle_output\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(single_output, single_output\u001b[38;5;241m.\u001b[39mT) \n\u001b[0;32m     82\u001b[0m \u001b[38;5;66;03m# Calculate sample-wise gradient \u001b[39;00m\n\u001b[0;32m     83\u001b[0m            \u001b[38;5;66;03m# and add it to the array of sample gradients \u001b[39;00m\n\u001b[0;32m     84\u001b[0m            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdinputs[index] \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(jacobian_matrix, \n\u001b[0;32m     85\u001b[0m                                         single_dvalues)\n",
      "File \u001b[1;32mc:\\Users\\binda\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\lib\\twodim_base.py:355\u001b[0m, in \u001b[0;36mdiagflat\u001b[1;34m(v, k)\u001b[0m\n\u001b[0;32m    353\u001b[0m res \u001b[38;5;241m=\u001b[39m zeros((n, n), v\u001b[38;5;241m.\u001b[39mdtype)\n\u001b[0;32m    354\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (k \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m):\n\u001b[1;32m--> 355\u001b[0m     i \u001b[38;5;241m=\u001b[39m \u001b[43marange\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mintp\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    356\u001b[0m     fi \u001b[38;5;241m=\u001b[39m i\u001b[38;5;241m+\u001b[39mk\u001b[38;5;241m+\u001b[39mi\u001b[38;5;241m*\u001b[39mn\n\u001b[0;32m    357\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "#again writing the code with decay rate and seeing how it performs\n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    " \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    "activation2 = Activation_Softmax()\n",
    "# Create second Dense layer with 64 input features (as we take output  \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "optimizer = Optimizer_SGD() \n",
    " \n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    "    activation2.forward(dense2.output)\n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(activation2.output, y) \n",
    " \n",
    "  \n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'currect_learning_rate: {optimizer.current_learning_rate}')\n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    activation2.backward(loss_activation.dinputs)\n",
    "    dense2.backward(activation2.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " # i noticed that softmax was not added in this so i made activation 2 and also applied it here but now its taking a lot more time, a lot more time.\n",
    "    # Update weights and biases \n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1) \n",
    "    optimizer.update_params(dense2) \n",
    "    optimizer.post_update_params()\n",
    "    # its too slow may be due to soft max function calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "392f707f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets add momentum also for theory go to book pl.\n",
    "# adding momentum will update my update_params function so lets update tha \n",
    "def update_params(self,Layer):\n",
    "  if self.momentum:\n",
    "    if not hasattr(Layer,'weight_momentums'):\n",
    "      Layer.weight_momentums = np.zeros_like(Layer.weights)\n",
    "      Layer.bias_momentums = np.zeros_like(Layer.biases)\n",
    "    \n",
    "\n",
    "    weight_updates = self.momentum*Layer.weight_momentum-self.current_learning_rate* Layer.dweights\n",
    "    Layer.weight_momentum = weight_updates\n",
    "    bias_updates = self.momentum*Layer.bias_momentum-self.current_learning_rate* Layer.dbiases\n",
    "    Layer.bias_momentum = bias_updates\n",
    "  else:\n",
    "    weight_updates = -self.current_learning_rate*Layer.dweights\n",
    "    bias_updates = -self.current_learning_rate*Layer.dbiases\n",
    "  Layer.weights += weight_updates\n",
    "  Layer.biases += bias_updates\n",
    "  # the new parameter to difne is momentum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "2c9753be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now lets chck the code \n",
    "# SGD optimizer \n",
    "class Optimizer_SGD: \n",
    " \n",
    "    # Initialize optimizer - set settings,  \n",
    "    # learning rate of 1. is default for this optimizer \n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.): \n",
    "        self.learning_rate = learning_rate \n",
    "        self.current_learning_rate = learning_rate \n",
    "        self.decay = decay \n",
    "        self.iterations = 0 \n",
    "        self.momentum = momentum \n",
    " \n",
    "    # Call once before any parameter updates \n",
    "    def pre_update_params(self): \n",
    "        if self.decay: \n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations)) \n",
    " \n",
    "    # Update parameters \n",
    "    def update_params(self, layer): \n",
    " \n",
    "        # If we use momentum \n",
    "        if self.momentum: \n",
    " \n",
    "            # If layer does not contain momentum arrays, create them \n",
    "            # filled with zeros \n",
    "            if not hasattr(layer, 'weight_momentums'): \n",
    "                layer.weight_momentums = np.zeros_like(layer.weights) \n",
    "                # If there is no momentum array for weights \n",
    "                # The array doesn't exist for biases yet either. \n",
    "                layer.bias_momentums = np.zeros_like(layer.biases) \n",
    " \n",
    "            # Build weight updates with momentum - take previous \n",
    "            # updates multiplied by retain factor and update with \n",
    "            # current gradients \n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights \n",
    "            layer.weight_momentums = weight_updates \n",
    " \n",
    "            # Build bias updates \n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases \n",
    "            layer.bias_momentums = bias_updates \n",
    " \n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update) \n",
    "        else: \n",
    "            weight_updates = -self.current_learning_rate * layer.dweights \n",
    "            bias_updates = -self.current_learning_rate *layer.dbiases \n",
    " \n",
    "        # Update weights and biases using either \n",
    "        # vanilla or momentum updates \n",
    "        layer.weights += weight_updates \n",
    "        layer.biases += bias_updates \n",
    " \n",
    "    # Call once after any parameter updates \n",
    "    def post_update_params(self): \n",
    "        self.iterations += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "114bf28b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.397, loss: 1.099, currect_learning_rate: 1.0\n",
      "epoch: 100, acc: 0.457, loss: 1.071, currect_learning_rate: 0.9099181073703367\n",
      "epoch: 200, acc: 0.447, loss: 1.068, currect_learning_rate: 0.8340283569641367\n",
      "epoch: 300, acc: 0.460, loss: 1.066, currect_learning_rate: 0.7698229407236336\n",
      "epoch: 400, acc: 0.467, loss: 1.063, currect_learning_rate: 0.7147962830593281\n",
      "epoch: 500, acc: 0.460, loss: 1.057, currect_learning_rate: 0.66711140760507\n",
      "epoch: 600, acc: 0.503, loss: 1.042, currect_learning_rate: 0.6253908692933083\n",
      "epoch: 700, acc: 0.520, loss: 1.025, currect_learning_rate: 0.5885815185403178\n",
      "epoch: 800, acc: 0.540, loss: 1.015, currect_learning_rate: 0.5558643690939411\n",
      "epoch: 900, acc: 0.550, loss: 1.008, currect_learning_rate: 0.526592943654555\n",
      "epoch: 1000, acc: 0.543, loss: 1.002, currect_learning_rate: 0.5002501250625312\n",
      "epoch: 1100, acc: 0.557, loss: 0.996, currect_learning_rate: 0.4764173415912339\n",
      "epoch: 1200, acc: 0.570, loss: 0.992, currect_learning_rate: 0.45475216007276037\n",
      "epoch: 1300, acc: 0.590, loss: 0.987, currect_learning_rate: 0.43497172683775553\n",
      "epoch: 1400, acc: 0.603, loss: 0.982, currect_learning_rate: 0.4168403501458941\n",
      "epoch: 1500, acc: 0.593, loss: 0.974, currect_learning_rate: 0.4001600640256102\n",
      "epoch: 1600, acc: 0.613, loss: 0.967, currect_learning_rate: 0.3847633705271258\n",
      "epoch: 1700, acc: 0.613, loss: 0.958, currect_learning_rate: 0.3705075954057058\n",
      "epoch: 1800, acc: 0.647, loss: 0.946, currect_learning_rate: 0.35727045373347627\n",
      "epoch: 1900, acc: 0.653, loss: 0.935, currect_learning_rate: 0.3449465332873405\n",
      "epoch: 2000, acc: 0.670, loss: 0.923, currect_learning_rate: 0.33344448149383127\n",
      "epoch: 2100, acc: 0.667, loss: 0.911, currect_learning_rate: 0.32268473701193934\n",
      "epoch: 2200, acc: 0.690, loss: 0.900, currect_learning_rate: 0.31259768677711786\n",
      "epoch: 2300, acc: 0.703, loss: 0.891, currect_learning_rate: 0.3031221582297666\n",
      "epoch: 2400, acc: 0.713, loss: 0.883, currect_learning_rate: 0.29420417769932333\n",
      "epoch: 2500, acc: 0.720, loss: 0.876, currect_learning_rate: 0.2857959416976279\n",
      "epoch: 2600, acc: 0.730, loss: 0.869, currect_learning_rate: 0.2778549597110308\n",
      "epoch: 2700, acc: 0.737, loss: 0.864, currect_learning_rate: 0.2703433360367667\n",
      "epoch: 2800, acc: 0.730, loss: 0.859, currect_learning_rate: 0.26322716504343247\n",
      "epoch: 2900, acc: 0.737, loss: 0.856, currect_learning_rate: 0.25647601949217746\n",
      "epoch: 3000, acc: 0.737, loss: 0.852, currect_learning_rate: 0.25006251562890724\n",
      "epoch: 3100, acc: 0.743, loss: 0.849, currect_learning_rate: 0.2439619419370578\n",
      "epoch: 3200, acc: 0.743, loss: 0.847, currect_learning_rate: 0.23815194093831865\n",
      "epoch: 3300, acc: 0.750, loss: 0.844, currect_learning_rate: 0.23261223540358225\n",
      "epoch: 3400, acc: 0.753, loss: 0.842, currect_learning_rate: 0.22732439190725165\n",
      "epoch: 3500, acc: 0.747, loss: 0.840, currect_learning_rate: 0.22227161591464767\n",
      "epoch: 3600, acc: 0.750, loss: 0.838, currect_learning_rate: 0.21743857360295715\n",
      "epoch: 3700, acc: 0.750, loss: 0.836, currect_learning_rate: 0.21281123643328367\n",
      "epoch: 3800, acc: 0.753, loss: 0.835, currect_learning_rate: 0.20837674515524068\n",
      "epoch: 3900, acc: 0.753, loss: 0.833, currect_learning_rate: 0.20412329046744235\n",
      "epoch: 4000, acc: 0.753, loss: 0.832, currect_learning_rate: 0.2000400080016003\n",
      "epoch: 4100, acc: 0.753, loss: 0.831, currect_learning_rate: 0.19611688566385566\n",
      "epoch: 4200, acc: 0.753, loss: 0.830, currect_learning_rate: 0.19234468166955185\n",
      "epoch: 4300, acc: 0.753, loss: 0.829, currect_learning_rate: 0.18871485185884126\n",
      "epoch: 4400, acc: 0.750, loss: 0.828, currect_learning_rate: 0.18521948508983144\n",
      "epoch: 4500, acc: 0.750, loss: 0.827, currect_learning_rate: 0.18185124568103292\n",
      "epoch: 4600, acc: 0.750, loss: 0.826, currect_learning_rate: 0.1786033220217896\n",
      "epoch: 4700, acc: 0.750, loss: 0.825, currect_learning_rate: 0.1754693805930865\n",
      "epoch: 4800, acc: 0.750, loss: 0.824, currect_learning_rate: 0.17244352474564578\n",
      "epoch: 4900, acc: 0.750, loss: 0.824, currect_learning_rate: 0.16952025767079165\n",
      "epoch: 5000, acc: 0.750, loss: 0.823, currect_learning_rate: 0.16669444907484582\n",
      "epoch: 5100, acc: 0.750, loss: 0.822, currect_learning_rate: 0.16396130513198884\n",
      "epoch: 5200, acc: 0.750, loss: 0.822, currect_learning_rate: 0.16131634134537828\n",
      "epoch: 5300, acc: 0.750, loss: 0.821, currect_learning_rate: 0.15875535799333226\n",
      "epoch: 5400, acc: 0.750, loss: 0.821, currect_learning_rate: 0.1562744178777934\n",
      "epoch: 5500, acc: 0.750, loss: 0.820, currect_learning_rate: 0.15386982612709646\n",
      "epoch: 5600, acc: 0.750, loss: 0.819, currect_learning_rate: 0.15153811183512653\n",
      "epoch: 5700, acc: 0.750, loss: 0.819, currect_learning_rate: 0.14927601134497687\n",
      "epoch: 5800, acc: 0.750, loss: 0.818, currect_learning_rate: 0.14708045300779526\n",
      "epoch: 5900, acc: 0.750, loss: 0.818, currect_learning_rate: 0.14494854326714016\n",
      "epoch: 6000, acc: 0.750, loss: 0.817, currect_learning_rate: 0.1428775539362766\n",
      "epoch: 6100, acc: 0.750, loss: 0.817, currect_learning_rate: 0.1408649105507818\n",
      "epoch: 6200, acc: 0.750, loss: 0.817, currect_learning_rate: 0.13890818169190167\n",
      "epoch: 6300, acc: 0.750, loss: 0.816, currect_learning_rate: 0.13700506918755992\n",
      "epoch: 6400, acc: 0.750, loss: 0.816, currect_learning_rate: 0.13515339910798757\n",
      "epoch: 6500, acc: 0.750, loss: 0.815, currect_learning_rate: 0.13335111348179757\n",
      "epoch: 6600, acc: 0.750, loss: 0.815, currect_learning_rate: 0.13159626266614027\n",
      "epoch: 6700, acc: 0.750, loss: 0.815, currect_learning_rate: 0.12988699831146902\n",
      "epoch: 6800, acc: 0.750, loss: 0.814, currect_learning_rate: 0.12822156686754713\n",
      "epoch: 6900, acc: 0.750, loss: 0.814, currect_learning_rate: 0.126598303582732\n",
      "epoch: 7000, acc: 0.750, loss: 0.814, currect_learning_rate: 0.12501562695336915\n",
      "epoch: 7100, acc: 0.750, loss: 0.813, currect_learning_rate: 0.12347203358439313\n",
      "epoch: 7200, acc: 0.750, loss: 0.813, currect_learning_rate: 0.12196609342602757\n",
      "epoch: 7300, acc: 0.750, loss: 0.813, currect_learning_rate: 0.12049644535486204\n",
      "epoch: 7400, acc: 0.750, loss: 0.812, currect_learning_rate: 0.11906179307060363\n",
      "epoch: 7500, acc: 0.750, loss: 0.812, currect_learning_rate: 0.11766090128250381\n",
      "epoch: 7600, acc: 0.750, loss: 0.812, currect_learning_rate: 0.11629259216187929\n",
      "epoch: 7700, acc: 0.750, loss: 0.812, currect_learning_rate: 0.11495574203931487\n",
      "epoch: 7800, acc: 0.750, loss: 0.812, currect_learning_rate: 0.11364927832708263\n",
      "epoch: 7900, acc: 0.750, loss: 0.811, currect_learning_rate: 0.11237217664906168\n",
      "epoch: 8000, acc: 0.750, loss: 0.811, currect_learning_rate: 0.11112345816201799\n",
      "epoch: 8100, acc: 0.750, loss: 0.811, currect_learning_rate: 0.10990218705352237\n",
      "epoch: 8200, acc: 0.750, loss: 0.811, currect_learning_rate: 0.10870746820306555\n",
      "epoch: 8300, acc: 0.750, loss: 0.810, currect_learning_rate: 0.1075384449940854\n",
      "epoch: 8400, acc: 0.750, loss: 0.810, currect_learning_rate: 0.10639429726566654\n",
      "epoch: 8500, acc: 0.750, loss: 0.810, currect_learning_rate: 0.10527423939362038\n",
      "epoch: 8600, acc: 0.750, loss: 0.810, currect_learning_rate: 0.10417751849150952\n",
      "epoch: 8700, acc: 0.750, loss: 0.810, currect_learning_rate: 0.10310341272296113\n",
      "epoch: 8800, acc: 0.750, loss: 0.810, currect_learning_rate: 0.1020512297173181\n",
      "epoch: 8900, acc: 0.750, loss: 0.809, currect_learning_rate: 0.10102030508132134\n",
      "epoch: 9000, acc: 0.750, loss: 0.809, currect_learning_rate: 0.1000100010001\n",
      "epoch: 9100, acc: 0.750, loss: 0.809, currect_learning_rate: 0.09901970492127933\n",
      "epoch: 9200, acc: 0.750, loss: 0.809, currect_learning_rate: 0.09804882831650162\n",
      "epoch: 9300, acc: 0.750, loss: 0.809, currect_learning_rate: 0.09709680551509856\n",
      "epoch: 9400, acc: 0.750, loss: 0.809, currect_learning_rate: 0.09616309260505818\n",
      "epoch: 9500, acc: 0.750, loss: 0.808, currect_learning_rate: 0.09524716639679968\n",
      "epoch: 9600, acc: 0.750, loss: 0.808, currect_learning_rate: 0.09434852344560807\n",
      "epoch: 9700, acc: 0.750, loss: 0.808, currect_learning_rate: 0.09346667912889055\n",
      "epoch: 9800, acc: 0.750, loss: 0.808, currect_learning_rate: 0.09260116677470137\n",
      "epoch: 9900, acc: 0.750, loss: 0.808, currect_learning_rate: 0.09175153683824203\n",
      "epoch: 10000, acc: 0.750, loss: 0.808, currect_learning_rate: 0.09091735612328393\n"
     ]
    }
   ],
   "source": [
    "#now lets check the result and accuracy\n",
    "#again writing the code with decay rate and seeing how it performs\n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    " \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    "activation2 = Activation_Softmax()\n",
    "# Create second Dense layer with 64 input features (as we take output  \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "optimizer = Optimizer_SGD(decay=1e-3,momentum=0.9) \n",
    " \n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    "    activation2.forward(dense2.output)\n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(activation2.output, y) \n",
    "    # loss = loss_activation.forward(dense2.output, y)\n",
    "  \n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'currect_learning_rate: {optimizer.current_learning_rate}')\n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    activation2.backward(loss_activation.dinputs)\n",
    "    dense2.backward(activation2.dinputs) \n",
    "    # dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " # i noticed that softmax was not added in this so i made activation 2 and also applied it here but now its taking a lot more time, a lot more time.\n",
    "    # Update weights and biases \n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1) \n",
    "    optimizer.update_params(dense2) \n",
    "    optimizer.post_update_params()\n",
    "    # on removing the softmax its too fast also accuracy is good(around 96% that i got) but after removing soft_max its not good\n",
    "# in this it seems luch also affect first few try i got 50 % accuracy wothout softmax and then suddeny after few try then i got 96 %üòÄü•π"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "38d214fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets go thorugh the adagard optimizer\n",
    "# so what it does is just decrease the learning rate for every counting loss it gets it stores that in chache and add a epsilon so that it dont go to zero \n",
    "# its not useful most of time because of stagnation problem as learning rate decreases whatever the lowest point is it takes a lot of time and also stagnate at most of cases \n",
    "# but at some cases where the given data have two properties one is dense and other is sparse then we use this technique to give low learnign rate to the dense one because it alread have a big influence of itself \n",
    "# lets see its code\n",
    "class Optimizer_Adagrad: \n",
    " \n",
    "    # Initialize optimizer - set settings \n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7): \n",
    "        self.learning_rate = learning_rate \n",
    "        self.current_learning_rate = learning_rate \n",
    "        self.decay = decay \n",
    "        self.iterations = 0 \n",
    "        self.epsilon = epsilon \n",
    " \n",
    "    # Call once before any parameter updates \n",
    "    def pre_update_params(self): \n",
    "        if self.decay: \n",
    "            self.current_learning_rate = self.learning_rate *   (1. / (1. + self.decay * self.iterations)) \n",
    " \n",
    "\n",
    "    # Update parameters \n",
    "    def update_params(self, layer): \n",
    " \n",
    "        # If layer does not contain cache arrays, \n",
    "        # create them filled with zeros \n",
    "        if not hasattr(layer, 'weight_cache'): \n",
    "            layer.weight_cache = np.zeros_like(layer.weights) \n",
    "            layer.bias_cache = np.zeros_like(layer.biases) \n",
    " \n",
    "        # Update cache with squared current gradients \n",
    "        layer.weight_cache += layer.dweights**2 \n",
    "        layer.bias_cache += layer.dbiases**2 \n",
    " \n",
    "        # Vanilla SGD parameter update + normalization \n",
    "        # with square rooted cache \n",
    "        layer.weights += -self.current_learning_rate *layer.dweights /(np.sqrt(layer.weight_cache) + self.epsilon) \n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases /(np.sqrt(layer.bias_cache) + self.epsilon) \n",
    " \n",
    "    # Call once after any parameter updates \n",
    "    def post_update_params(self): \n",
    "        self.iterations += 1 \n",
    " \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "id": "8f3c170b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.343, loss: 1.099, lr: 1.0\n",
      "epoch: 100, acc: 0.487, loss: 0.984, lr: 0.999990100098009\n",
      "epoch: 200, acc: 0.480, loss: 0.947, lr: 0.999980100396002\n",
      "epoch: 300, acc: 0.497, loss: 0.910, lr: 0.9999701008939833\n",
      "epoch: 400, acc: 0.593, loss: 0.879, lr: 0.9999601015919465\n",
      "epoch: 500, acc: 0.630, loss: 0.843, lr: 0.9999501024898857\n",
      "epoch: 600, acc: 0.607, loss: 0.822, lr: 0.9999401035877952\n",
      "epoch: 700, acc: 0.593, loss: 0.799, lr: 0.9999301048856686\n",
      "epoch: 800, acc: 0.583, loss: 0.784, lr: 0.9999201063834999\n",
      "epoch: 900, acc: 0.620, loss: 0.777, lr: 0.9999101080812834\n",
      "epoch: 1000, acc: 0.650, loss: 0.775, lr: 0.9999001099790131\n",
      "epoch: 1100, acc: 0.617, loss: 0.745, lr: 0.9998901120766828\n",
      "epoch: 1200, acc: 0.677, loss: 0.721, lr: 0.9998801143742865\n",
      "epoch: 1300, acc: 0.627, loss: 0.716, lr: 0.9998701168718185\n",
      "epoch: 1400, acc: 0.637, loss: 0.699, lr: 0.9998601195692723\n",
      "epoch: 1500, acc: 0.643, loss: 0.692, lr: 0.9998501224666422\n",
      "epoch: 1600, acc: 0.650, loss: 0.713, lr: 0.9998401255639222\n",
      "epoch: 1700, acc: 0.667, loss: 0.678, lr: 0.9998301288611066\n",
      "epoch: 1800, acc: 0.663, loss: 0.673, lr: 0.9998201323581888\n",
      "epoch: 1900, acc: 0.677, loss: 0.663, lr: 0.999810136055163\n",
      "epoch: 2000, acc: 0.673, loss: 0.672, lr: 0.9998001399520237\n",
      "epoch: 2100, acc: 0.683, loss: 0.649, lr: 0.9997901440487642\n",
      "epoch: 2200, acc: 0.690, loss: 0.643, lr: 0.9997801483453788\n",
      "epoch: 2300, acc: 0.707, loss: 0.635, lr: 0.9997701528418618\n",
      "epoch: 2400, acc: 0.717, loss: 0.623, lr: 0.9997601575382066\n",
      "epoch: 2500, acc: 0.733, loss: 0.634, lr: 0.9997501624344076\n",
      "epoch: 2600, acc: 0.710, loss: 0.616, lr: 0.9997401675304587\n",
      "epoch: 2700, acc: 0.737, loss: 0.593, lr: 0.9997301728263542\n",
      "epoch: 2800, acc: 0.747, loss: 0.588, lr: 0.9997201783220876\n",
      "epoch: 2900, acc: 0.727, loss: 0.587, lr: 0.9997101840176532\n",
      "epoch: 3000, acc: 0.753, loss: 0.560, lr: 0.9997001899130452\n",
      "epoch: 3100, acc: 0.767, loss: 0.549, lr: 0.999690196008257\n",
      "epoch: 3200, acc: 0.767, loss: 0.554, lr: 0.9996802023032831\n",
      "epoch: 3300, acc: 0.777, loss: 0.538, lr: 0.9996702087981174\n",
      "epoch: 3400, acc: 0.777, loss: 0.518, lr: 0.9996602154927541\n",
      "epoch: 3500, acc: 0.787, loss: 0.535, lr: 0.9996502223871867\n",
      "epoch: 3600, acc: 0.797, loss: 0.503, lr: 0.9996402294814095\n",
      "epoch: 3700, acc: 0.787, loss: 0.491, lr: 0.9996302367754168\n",
      "epoch: 3800, acc: 0.793, loss: 0.481, lr: 0.9996202442692022\n",
      "epoch: 3900, acc: 0.797, loss: 0.496, lr: 0.9996102519627597\n",
      "epoch: 4000, acc: 0.800, loss: 0.482, lr: 0.9996002598560837\n",
      "epoch: 4100, acc: 0.813, loss: 0.466, lr: 0.9995902679491677\n",
      "epoch: 4200, acc: 0.810, loss: 0.454, lr: 0.999580276242006\n",
      "epoch: 4300, acc: 0.810, loss: 0.458, lr: 0.9995702847345925\n",
      "epoch: 4400, acc: 0.807, loss: 0.452, lr: 0.9995602934269215\n",
      "epoch: 4500, acc: 0.810, loss: 0.467, lr: 0.9995503023189867\n",
      "epoch: 4600, acc: 0.830, loss: 0.436, lr: 0.9995403114107821\n",
      "epoch: 4700, acc: 0.817, loss: 0.440, lr: 0.9995303207023021\n",
      "epoch: 4800, acc: 0.817, loss: 0.432, lr: 0.9995203301935401\n",
      "epoch: 4900, acc: 0.833, loss: 0.429, lr: 0.9995103398844906\n",
      "epoch: 5000, acc: 0.837, loss: 0.428, lr: 0.9995003497751473\n",
      "epoch: 5100, acc: 0.843, loss: 0.410, lr: 0.9994903598655046\n",
      "epoch: 5200, acc: 0.833, loss: 0.421, lr: 0.9994803701555561\n",
      "epoch: 5300, acc: 0.843, loss: 0.414, lr: 0.9994703806452959\n",
      "epoch: 5400, acc: 0.857, loss: 0.402, lr: 0.9994603913347184\n",
      "epoch: 5500, acc: 0.860, loss: 0.399, lr: 0.9994504022238171\n",
      "epoch: 5600, acc: 0.857, loss: 0.393, lr: 0.9994404133125863\n",
      "epoch: 5700, acc: 0.860, loss: 0.386, lr: 0.9994304246010199\n",
      "epoch: 5800, acc: 0.860, loss: 0.384, lr: 0.9994204360891119\n",
      "epoch: 5900, acc: 0.863, loss: 0.382, lr: 0.9994104477768564\n",
      "epoch: 6000, acc: 0.863, loss: 0.377, lr: 0.9994004596642473\n",
      "epoch: 6100, acc: 0.867, loss: 0.363, lr: 0.9993904717512789\n",
      "epoch: 6200, acc: 0.877, loss: 0.371, lr: 0.9993804840379449\n",
      "epoch: 6300, acc: 0.863, loss: 0.365, lr: 0.9993704965242393\n",
      "epoch: 6400, acc: 0.873, loss: 0.355, lr: 0.9993605092101565\n",
      "epoch: 6500, acc: 0.870, loss: 0.354, lr: 0.99935052209569\n",
      "epoch: 6600, acc: 0.887, loss: 0.356, lr: 0.9993405351808341\n",
      "epoch: 6700, acc: 0.880, loss: 0.349, lr: 0.999330548465583\n",
      "epoch: 6800, acc: 0.867, loss: 0.336, lr: 0.9993205619499302\n",
      "epoch: 6900, acc: 0.853, loss: 0.343, lr: 0.9993105756338702\n",
      "epoch: 7000, acc: 0.853, loss: 0.342, lr: 0.9993005895173966\n",
      "epoch: 7100, acc: 0.853, loss: 0.341, lr: 0.999290603600504\n",
      "epoch: 7200, acc: 0.853, loss: 0.338, lr: 0.9992806178831859\n",
      "epoch: 7300, acc: 0.850, loss: 0.336, lr: 0.9992706323654365\n",
      "epoch: 7400, acc: 0.850, loss: 0.333, lr: 0.9992606470472498\n",
      "epoch: 7500, acc: 0.853, loss: 0.330, lr: 0.9992506619286198\n",
      "epoch: 7600, acc: 0.853, loss: 0.327, lr: 0.9992406770095404\n",
      "epoch: 7700, acc: 0.857, loss: 0.325, lr: 0.9992306922900058\n",
      "epoch: 7800, acc: 0.857, loss: 0.321, lr: 0.9992207077700103\n",
      "epoch: 7900, acc: 0.857, loss: 0.319, lr: 0.9992107234495472\n",
      "epoch: 8000, acc: 0.857, loss: 0.316, lr: 0.999200739328611\n",
      "epoch: 8100, acc: 0.857, loss: 0.315, lr: 0.9991907554071958\n",
      "epoch: 8200, acc: 0.860, loss: 0.312, lr: 0.9991807716852953\n",
      "epoch: 8300, acc: 0.860, loss: 0.310, lr: 0.9991707881629036\n",
      "epoch: 8400, acc: 0.860, loss: 0.308, lr: 0.999160804840015\n",
      "epoch: 8500, acc: 0.860, loss: 0.306, lr: 0.9991508217166231\n",
      "epoch: 8600, acc: 0.863, loss: 0.304, lr: 0.9991408387927221\n",
      "epoch: 8700, acc: 0.863, loss: 0.302, lr: 0.9991308560683061\n",
      "epoch: 8800, acc: 0.863, loss: 0.300, lr: 0.9991208735433692\n",
      "epoch: 8900, acc: 0.863, loss: 0.298, lr: 0.9991108912179052\n",
      "epoch: 9000, acc: 0.867, loss: 0.296, lr: 0.9991009090919081\n",
      "epoch: 9100, acc: 0.867, loss: 0.295, lr: 0.9990909271653723\n",
      "epoch: 9200, acc: 0.870, loss: 0.293, lr: 0.9990809454382913\n",
      "epoch: 9300, acc: 0.870, loss: 0.292, lr: 0.9990709639106594\n",
      "epoch: 9400, acc: 0.870, loss: 0.290, lr: 0.9990609825824707\n",
      "epoch: 9500, acc: 0.873, loss: 0.289, lr: 0.9990510014537192\n",
      "epoch: 9600, acc: 0.867, loss: 0.287, lr: 0.9990410205243986\n",
      "epoch: 9700, acc: 0.867, loss: 0.285, lr: 0.9990310397945033\n",
      "epoch: 9800, acc: 0.870, loss: 0.283, lr: 0.9990210592640273\n",
      "epoch: 9900, acc: 0.870, loss: 0.281, lr: 0.9990110789329643\n",
      "epoch: 10000, acc: 0.870, loss: 0.280, lr: 0.9990010988013085\n"
     ]
    }
   ],
   "source": [
    "# Testing this optimizer now with decaying set to 1e-4‚Äã as well as 1e-5‚Äã works better than 1e-3‚Äã, \n",
    "# which we have used previously. This optimizer with our dataset works better with lesser \n",
    "# decaying: \n",
    "# Create dataset \n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    " \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    " \n",
    "# Create second Dense layer with 64 input features (as we take output \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "#optimizer = Optimizer_SGD(decay=8e-8, momentum=0.9) \n",
    "\n",
    "optimizer = Optimizer_Adagrad(decay=1e-7) \n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    " \n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(dense2.output, y) \n",
    " \n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_learning_rate}') \n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y) \n",
    "    dense2.backward(loss_activation.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " \n",
    "    # Update weights and biases \n",
    "    optimizer.pre_update_params() \n",
    "    optimizer.update_params(dense1) \n",
    "    optimizer.update_params(dense2) \n",
    "    optimizer.post_update_params()\n",
    "    # in this also softmax not there\n",
    "    # also its slow and accuracy is less may be due to stagnation\n",
    "    # scholingly removing the dacy favtor improved the accuracy to a whooping 86%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b0ba7687",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now lets go to RMSProp its simple just combine both momentum and adagrad to get this \n",
    "# we will now go to the Adam(adaptive momentum most important optimization method)\n",
    "# Awe apply momentum like in SGD, then apply a per-weight adavtive learning rate with the cache as done in RMSProp. Why not adagrad like?üò∫\n",
    "# its also add a bias correction mechanism, both momenum and caches are divided by 1-beta^step,\n",
    "#lets implement it\n",
    "class Optimizer_Adam:\n",
    "  def __init__(self, Learning_rate=0.001, decay=0., epsilon=1e-7, beta_1 = 0.9, beta_2 = 0.999):\n",
    "    self.Learning_rate = Learning_rate\n",
    "    self.decay = decay\n",
    "    self.current_Learning_rate = Learning_rate\n",
    "    self.iterations = 0\n",
    "    self.beta_1 = beta_1\n",
    "    self.beta_2 = beta_2\n",
    "    self.epsilon = epsilon\n",
    "  def pre_update_params(self):\n",
    "    if self.decay:\n",
    "      self.current_Learning_rate = self.Learning_rate /(1+self.decay*self.iterations)\n",
    "\n",
    "  def update_params(self, Layer):\n",
    "    if not hasattr(Layer, 'weight_cache'):\n",
    "      Layer.weight_momentums = np.zeros_like(Layer.weights) \n",
    "      Layer.weight_cache = np.zeros_like(Layer.weights) \n",
    "      Layer.bias_momentums = np.zeros_like(Layer.biases) \n",
    "      Layer.bias_cache = np.zeros_like(Layer.biases) \n",
    "    Layer.weight_momentums = self.beta_1 *Layer.weight_momentums + (1-self.beta_1)*Layer.dweights\n",
    "    Layer.bias_momentums = self.beta_1 *Layer.bias_momentums + (1-self.beta_1)*Layer.dbiases\n",
    "    weight_momentums_corrected = Layer.weight_momentums/(1-self.beta_1**(self.iterations + 1))\n",
    "    bias_momentums_corrected = Layer.bias_momentums/(1-self.beta_1**(self.iterations+1))\n",
    "\n",
    "    Layer.weight_cache = self.beta_2 *Layer.weight_cache + (1-self.beta_2)*Layer.dweights**2\n",
    "    Layer.bias_cache = self.beta_2 * Layer.bias_cache + (1-self.beta_2)*Layer.dbiases**2\n",
    "    weight_cache_corrected = Layer.weight_cache/(1-self.beta_2**(self.iterations + 1))\n",
    "    bias_cache_corrected = Layer.bias_cache/(1-self.beta_2**(self.iterations + 1))\n",
    "\n",
    "    Layer.weights += -self.current_Learning_rate*weight_momentums_corrected/(np.sqrt(bias_cache_corrected)+ self.epsilon)\n",
    "    Layer.biases += -self.current_Learning_rate*bias_momentums_corrected/(np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "  def post_update_params(self):\n",
    "    self.iterations += 1\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "259f7e1b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.350, loss: 1.099, lr: 0.02\n",
      "epoch: 100, acc: 0.510, loss: 0.932, lr: 0.01998021958261321\n",
      "epoch: 200, acc: 0.607, loss: 0.800, lr: 0.019960279044701046\n",
      "epoch: 300, acc: 0.667, loss: 0.712, lr: 0.019940378268975763\n",
      "epoch: 400, acc: 0.697, loss: 0.655, lr: 0.01992051713662487\n",
      "epoch: 500, acc: 0.747, loss: 0.616, lr: 0.01990069552930875\n",
      "epoch: 600, acc: 0.767, loss: 0.578, lr: 0.019880913329158343\n",
      "epoch: 700, acc: 0.773, loss: 0.552, lr: 0.019861170418772778\n",
      "epoch: 800, acc: 0.773, loss: 0.527, lr: 0.019841466681217078\n",
      "epoch: 900, acc: 0.773, loss: 0.507, lr: 0.01982180200001982\n",
      "epoch: 1000, acc: 0.787, loss: 0.491, lr: 0.019802176259170884\n",
      "epoch: 1100, acc: 0.790, loss: 0.479, lr: 0.01978258934311912\n",
      "epoch: 1200, acc: 0.797, loss: 0.468, lr: 0.01976304113677013\n",
      "epoch: 1300, acc: 0.797, loss: 0.458, lr: 0.019743531525483964\n",
      "epoch: 1400, acc: 0.810, loss: 0.450, lr: 0.01972406039507293\n",
      "epoch: 1500, acc: 0.810, loss: 0.443, lr: 0.019704627631799327\n",
      "epoch: 1600, acc: 0.813, loss: 0.435, lr: 0.019685233122373254\n",
      "epoch: 1700, acc: 0.820, loss: 0.428, lr: 0.019665876753950384\n",
      "epoch: 1800, acc: 0.827, loss: 0.421, lr: 0.019646558414129805\n",
      "epoch: 1900, acc: 0.827, loss: 0.416, lr: 0.019627277990951823\n",
      "epoch: 2000, acc: 0.837, loss: 0.414, lr: 0.019608035372895814\n",
      "epoch: 2100, acc: 0.830, loss: 0.408, lr: 0.019588830448878047\n",
      "epoch: 2200, acc: 0.833, loss: 0.403, lr: 0.019569663108249594\n",
      "epoch: 2300, acc: 0.830, loss: 0.400, lr: 0.019550533240794143\n",
      "epoch: 2400, acc: 0.830, loss: 0.397, lr: 0.019531440736725945\n",
      "epoch: 2500, acc: 0.837, loss: 0.395, lr: 0.019512385486687673\n",
      "epoch: 2600, acc: 0.833, loss: 0.395, lr: 0.01949336738174836\n",
      "epoch: 2700, acc: 0.830, loss: 0.389, lr: 0.019474386313401298\n",
      "epoch: 2800, acc: 0.830, loss: 0.386, lr: 0.019455442173562\n",
      "epoch: 2900, acc: 0.843, loss: 0.383, lr: 0.019436534854566128\n",
      "epoch: 3000, acc: 0.827, loss: 0.382, lr: 0.01941766424916747\n",
      "epoch: 3100, acc: 0.837, loss: 0.378, lr: 0.019398830250535893\n",
      "epoch: 3200, acc: 0.837, loss: 0.376, lr: 0.019380032752255354\n",
      "epoch: 3300, acc: 0.833, loss: 0.374, lr: 0.01936127164832186\n",
      "epoch: 3400, acc: 0.850, loss: 0.374, lr: 0.01934254683314152\n",
      "epoch: 3500, acc: 0.847, loss: 0.373, lr: 0.019323858201528515\n",
      "epoch: 3600, acc: 0.850, loss: 0.369, lr: 0.019305205648703173\n",
      "epoch: 3700, acc: 0.857, loss: 0.365, lr: 0.01928658907028997\n",
      "epoch: 3800, acc: 0.847, loss: 0.363, lr: 0.01926800836231563\n",
      "epoch: 3900, acc: 0.847, loss: 0.361, lr: 0.019249463421207133\n",
      "epoch: 4000, acc: 0.837, loss: 0.363, lr: 0.019230954143789846\n",
      "epoch: 4100, acc: 0.850, loss: 0.358, lr: 0.019212480427285565\n",
      "epoch: 4200, acc: 0.853, loss: 0.356, lr: 0.019194042169310647\n",
      "epoch: 4300, acc: 0.850, loss: 0.355, lr: 0.019175639267874092\n",
      "epoch: 4400, acc: 0.850, loss: 0.355, lr: 0.019157271621375684\n",
      "epoch: 4500, acc: 0.857, loss: 0.354, lr: 0.0191389391286041\n",
      "epoch: 4600, acc: 0.860, loss: 0.352, lr: 0.019120641688735073\n",
      "epoch: 4700, acc: 0.853, loss: 0.352, lr: 0.019102379201329525\n",
      "epoch: 4800, acc: 0.857, loss: 0.350, lr: 0.01908415156633174\n",
      "epoch: 4900, acc: 0.853, loss: 0.351, lr: 0.01906595868406753\n",
      "epoch: 5000, acc: 0.853, loss: 0.346, lr: 0.01904780045524243\n",
      "epoch: 5100, acc: 0.840, loss: 0.346, lr: 0.019029676780939874\n",
      "epoch: 5200, acc: 0.853, loss: 0.347, lr: 0.019011587562619416\n",
      "epoch: 5300, acc: 0.857, loss: 0.344, lr: 0.01899353270211493\n",
      "epoch: 5400, acc: 0.850, loss: 0.342, lr: 0.018975512101632844\n",
      "epoch: 5500, acc: 0.850, loss: 0.341, lr: 0.018957525663750367\n",
      "epoch: 5600, acc: 0.857, loss: 0.340, lr: 0.018939573291413745\n",
      "epoch: 5700, acc: 0.853, loss: 0.338, lr: 0.018921654887936498\n",
      "epoch: 5800, acc: 0.850, loss: 0.336, lr: 0.018903770356997703\n",
      "epoch: 5900, acc: 0.853, loss: 0.336, lr: 0.01888591960264025\n",
      "epoch: 6000, acc: 0.850, loss: 0.338, lr: 0.018868102529269144\n",
      "epoch: 6100, acc: 0.847, loss: 0.333, lr: 0.018850319041649778\n",
      "epoch: 6200, acc: 0.857, loss: 0.329, lr: 0.018832569044906263\n",
      "epoch: 6300, acc: 0.850, loss: 0.328, lr: 0.018814852444519702\n",
      "epoch: 6400, acc: 0.847, loss: 0.325, lr: 0.018797169146326564\n",
      "epoch: 6500, acc: 0.853, loss: 0.323, lr: 0.018779519056516963\n",
      "epoch: 6600, acc: 0.860, loss: 0.322, lr: 0.018761902081633038\n",
      "epoch: 6700, acc: 0.860, loss: 0.321, lr: 0.018744318128567278\n",
      "epoch: 6800, acc: 0.860, loss: 0.318, lr: 0.018726767104560903\n",
      "epoch: 6900, acc: 0.847, loss: 0.317, lr: 0.018709248917202218\n",
      "epoch: 7000, acc: 0.847, loss: 0.316, lr: 0.018691763474424996\n",
      "epoch: 7100, acc: 0.857, loss: 0.315, lr: 0.018674310684506857\n",
      "epoch: 7200, acc: 0.853, loss: 0.314, lr: 0.018656890456067686\n",
      "epoch: 7300, acc: 0.857, loss: 0.313, lr: 0.01863950269806802\n",
      "epoch: 7400, acc: 0.857, loss: 0.312, lr: 0.018622147319807447\n",
      "epoch: 7500, acc: 0.860, loss: 0.311, lr: 0.018604824230923078\n",
      "epoch: 7600, acc: 0.860, loss: 0.311, lr: 0.01858753334138793\n",
      "epoch: 7700, acc: 0.860, loss: 0.311, lr: 0.018570274561509396\n",
      "epoch: 7800, acc: 0.867, loss: 0.311, lr: 0.018553047801927663\n",
      "epoch: 7900, acc: 0.863, loss: 0.308, lr: 0.018535852973614212\n",
      "epoch: 8000, acc: 0.860, loss: 0.309, lr: 0.01851868998787026\n",
      "epoch: 8100, acc: 0.867, loss: 0.308, lr: 0.018501558756325222\n",
      "epoch: 8200, acc: 0.863, loss: 0.308, lr: 0.01848445919093522\n",
      "epoch: 8300, acc: 0.870, loss: 0.306, lr: 0.018467391203981567\n",
      "epoch: 8400, acc: 0.863, loss: 0.307, lr: 0.01845035470806926\n",
      "epoch: 8500, acc: 0.863, loss: 0.307, lr: 0.018433349616125496\n",
      "epoch: 8600, acc: 0.867, loss: 0.305, lr: 0.018416375841398172\n",
      "epoch: 8700, acc: 0.867, loss: 0.307, lr: 0.018399433297454436\n",
      "epoch: 8800, acc: 0.867, loss: 0.305, lr: 0.01838252189817921\n",
      "epoch: 8900, acc: 0.870, loss: 0.305, lr: 0.018365641557773718\n",
      "epoch: 9000, acc: 0.853, loss: 0.303, lr: 0.018348792190754044\n",
      "epoch: 9100, acc: 0.857, loss: 0.303, lr: 0.0183319737119497\n",
      "epoch: 9200, acc: 0.867, loss: 0.312, lr: 0.018315186036502167\n",
      "epoch: 9300, acc: 0.867, loss: 0.303, lr: 0.018298429079863496\n",
      "epoch: 9400, acc: 0.857, loss: 0.301, lr: 0.018281702757794862\n",
      "epoch: 9500, acc: 0.860, loss: 0.301, lr: 0.018265006986365174\n",
      "epoch: 9600, acc: 0.867, loss: 0.301, lr: 0.018248341681949654\n",
      "epoch: 9700, acc: 0.860, loss: 0.299, lr: 0.018231706761228456\n",
      "epoch: 9800, acc: 0.857, loss: 0.300, lr: 0.01821510214118526\n",
      "epoch: 9900, acc: 0.867, loss: 0.300, lr: 0.018198527739105907\n",
      "epoch: 10000, acc: 0.867, loss: 0.300, lr: 0.018181983472577025\n"
     ]
    }
   ],
   "source": [
    "X, y = spiral_data(samples=100, classes=3) \n",
    " \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    " \n",
    "# Create second Dense layer with 64 input features (as we take output \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "#optimizer = Optimizer_SGD(decay=8e-8, momentum=0.9) \n",
    "\n",
    "optimizer = Optimizer_Adam(Learning_rate=0.02, decay=1e-5) \n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    " \n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(dense2.output, y) \n",
    " \n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' + \n",
    "              f'lr: {optimizer.current_Learning_rate}') \n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y) \n",
    "    dense2.backward(loss_activation.dinputs) \n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " \n",
    "    # Update weights and biases \n",
    "    optimizer.pre_update_params() \n",
    "    optimizer.update_params(dense1) \n",
    "    optimizer.update_params(dense2) \n",
    "    optimizer.post_update_params()\n",
    "    # in this also softmax not there\n",
    "    # accuracy is 96 % in this also not softmax so it was fast also \n",
    "    # overall it worked best"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "397384a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.340, loss: 1.099, currect_learning_rate: 0.02\n",
      "epoch: 100, acc: 0.420, loss: 1.095, currect_learning_rate: 0.01998021958261321\n",
      "epoch: 200, acc: 0.533, loss: 1.050, currect_learning_rate: 0.019960279044701046\n",
      "epoch: 300, acc: 0.587, loss: 1.014, currect_learning_rate: 0.019940378268975763\n",
      "epoch: 400, acc: 0.587, loss: 0.992, currect_learning_rate: 0.01992051713662487\n",
      "epoch: 500, acc: 0.593, loss: 0.980, currect_learning_rate: 0.01990069552930875\n",
      "epoch: 600, acc: 0.597, loss: 0.972, currect_learning_rate: 0.019880913329158343\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[55], line 42\u001b[0m\n\u001b[0;32m     36\u001b[0m loss \u001b[38;5;241m=\u001b[39m loss_activation\u001b[38;5;241m.\u001b[39mforward(activation2\u001b[38;5;241m.\u001b[39moutput, y) \n\u001b[0;32m     37\u001b[0m \u001b[38;5;66;03m# loss = loss_activation.forward(dense2.output, y)\u001b[39;00m\n\u001b[0;32m     38\u001b[0m \n\u001b[0;32m     39\u001b[0m \n\u001b[0;32m     40\u001b[0m \u001b[38;5;66;03m# Calculate accuracy from output of activation2 and targets \u001b[39;00m\n\u001b[0;32m     41\u001b[0m \u001b[38;5;66;03m# calculate values along first axis \u001b[39;00m\n\u001b[1;32m---> 42\u001b[0m predictions \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43margmax\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_activation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m \n\u001b[0;32m     43\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(y\u001b[38;5;241m.\u001b[39mshape) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m: \n\u001b[0;32m     44\u001b[0m     y \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39margmax(y, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m) \n",
      "File \u001b[1;32mc:\\Users\\binda\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:1229\u001b[0m, in \u001b[0;36margmax\u001b[1;34m(a, axis, out, keepdims)\u001b[0m\n\u001b[0;32m   1142\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1143\u001b[0m \u001b[38;5;124;03mReturns the indices of the maximum values along an axis.\u001b[39;00m\n\u001b[0;32m   1144\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1226\u001b[0m \u001b[38;5;124;03m(2, 1, 4)\u001b[39;00m\n\u001b[0;32m   1227\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1228\u001b[0m kwds \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mkeepdims\u001b[39m\u001b[38;5;124m'\u001b[39m: keepdims} \u001b[38;5;28;01mif\u001b[39;00m keepdims \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39m_NoValue \u001b[38;5;28;01melse\u001b[39;00m {}\n\u001b[1;32m-> 1229\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_wrapfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43margmax\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\binda\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\numpy\\core\\fromnumeric.py:59\u001b[0m, in \u001b[0;36m_wrapfunc\u001b[1;34m(obj, method, *args, **kwds)\u001b[0m\n\u001b[0;32m     56\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m     58\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m---> 59\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mbound\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     60\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[0;32m     61\u001b[0m     \u001b[38;5;66;03m# A TypeError occurs if the object does have such a method in its\u001b[39;00m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;66;03m# class, but its signature is not identical to that of NumPy's. This\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     66\u001b[0m     \u001b[38;5;66;03m# Call _wrapit from within the except clause to ensure a potential\u001b[39;00m\n\u001b[0;32m     67\u001b[0m     \u001b[38;5;66;03m# exception has a traceback chain.\u001b[39;00m\n\u001b[0;32m     68\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _wrapit(obj, method, \u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# now lets add softmax also in this \n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    " \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    "activation2 = Activation_Softmax()\n",
    "# Create second Dense layer with 64 input features (as we take output  \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "optimizer = Optimizer_Adam(decay=1e-5,Learning_rate=0.02) \n",
    " \n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    "    activation2.forward(dense2.output)\n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(activation2.output, y) \n",
    "    # loss = loss_activation.forward(dense2.output, y)\n",
    "  \n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'currect_learning_rate: {optimizer.current_Learning_rate}')\n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    activation2.backward(loss_activation.dinputs)\n",
    "    dense2.backward(activation2.dinputs) \n",
    "    # dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " # i noticed that softmax was not added in this so i made activation 2 and also applied it here but now its taking a lot more time, a lot more time.\n",
    "    # Update weights and biases \n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1) \n",
    "    optimizer.update_params(dense2) \n",
    "    optimizer.post_update_params()\n",
    "    # adding softmax devreased the accuracy to 72 % let me run few more times may be it gives more "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4389f96b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.360, loss: 1.099, currect_learning_rate: 0.05\n",
      "epoch: 100, acc: 0.543, loss: 1.023, currect_learning_rate: 0.04999752512250644\n",
      "epoch: 200, acc: 0.580, loss: 0.967, currect_learning_rate: 0.049995025494963256\n",
      "epoch: 300, acc: 0.643, loss: 0.932, currect_learning_rate: 0.049992526117345455\n",
      "epoch: 400, acc: 0.660, loss: 0.908, currect_learning_rate: 0.04999002698961558\n",
      "epoch: 500, acc: 0.673, loss: 0.888, currect_learning_rate: 0.049987528111736124\n",
      "epoch: 600, acc: 0.683, loss: 0.873, currect_learning_rate: 0.049985029483669646\n",
      "epoch: 700, acc: 0.707, loss: 0.859, currect_learning_rate: 0.049982531105378675\n",
      "epoch: 800, acc: 0.723, loss: 0.847, currect_learning_rate: 0.04998003297682575\n",
      "epoch: 900, acc: 0.727, loss: 0.841, currect_learning_rate: 0.049977535097973466\n",
      "epoch: 1000, acc: 0.730, loss: 0.835, currect_learning_rate: 0.04997503746878434\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[56], line 414\u001b[0m\n\u001b[0;32m    412\u001b[0m \u001b[38;5;66;03m# Backward pass \u001b[39;00m\n\u001b[0;32m    413\u001b[0m loss_activation\u001b[38;5;241m.\u001b[39mbackward(loss_activation\u001b[38;5;241m.\u001b[39moutput, y)\n\u001b[1;32m--> 414\u001b[0m \u001b[43mactivation2\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mloss_activation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdinputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    415\u001b[0m dense2\u001b[38;5;241m.\u001b[39mbackward(activation2\u001b[38;5;241m.\u001b[39mdinputs) \n\u001b[0;32m    416\u001b[0m \u001b[38;5;66;03m# dense2.backward(loss_activation.dinputs)\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[56], line 85\u001b[0m, in \u001b[0;36mActivation_Softmax.backward\u001b[1;34m(self, dvalues)\u001b[0m\n\u001b[0;32m     82\u001b[0m            jacobian_matrix \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mdiagflat(single_output) \u001b[38;5;241m-\u001b[39m np\u001b[38;5;241m.\u001b[39mdot(single_output, single_output\u001b[38;5;241m.\u001b[39mT) \n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Calculate sample-wise gradient \u001b[39;00m\n\u001b[0;32m     84\u001b[0m            \u001b[38;5;66;03m# and add it to the array of sample gradients \u001b[39;00m\n\u001b[1;32m---> 85\u001b[0m            \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdinputs[index] \u001b[38;5;241m=\u001b[39m \u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdot\u001b[49m\u001b[43m(\u001b[49m\u001b[43mjacobian_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43msingle_dvalues\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\binda\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nnfs\\core.py:22\u001b[0m, in \u001b[0;36minit.<locals>.dot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43morig_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfloat64\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\binda\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nnfs\\core.py:22\u001b[0m, in \u001b[0;36minit.<locals>.dot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43morig_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfloat64\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\binda\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\nnfs\\core.py:22\u001b[0m, in \u001b[0;36minit.<locals>.dot\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdot\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m---> 22\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43morig_dot\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43ma\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mastype\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mfloat64\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ma\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfloat32\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# now lets add all the classes and code up to this point in one cell\n",
    "import numpy as np \n",
    "import nnfs \n",
    "from nnfs.datasets import spiral_data \n",
    " \n",
    "nnfs.init() \n",
    " \n",
    " \n",
    "# Dense layer \n",
    "class Layer_Dense: \n",
    " \n",
    "    # Layer initialization \n",
    "    def __init__(self, n_inputs, n_neurons): \n",
    "        # Initialize weights and biases \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons) \n",
    "        self.biases = np.zeros((1, n_neurons)) \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs): \n",
    "        # Remember input values \n",
    "        self.inputs = inputs \n",
    "        # Calculate output values from inputs, weights and biases \n",
    "        self.output = np.dot(inputs, self.weights) + self.biases \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues): \n",
    "        # Gradients on parameters \n",
    "        self.dweights = np.dot(self.inputs.T, dvalues) \n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True) \n",
    "        # Gradient on values \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T) \n",
    " \n",
    " \n",
    "# ReLU activation \n",
    "class Activation_ReLU: \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs):\n",
    " # Remember input values \n",
    "        self.inputs = inputs \n",
    "        # Calculate output values from inputs \n",
    "        self.output = np.maximum(0, inputs) \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues): \n",
    "        # Since we need to modify original variable, \n",
    "        # let's make a copy of values first \n",
    "        self.dinputs = dvalues.copy() \n",
    " \n",
    "        # Zero gradient where input values were negative \n",
    "        self.dinputs[self.inputs <= 0] = 0 \n",
    " \n",
    " \n",
    "# Softmax activation \n",
    "class Activation_Softmax: \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs): \n",
    "        # Remember input values \n",
    "        self.inputs = inputs \n",
    " \n",
    "        # Get unnormalized probabilities \n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, \n",
    "                                            keepdims=True)) \n",
    "        # Normalize them for each sample \n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, \n",
    "                                            keepdims=True) \n",
    " \n",
    "        self.output = probabilities \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues): \n",
    " \n",
    "        # Create uninitialized array \n",
    "        self.dinputs = np.empty_like(dvalues) \n",
    " \n",
    "        # Enumerate outputs and gradients \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)): \n",
    "            # Flatten output array \n",
    "            single_output = single_output.reshape(-1, 1) \n",
    "            # Calculate Jacobian matrix of the output and \n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T) \n",
    " # Calculate sample-wise gradient \n",
    "            # and add it to the array of sample gradients \n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,single_dvalues) \n",
    " \n",
    " \n",
    "# SGD optimizer \n",
    "class Optimizer_SGD: \n",
    " \n",
    "    # Initialize optimizer - set settings,  \n",
    "    # learning rate of 1. is default for this optimizer \n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.): \n",
    "        self.learning_rate = learning_rate \n",
    "        self.current_learning_rate = learning_rate \n",
    "        self.decay = decay \n",
    "        self.iterations = 0 \n",
    "        self.momentum = momentum \n",
    " \n",
    "    # Call once before any parameter updates \n",
    "    def pre_update_params(self): \n",
    "        if self.decay: \n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations)) \n",
    " \n",
    "    # Update parameters \n",
    "    def update_params(self, layer): \n",
    " \n",
    "        # If we use momentum \n",
    "        if self.momentum: \n",
    " \n",
    "            # If layer does not contain momentum arrays, create them \n",
    "            # filled with zeros \n",
    "            if not hasattr(layer, 'weight_momentums'): \n",
    "                layer.weight_momentums = np.zeros_like(layer.weights) \n",
    "                # If there is no momentum array for weights \n",
    "                # The array doesn't exist for biases yet either. \n",
    "                layer.bias_momentums = np.zeros_like(layer.biases) \n",
    " \n",
    "            # Build weight updates with momentum - take previous \n",
    "            # updates multiplied by retain factor and update with \n",
    "            # current gradients \n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights \n",
    "            layer.weight_momentums = weight_updates \n",
    " \n",
    "            # Build bias updates \n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases \n",
    "            layer.bias_momentums = bias_updates \n",
    " \n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update) \n",
    "        else: \n",
    "            weight_updates = -self.current_learning_rate * layer.dweights \n",
    "            bias_updates = -self.current_learning_rate *layer.dbiases \n",
    " \n",
    "        # Update weights and biases using either \n",
    "        # vanilla or momentum updates \n",
    "        layer.weights += weight_updates \n",
    "        layer.biases += bias_updates \n",
    " \n",
    "    # Call once after any parameter updates \n",
    "    def post_update_params(self): \n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "class Optimizer_Adagrad: \n",
    " \n",
    "    # Initialize optimizer - set settings \n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7): \n",
    "        self.learning_rate = learning_rate \n",
    "        self.current_learning_rate = learning_rate \n",
    "        self.decay = decay \n",
    "        self.iterations = 0 \n",
    "        self.epsilon = epsilon \n",
    " \n",
    "    # Call once before any parameter updates \n",
    "    def pre_update_params(self): \n",
    "        if self.decay: \n",
    "            self.current_learning_rate = self.learning_rate *   (1. / (1. + self.decay * self.iterations)) \n",
    " \n",
    "\n",
    "    # Update parameters \n",
    "    def update_params(self, layer): \n",
    " \n",
    "        # If layer does not contain cache arrays, \n",
    "        # create them filled with zeros \n",
    "        if not hasattr(layer, 'weight_cache'): \n",
    "            layer.weight_cache = np.zeros_like(layer.weights) \n",
    "            layer.bias_cache = np.zeros_like(layer.biases) \n",
    " \n",
    "        # Update cache with squared current gradients \n",
    "        layer.weight_cache += layer.dweights**2 \n",
    "        layer.bias_cache += layer.dbiases**2 \n",
    " \n",
    "        # Vanilla SGD parameter update + normalization \n",
    "        # with square rooted cache \n",
    "        layer.weights += -self.current_learning_rate *layer.dweights /(np.sqrt(layer.weight_cache) + self.epsilon) \n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases /(np.sqrt(layer.bias_cache) + self.epsilon) \n",
    " \n",
    "    # Call once after any parameter updates \n",
    "    def post_update_params(self): \n",
    "        self.iterations += 1 \n",
    "# RMSprop optimizer \n",
    "class Optimizer_RMSprop: \n",
    " \n",
    "    # Initialize optimizer - set settings \n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, \n",
    "                 rho=0.9): \n",
    "        self.learning_rate = learning_rate \n",
    "        self.current_learning_rate = learning_rate \n",
    "        self.decay = decay \n",
    "        self.iterations = 0 \n",
    "        self.epsilon = epsilon \n",
    "        self.rho = rho \n",
    " \n",
    "    # Call once before any parameter updates \n",
    "    def pre_update_params(self): \n",
    "        if self.decay: \n",
    "            self.current_learning_rate = self.learning_rate *  (1. / (1. + self.decay * self.iterations)) \n",
    " \n",
    "    # Update parameters \n",
    "    def update_params(self, layer): \n",
    " \n",
    "        # If layer does not contain cache arrays, \n",
    "        # create them filled with zeros \n",
    "        if not hasattr(layer, 'weight_cache'): \n",
    "            layer.weight_cache = np.zeros_like(layer.weights) \n",
    "            layer.bias_cache = np.zeros_like(layer.biases) \n",
    " \n",
    "        # Update cache with squared current gradients \n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2 \n",
    "        layer.bias_cache = self.rho * layer.bias_cache +  (1 - self.rho) * layer.dbiases**2 \n",
    " \n",
    "        # Vanilla SGD parameter update + normalization \n",
    "        # with square rooted cache \n",
    "        layer.weights += -self.current_learning_rate *  layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon) \n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases /  (np.sqrt(layer.bias_cache) + self.epsilon) \n",
    " \n",
    "    # Call once after any parameter updates \n",
    "    def post_update_params(self): \n",
    "        self.iterations += 1 \n",
    "\n",
    "class Optimizer_Adam:\n",
    "  def __init__(self, Learning_rate=0.001, decay=0., epsilon=1e-7, beta_1 = 0.9, beta_2 = 0.999):\n",
    "    self.Learning_rate = Learning_rate\n",
    "    self.decay = decay\n",
    "    self.current_Learning_rate = Learning_rate\n",
    "    self.iterations = 0\n",
    "    self.beta_1 = beta_1\n",
    "    self.beta_2 = beta_2\n",
    "    self.epsilon = epsilon\n",
    "  def pre_update_params(self):\n",
    "    if self.decay:\n",
    "      self.current_Learning_rate = self.Learning_rate /(1+self.decay*self.iterations)\n",
    "\n",
    "  def update_params(self, Layer):\n",
    "    if not hasattr(Layer, 'weight_cache'):\n",
    "      Layer.weight_momentums = np.zeros_like(Layer.weights) \n",
    "      Layer.weight_cache = np.zeros_like(Layer.weights) \n",
    "      Layer.bias_momentums = np.zeros_like(Layer.biases) \n",
    "      Layer.bias_cache = np.zeros_like(Layer.biases) \n",
    "    Layer.weight_momentums = self.beta_1 *Layer.weight_momentums + (1-self.beta_1)*Layer.dweights\n",
    "    Layer.bias_momentums = self.beta_1 *Layer.bias_momentums + (1-self.beta_1)*Layer.dbiases\n",
    "    weight_momentums_corrected = Layer.weight_momentums/(1-self.beta_1**(self.iterations + 1))\n",
    "    bias_momentums_corrected = Layer.bias_momentums/(1-self.beta_1**(self.iterations+1))\n",
    "\n",
    "    Layer.weight_cache = self.beta_2 *Layer.weight_cache + (1-self.beta_2)*Layer.dweights**2\n",
    "    Layer.bias_cache = self.beta_2 * Layer.bias_cache + (1-self.beta_2)*Layer.dbiases**2\n",
    "    weight_cache_corrected = Layer.weight_cache/(1-self.beta_2**(self.iterations + 1))\n",
    "    bias_cache_corrected = Layer.bias_cache/(1-self.beta_2**(self.iterations + 1))\n",
    "\n",
    "    Layer.weights += -self.current_Learning_rate*weight_momentums_corrected/(np.sqrt(bias_cache_corrected)+ self.epsilon)\n",
    "    Layer.biases += -self.current_Learning_rate*bias_momentums_corrected/(np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "  def post_update_params(self):\n",
    "    self.iterations += 1\n",
    "\n",
    "\n",
    "# Common loss class \n",
    "class Loss: \n",
    " \n",
    "    # Calculates the data and regularization losses \n",
    "    # given model output and ground truth values \n",
    "    def calculate(self, output, y): \n",
    " \n",
    "        # Calculate sample losses \n",
    "        sample_losses = self.forward(output, y) \n",
    " \n",
    "        # Calculate mean loss \n",
    "        data_loss = np.mean(sample_losses) \n",
    " \n",
    "        # Return loss \n",
    "        return data_loss \n",
    "    \n",
    " \n",
    "# Cross-entropy loss \n",
    "class Loss_CategoricalCrossentropy(Loss): \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, y_pred, y_true): \n",
    " \n",
    "        # Number of samples in a batch \n",
    "        samples = len(y_pred) \n",
    " \n",
    "        # Clip data to prevent division by 0 \n",
    "        # Clip both sides to not drag mean towards any value \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7) \n",
    " \n",
    "        # Probabilities for target values - \n",
    "        # only if categorical labels \n",
    "        if len(y_true.shape) == 1: \n",
    "            correct_confidences = y_pred_clipped[ \n",
    "                range(samples), \n",
    "                y_true \n",
    "            ] \n",
    " # Mask values - only for one-hot encoded labels \n",
    "        elif len(y_true.shape) == 2: \n",
    "            correct_confidences = np.sum( \n",
    "                y_pred_clipped * y_true, \n",
    "                axis=1 \n",
    "            ) \n",
    " \n",
    "        # Losses \n",
    "        negative_log_likelihoods = -np.log(correct_confidences) \n",
    "        return negative_log_likelihoods \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues, y_true): \n",
    " \n",
    "        # Number of samples \n",
    "        samples = len(dvalues) \n",
    "        # Number of labels in every sample \n",
    "        # We'll use the first sample to count them \n",
    "        labels = len(dvalues[0]) \n",
    " \n",
    "        # If labels are sparse, turn them into one-hot vector \n",
    "        if len(y_true.shape) == 1: \n",
    "            y_true = np.eye(labels)[y_true] \n",
    " \n",
    "        # Calculate gradient \n",
    "        self.dinputs = -y_true / dvalues \n",
    "        # Normalize gradient \n",
    "        self.dinputs = self.dinputs / samples \n",
    " \n",
    " \n",
    "# Softmax classifier - combined Softmax activation \n",
    "# and cross-entropy loss for faster backward step \n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy(): \n",
    " \n",
    "    # Creates activation and loss function objects \n",
    "    def __init__(self): \n",
    "        self.activation = Activation_Softmax() \n",
    "        self.loss = Loss_CategoricalCrossentropy() \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs, y_true): \n",
    "        # Output layer's activation function \n",
    "        self.activation.forward(inputs) \n",
    "        # Set the output \n",
    "        self.output = self.activation.output \n",
    "        # Calculate and return loss value \n",
    "        return self.loss.calculate(self.output, y_true) \n",
    "\n",
    "  # Backward pass \n",
    "    def backward(self, dvalues, y_true): \n",
    " \n",
    "        # Number of samples \n",
    "        samples = len(dvalues) \n",
    " \n",
    "        # If labels are one-hot encoded, \n",
    "        # turn them into discrete values \n",
    "        if len(y_true.shape) == 2: \n",
    "            y_true = np.argmax(y_true, axis=1) \n",
    " \n",
    "        # Copy so we can safely modify \n",
    "        self.dinputs = dvalues.copy() \n",
    "        # Calculate gradient \n",
    "        self.dinputs[range(samples), y_true] -= 1 \n",
    "        # Normalize gradient \n",
    "        self.dinputs = self.dinputs / samples \n",
    "\n",
    "# now lets add softmax also in this \n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    " \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    "activation2 = Activation_Softmax()\n",
    "# Create second Dense layer with 64 input features (as we take output  \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "optimizer = Optimizer_Adam(decay=5e-7,Learning_rate=0.05) \n",
    " \n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    "    activation2.forward(dense2.output)\n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    loss = loss_activation.forward(activation2.output, y) \n",
    "    # loss = loss_activation.forward(dense2.output, y)\n",
    "  \n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'currect_learning_rate: {optimizer.current_Learning_rate}')\n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    activation2.backward(loss_activation.dinputs)\n",
    "    dense2.backward(activation2.dinputs) \n",
    "    # dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " # i noticed that softmax was not added in this so i made activation 2 and also applied it here but now its taking a lot more time, a lot more time.\n",
    "    # Update weights and biases \n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1) \n",
    "    optimizer.update_params(dense2) \n",
    "    optimizer.post_update_params()\n",
    "    # with softmax_activations acc: 78%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "f2bdc6cf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.343, loss: 1.099, currect_learning_rate: 0.05\n",
      "epoch: 100, acc: 0.587, loss: 0.938, currect_learning_rate: 0.04999752512250644\n",
      "epoch: 200, acc: 0.670, loss: 0.722, currect_learning_rate: 0.049995025494963256\n",
      "epoch: 300, acc: 0.727, loss: 0.621, currect_learning_rate: 0.049992526117345455\n",
      "epoch: 400, acc: 0.747, loss: 0.572, currect_learning_rate: 0.04999002698961558\n",
      "epoch: 500, acc: 0.753, loss: 0.534, currect_learning_rate: 0.049987528111736124\n",
      "epoch: 600, acc: 0.780, loss: 0.509, currect_learning_rate: 0.049985029483669646\n",
      "epoch: 700, acc: 0.807, loss: 0.479, currect_learning_rate: 0.049982531105378675\n",
      "epoch: 800, acc: 0.817, loss: 0.457, currect_learning_rate: 0.04998003297682575\n",
      "epoch: 900, acc: 0.837, loss: 0.447, currect_learning_rate: 0.049977535097973466\n",
      "epoch: 1000, acc: 0.827, loss: 0.432, currect_learning_rate: 0.04997503746878434\n",
      "epoch: 1100, acc: 0.823, loss: 0.424, currect_learning_rate: 0.049972540089220974\n",
      "epoch: 1200, acc: 0.837, loss: 0.419, currect_learning_rate: 0.04997004295924593\n",
      "epoch: 1300, acc: 0.823, loss: 0.413, currect_learning_rate: 0.04996754607882181\n",
      "epoch: 1400, acc: 0.830, loss: 0.410, currect_learning_rate: 0.049965049447911185\n",
      "epoch: 1500, acc: 0.840, loss: 0.403, currect_learning_rate: 0.04996255306647668\n",
      "epoch: 1600, acc: 0.833, loss: 0.394, currect_learning_rate: 0.049960056934480884\n",
      "epoch: 1700, acc: 0.833, loss: 0.392, currect_learning_rate: 0.04995756105188643\n",
      "epoch: 1800, acc: 0.833, loss: 0.390, currect_learning_rate: 0.04995506541865592\n",
      "epoch: 1900, acc: 0.833, loss: 0.384, currect_learning_rate: 0.04995257003475201\n",
      "epoch: 2000, acc: 0.823, loss: 0.386, currect_learning_rate: 0.049950074900137316\n",
      "epoch: 2100, acc: 0.830, loss: 0.378, currect_learning_rate: 0.0499475800147745\n",
      "epoch: 2200, acc: 0.840, loss: 0.371, currect_learning_rate: 0.0499450853786262\n",
      "epoch: 2300, acc: 0.837, loss: 0.368, currect_learning_rate: 0.0499425909916551\n",
      "epoch: 2400, acc: 0.857, loss: 0.360, currect_learning_rate: 0.04994009685382384\n",
      "epoch: 2500, acc: 0.863, loss: 0.360, currect_learning_rate: 0.04993760296509512\n",
      "epoch: 2600, acc: 0.860, loss: 0.347, currect_learning_rate: 0.049935109325431604\n",
      "epoch: 2700, acc: 0.863, loss: 0.342, currect_learning_rate: 0.049932615934796004\n",
      "epoch: 2800, acc: 0.860, loss: 0.339, currect_learning_rate: 0.04993012279315099\n",
      "epoch: 2900, acc: 0.857, loss: 0.336, currect_learning_rate: 0.049927629900459285\n",
      "epoch: 3000, acc: 0.870, loss: 0.330, currect_learning_rate: 0.049925137256683606\n",
      "epoch: 3100, acc: 0.870, loss: 0.333, currect_learning_rate: 0.04992264486178666\n",
      "epoch: 3200, acc: 0.863, loss: 0.314, currect_learning_rate: 0.04992015271573119\n",
      "epoch: 3300, acc: 0.863, loss: 0.311, currect_learning_rate: 0.04991766081847992\n",
      "epoch: 3400, acc: 0.873, loss: 0.324, currect_learning_rate: 0.049915169169995596\n",
      "epoch: 3500, acc: 0.873, loss: 0.302, currect_learning_rate: 0.049912677770240964\n",
      "epoch: 3600, acc: 0.877, loss: 0.308, currect_learning_rate: 0.049910186619178794\n",
      "epoch: 3700, acc: 0.877, loss: 0.296, currect_learning_rate: 0.04990769571677183\n",
      "epoch: 3800, acc: 0.887, loss: 0.293, currect_learning_rate: 0.04990520506298287\n",
      "epoch: 3900, acc: 0.893, loss: 0.288, currect_learning_rate: 0.04990271465777467\n",
      "epoch: 4000, acc: 0.893, loss: 0.285, currect_learning_rate: 0.049900224501110035\n",
      "epoch: 4100, acc: 0.890, loss: 0.283, currect_learning_rate: 0.04989773459295175\n",
      "epoch: 4200, acc: 0.900, loss: 0.293, currect_learning_rate: 0.049895244933262625\n",
      "epoch: 4300, acc: 0.900, loss: 0.282, currect_learning_rate: 0.04989275552200545\n",
      "epoch: 4400, acc: 0.900, loss: 0.274, currect_learning_rate: 0.04989026635914307\n",
      "epoch: 4500, acc: 0.870, loss: 0.286, currect_learning_rate: 0.049887777444638286\n",
      "epoch: 4600, acc: 0.867, loss: 0.278, currect_learning_rate: 0.049885288778453954\n",
      "epoch: 4700, acc: 0.897, loss: 0.272, currect_learning_rate: 0.049882800360552884\n",
      "epoch: 4800, acc: 0.900, loss: 0.268, currect_learning_rate: 0.04988031219089794\n",
      "epoch: 4900, acc: 0.897, loss: 0.265, currect_learning_rate: 0.04987782426945198\n",
      "epoch: 5000, acc: 0.897, loss: 0.266, currect_learning_rate: 0.04987533659617785\n",
      "epoch: 5100, acc: 0.877, loss: 0.264, currect_learning_rate: 0.049872849171038444\n",
      "epoch: 5200, acc: 0.910, loss: 0.266, currect_learning_rate: 0.0498703619939966\n",
      "epoch: 5300, acc: 0.877, loss: 0.258, currect_learning_rate: 0.04986787506501525\n",
      "epoch: 5400, acc: 0.910, loss: 0.255, currect_learning_rate: 0.049865388384057234\n",
      "epoch: 5500, acc: 0.907, loss: 0.268, currect_learning_rate: 0.049862901951085496\n",
      "epoch: 5600, acc: 0.870, loss: 0.254, currect_learning_rate: 0.049860415766062906\n",
      "epoch: 5700, acc: 0.883, loss: 0.251, currect_learning_rate: 0.0498579298289524\n",
      "epoch: 5800, acc: 0.913, loss: 0.251, currect_learning_rate: 0.04985544413971689\n",
      "epoch: 5900, acc: 0.893, loss: 0.248, currect_learning_rate: 0.049852958698319315\n",
      "epoch: 6000, acc: 0.910, loss: 0.261, currect_learning_rate: 0.04985047350472258\n",
      "epoch: 6100, acc: 0.910, loss: 0.244, currect_learning_rate: 0.04984798855888967\n",
      "epoch: 6200, acc: 0.913, loss: 0.247, currect_learning_rate: 0.0498455038607835\n",
      "epoch: 6300, acc: 0.917, loss: 0.242, currect_learning_rate: 0.049843019410367055\n",
      "epoch: 6400, acc: 0.907, loss: 0.243, currect_learning_rate: 0.04984053520760327\n",
      "epoch: 6500, acc: 0.883, loss: 0.242, currect_learning_rate: 0.049838051252455155\n",
      "epoch: 6600, acc: 0.917, loss: 0.252, currect_learning_rate: 0.049835567544885655\n",
      "epoch: 6700, acc: 0.873, loss: 0.244, currect_learning_rate: 0.04983308408485778\n",
      "epoch: 6800, acc: 0.883, loss: 0.237, currect_learning_rate: 0.0498306008723345\n",
      "epoch: 6900, acc: 0.880, loss: 0.243, currect_learning_rate: 0.04982811790727884\n",
      "epoch: 7000, acc: 0.903, loss: 0.238, currect_learning_rate: 0.04982563518965381\n",
      "epoch: 7100, acc: 0.890, loss: 0.233, currect_learning_rate: 0.049823152719422406\n",
      "epoch: 7200, acc: 0.883, loss: 0.235, currect_learning_rate: 0.04982067049654768\n",
      "epoch: 7300, acc: 0.920, loss: 0.237, currect_learning_rate: 0.04981818852099264\n",
      "epoch: 7400, acc: 0.917, loss: 0.230, currect_learning_rate: 0.049815706792720335\n",
      "epoch: 7500, acc: 0.923, loss: 0.234, currect_learning_rate: 0.049813225311693805\n",
      "epoch: 7600, acc: 0.920, loss: 0.232, currect_learning_rate: 0.04981074407787611\n",
      "epoch: 7700, acc: 0.897, loss: 0.235, currect_learning_rate: 0.049808263091230306\n",
      "epoch: 7800, acc: 0.897, loss: 0.225, currect_learning_rate: 0.04980578235171947\n",
      "epoch: 7900, acc: 0.883, loss: 0.230, currect_learning_rate: 0.04980330185930667\n",
      "epoch: 8000, acc: 0.900, loss: 0.223, currect_learning_rate: 0.04980082161395499\n",
      "epoch: 8100, acc: 0.900, loss: 0.230, currect_learning_rate: 0.04979834161562752\n",
      "epoch: 8200, acc: 0.920, loss: 0.227, currect_learning_rate: 0.04979586186428736\n",
      "epoch: 8300, acc: 0.923, loss: 0.223, currect_learning_rate: 0.04979338235989761\n",
      "epoch: 8400, acc: 0.923, loss: 0.228, currect_learning_rate: 0.04979090310242139\n",
      "epoch: 8500, acc: 0.930, loss: 0.219, currect_learning_rate: 0.049788424091821805\n",
      "epoch: 8600, acc: 0.893, loss: 0.222, currect_learning_rate: 0.049785945328062006\n",
      "epoch: 8700, acc: 0.900, loss: 0.235, currect_learning_rate: 0.0497834668111051\n",
      "epoch: 8800, acc: 0.933, loss: 0.215, currect_learning_rate: 0.049780988540914256\n",
      "epoch: 8900, acc: 0.897, loss: 0.218, currect_learning_rate: 0.0497785105174526\n",
      "epoch: 9000, acc: 0.930, loss: 0.218, currect_learning_rate: 0.04977603274068329\n",
      "epoch: 9100, acc: 0.923, loss: 0.220, currect_learning_rate: 0.04977355521056952\n",
      "epoch: 9200, acc: 0.923, loss: 0.223, currect_learning_rate: 0.04977107792707442\n",
      "epoch: 9300, acc: 0.897, loss: 0.236, currect_learning_rate: 0.0497686008901612\n",
      "epoch: 9400, acc: 0.910, loss: 0.224, currect_learning_rate: 0.04976612409979302\n",
      "epoch: 9500, acc: 0.897, loss: 0.232, currect_learning_rate: 0.0497636475559331\n",
      "epoch: 9600, acc: 0.897, loss: 0.219, currect_learning_rate: 0.049761171258544616\n",
      "epoch: 9700, acc: 0.897, loss: 0.222, currect_learning_rate: 0.04975869520759079\n",
      "epoch: 9800, acc: 0.893, loss: 0.218, currect_learning_rate: 0.04975621940303483\n",
      "epoch: 9900, acc: 0.930, loss: 0.211, currect_learning_rate: 0.04975374384483997\n",
      "epoch: 10000, acc: 0.917, loss: 0.207, currect_learning_rate: 0.04975126853296942\n"
     ]
    }
   ],
   "source": [
    "# now lets remove softmax also in this \n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    " \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    "activation2 = Activation_Softmax()\n",
    "# Create second Dense layer with 64 input features (as we take output  \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "optimizer = Optimizer_Adam(decay=5e-7,Learning_rate=0.05) \n",
    " \n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    "    # activation2.forward(dense2.output)\n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "    # loss = loss_activation.forward(activation2.output, y) \n",
    "    loss = loss_activation.forward(dense2.output, y)\n",
    "  \n",
    "\n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f}, ' +\n",
    "              f'currect_learning_rate: {optimizer.current_Learning_rate}')\n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    # activation2.backward(loss_activation.dinputs)\n",
    "    # dense2.backward(activation2.dinputs) \n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " # i noticed that softmax was not added in this so i made activation 2 and also applied it here but now its taking a lot more time, a lot more time.\n",
    "    # Update weights and biases \n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1) \n",
    "    optimizer.update_params(dense2) \n",
    "    optimizer.post_update_params()\n",
    "\n",
    "# without soft_max acc:92% and also it was more fast listen i am an idiot the soft mwax function is already in the soft+ cross entropy  so we are using softmax alsways it not missing\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "b2f8dc06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.817, loss:0.747\n"
     ]
    }
   ],
   "source": [
    "# now lets test the whole model, by out of sample data \n",
    "# in time sereis cases checking if testing data is truly different from sample it is necessary to check this or model will give goood results in both while not being genralized\n",
    "# so for this we are using same spiral dataset to create on emore data set for us \n",
    "X_test, y_test = spiral_data(samples = 100, classes =3)\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "predictions = np.argmax(loss_activation.output, axis =1)\n",
    "if len(y_test.shape) ==2:\n",
    "  y_test = np.argmax(y_test, axis = 1)\n",
    "accuracy = np.mean(predictions==y_test)\n",
    "print(f'validation, acc: {accuracy:.3f}, loss:{loss:.3f}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ab01798",
   "metadata": {},
   "outputs": [],
   "source": [
    "# we studied about different techniques like augmentation and what to do when data is less, how to test and validate,\n",
    "# now we will do L1 and L2 Regularization\n",
    "# Regularization methods are those which reduce generalization error.\n",
    "# L1 regularization's penalty is the sum of all the absolute values fro the weights and biases, (linear penalty)\n",
    "# l2reg. penalty is the sum of squares of weights and biases this penalize larger weights and biases more thatn the smaller one\n",
    "# l2 reg. is best \n",
    "# so loss becomes loss = dataloss + L1w + L1b + L2w + L2b\n",
    "# l1w = lambda_l1w*sum(abs(weights))\n",
    "# l2w = lambda_l2w*sum(weights**2)\n",
    "# l1b = lambda_l1b*sum(abs(biases))\n",
    "# l2b = lambda_l2b*sum(biases**2)\n",
    "# loss = data_loss + l1w + l2w + l1b +l2b\n",
    "# we will difine all the important lambdas in dense layer construction function \n",
    "def __init__ (self,n_inputs, n_neurons, weight_regularizer_l1 = 0, weight_regularizer_l2 = 0,bias_regularizer_l1 = 0, bias_regularizer_l2 = 0):\n",
    "  self.weights = 0.01*np.random.randn(inputs, n_neurons)\n",
    "  self.biases = np.zeros((1,n_neurons))\n",
    "# Set regularization strength \n",
    "  self.weight_regularizer_l1 = weight_regularizer_l1 \n",
    "  self.weight_regularizer_l2 = weight_regularizer_l2 \n",
    "  self.bias_regularizer_l1 = bias_regularizer_l1 \n",
    "  self.bias_regularizer_l2 = bias_regularizer_l2 \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "274f2f72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# regularization loss calculation\n",
    "def regularization_loss(self,layer):\n",
    "  regularization_loss = 0\n",
    "# L1 regularization - weights \n",
    "  # calculate only when factor greater than 0 \n",
    "  if layer.weight_regularizer_l1 > 0: \n",
    "      regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights))\n",
    "  if layer.weight_regularizer_l2 > 0: \n",
    "      regularization_loss += layer.weight_regularizer_l2 * np.sum(layer.weights *layer.weights) \n",
    "\n",
    "  # L1 regularization - biases \n",
    "  # calculate only when factor greater than 0 \n",
    "  if layer.bias_regularizer_l1 > 0: \n",
    "      regularization_loss += layer.bias_regularizer_l1 *np.sum(np.abs(layer.biases)) \n",
    "\n",
    "  # L2 regularization - biases \n",
    "  if layer.bias_regularizer_l2 > 0: \n",
    "      regularization_loss += layer.bias_regularizer_l2 *np.sum(layer.biases *layer.biases) \n",
    "\n",
    "  return regularization_loss\n",
    "\n",
    "# do this for all the layers in the neural networks and then add the loss in it for complete loss but \n",
    "# now overall loss has changed so we need a backward pass for L1 & L2 regularization also \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "10228fed",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1 1 1 1]\n",
      " [1 1 1 1]\n",
      " [1 1 1 1]]\n",
      "[[-1  1  1  1]\n",
      " [ 1  1 -1  1]\n",
      " [ 1  1  1 -1]]\n"
     ]
    }
   ],
   "source": [
    "weights = np.array([[-1,2,3,4],[1,2,-3,4],[1,2,3,-4]])\n",
    "dL1 = np.ones_like(weights)# makes the whole array 1\n",
    "print(dL1)\n",
    "dL1[weights<0] = -1\n",
    "print(dL1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "135881db",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Backward pass \n",
    "def backward(self, dvalues): \n",
    "    # Gradients on parameters \n",
    "    self.dweights = np.dot(self.inputs.T, dvalues) \n",
    "    self.dbiases = np.sum(dvalues, axis=0, keepdims=True) \n",
    "\n",
    "    # Gradients on regularization \n",
    "    # L1 on weights \n",
    "    if self.weight_regularizer_l1 > 0: # i dont get why only when the regularization constant are positive we are doin this \n",
    "        dL1 = np.ones_like(self.weights) \n",
    "        dL1[self.weights < 0] = -1 \n",
    "        self.dweights += self.weight_regularizer_l1 * dL1 \n",
    "    # L2 on weights \n",
    "    if self.weight_regularizer_l2 > 0: \n",
    "        self.dweights += 2 * self.weight_regularizer_l2 *  self.weights \n",
    "    # L1 on biases \n",
    "    if self.bias_regularizer_l1 > 0: \n",
    "        dL1 = np.ones_like(self.biases) \n",
    "        dL1[self.biases < 0] = -1 \n",
    "        self.dbiases += self.bias_regularizer_l1 * dL1 \n",
    "    # L2 on biases \n",
    "    if self.bias_regularizer_l2 > 0: \n",
    "        self.dbiases += 2 * self.bias_regularizer_l2 *  self.biases \n",
    "\n",
    "    # Gradient on values \n",
    "    self.dinputs = np.dot(dvalues, self.weights.T) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d418a78f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.320, loss: 1.099 (data_loss: 1.099, reg_loss: 0.000), currect_learning_rate: 0.05\n",
      "epoch: 100, acc: 0.433, loss: 1.117 (data_loss: 1.037, reg_loss: 0.080), currect_learning_rate: 0.04999752512250644\n",
      "epoch: 200, acc: 0.523, loss: 1.033 (data_loss: 0.973, reg_loss: 0.061), currect_learning_rate: 0.049995025494963256\n",
      "epoch: 300, acc: 0.600, loss: 0.952 (data_loss: 0.888, reg_loss: 0.064), currect_learning_rate: 0.049992526117345455\n",
      "epoch: 400, acc: 0.673, loss: 0.883 (data_loss: 0.808, reg_loss: 0.076), currect_learning_rate: 0.04999002698961558\n",
      "epoch: 500, acc: 0.717, loss: 0.826 (data_loss: 0.739, reg_loss: 0.087), currect_learning_rate: 0.049987528111736124\n",
      "epoch: 600, acc: 0.767, loss: 0.778 (data_loss: 0.682, reg_loss: 0.097), currect_learning_rate: 0.049985029483669646\n",
      "epoch: 700, acc: 0.797, loss: 0.741 (data_loss: 0.638, reg_loss: 0.104), currect_learning_rate: 0.049982531105378675\n",
      "epoch: 800, acc: 0.807, loss: 0.704 (data_loss: 0.595, reg_loss: 0.108), currect_learning_rate: 0.04998003297682575\n",
      "epoch: 900, acc: 0.790, loss: 0.682 (data_loss: 0.571, reg_loss: 0.111), currect_learning_rate: 0.049977535097973466\n",
      "epoch: 1000, acc: 0.767, loss: 0.680 (data_loss: 0.568, reg_loss: 0.113), currect_learning_rate: 0.04997503746878434\n",
      "epoch: 1100, acc: 0.807, loss: 0.633 (data_loss: 0.519, reg_loss: 0.114), currect_learning_rate: 0.049972540089220974\n",
      "epoch: 1200, acc: 0.820, loss: 0.620 (data_loss: 0.506, reg_loss: 0.115), currect_learning_rate: 0.04997004295924593\n",
      "epoch: 1300, acc: 0.823, loss: 0.600 (data_loss: 0.484, reg_loss: 0.115), currect_learning_rate: 0.04996754607882181\n",
      "epoch: 1400, acc: 0.853, loss: 0.587 (data_loss: 0.471, reg_loss: 0.116), currect_learning_rate: 0.049965049447911185\n",
      "epoch: 1500, acc: 0.857, loss: 0.569 (data_loss: 0.452, reg_loss: 0.117), currect_learning_rate: 0.04996255306647668\n",
      "epoch: 1600, acc: 0.830, loss: 0.560 (data_loss: 0.443, reg_loss: 0.117), currect_learning_rate: 0.049960056934480884\n",
      "epoch: 1700, acc: 0.853, loss: 0.544 (data_loss: 0.426, reg_loss: 0.118), currect_learning_rate: 0.04995756105188643\n",
      "epoch: 1800, acc: 0.853, loss: 0.543 (data_loss: 0.425, reg_loss: 0.118), currect_learning_rate: 0.04995506541865592\n",
      "epoch: 1900, acc: 0.850, loss: 0.532 (data_loss: 0.413, reg_loss: 0.119), currect_learning_rate: 0.04995257003475201\n",
      "epoch: 2000, acc: 0.843, loss: 0.519 (data_loss: 0.401, reg_loss: 0.119), currect_learning_rate: 0.049950074900137316\n",
      "epoch: 2100, acc: 0.863, loss: 0.503 (data_loss: 0.385, reg_loss: 0.119), currect_learning_rate: 0.0499475800147745\n",
      "epoch: 2200, acc: 0.863, loss: 0.504 (data_loss: 0.385, reg_loss: 0.119), currect_learning_rate: 0.0499450853786262\n",
      "epoch: 2300, acc: 0.883, loss: 0.485 (data_loss: 0.366, reg_loss: 0.119), currect_learning_rate: 0.0499425909916551\n",
      "epoch: 2400, acc: 0.880, loss: 0.482 (data_loss: 0.364, reg_loss: 0.118), currect_learning_rate: 0.04994009685382384\n",
      "epoch: 2500, acc: 0.863, loss: 0.482 (data_loss: 0.364, reg_loss: 0.118), currect_learning_rate: 0.04993760296509512\n",
      "epoch: 2600, acc: 0.893, loss: 0.463 (data_loss: 0.346, reg_loss: 0.118), currect_learning_rate: 0.049935109325431604\n",
      "epoch: 2700, acc: 0.887, loss: 0.463 (data_loss: 0.346, reg_loss: 0.117), currect_learning_rate: 0.049932615934796004\n",
      "epoch: 2800, acc: 0.880, loss: 0.463 (data_loss: 0.346, reg_loss: 0.117), currect_learning_rate: 0.04993012279315099\n",
      "epoch: 2900, acc: 0.890, loss: 0.447 (data_loss: 0.331, reg_loss: 0.116), currect_learning_rate: 0.049927629900459285\n",
      "epoch: 3000, acc: 0.893, loss: 0.444 (data_loss: 0.329, reg_loss: 0.115), currect_learning_rate: 0.049925137256683606\n",
      "epoch: 3100, acc: 0.883, loss: 0.446 (data_loss: 0.331, reg_loss: 0.115), currect_learning_rate: 0.04992264486178666\n",
      "epoch: 3200, acc: 0.897, loss: 0.432 (data_loss: 0.318, reg_loss: 0.114), currect_learning_rate: 0.04992015271573119\n",
      "epoch: 3300, acc: 0.880, loss: 0.433 (data_loss: 0.319, reg_loss: 0.113), currect_learning_rate: 0.04991766081847992\n",
      "epoch: 3400, acc: 0.907, loss: 0.422 (data_loss: 0.310, reg_loss: 0.113), currect_learning_rate: 0.049915169169995596\n",
      "epoch: 3500, acc: 0.907, loss: 0.418 (data_loss: 0.306, reg_loss: 0.113), currect_learning_rate: 0.049912677770240964\n",
      "epoch: 3600, acc: 0.887, loss: 0.420 (data_loss: 0.308, reg_loss: 0.112), currect_learning_rate: 0.049910186619178794\n",
      "epoch: 3700, acc: 0.910, loss: 0.414 (data_loss: 0.302, reg_loss: 0.111), currect_learning_rate: 0.04990769571677183\n",
      "epoch: 3800, acc: 0.893, loss: 0.413 (data_loss: 0.303, reg_loss: 0.110), currect_learning_rate: 0.04990520506298287\n",
      "epoch: 3900, acc: 0.893, loss: 0.408 (data_loss: 0.298, reg_loss: 0.110), currect_learning_rate: 0.04990271465777467\n",
      "epoch: 4000, acc: 0.917, loss: 0.407 (data_loss: 0.298, reg_loss: 0.109), currect_learning_rate: 0.049900224501110035\n",
      "epoch: 4100, acc: 0.860, loss: 0.438 (data_loss: 0.329, reg_loss: 0.109), currect_learning_rate: 0.04989773459295175\n",
      "epoch: 4200, acc: 0.913, loss: 0.394 (data_loss: 0.286, reg_loss: 0.108), currect_learning_rate: 0.049895244933262625\n",
      "epoch: 4300, acc: 0.913, loss: 0.392 (data_loss: 0.285, reg_loss: 0.107), currect_learning_rate: 0.04989275552200545\n",
      "epoch: 4400, acc: 0.907, loss: 0.391 (data_loss: 0.284, reg_loss: 0.107), currect_learning_rate: 0.04989026635914307\n",
      "epoch: 4500, acc: 0.907, loss: 0.386 (data_loss: 0.279, reg_loss: 0.107), currect_learning_rate: 0.049887777444638286\n",
      "epoch: 4600, acc: 0.893, loss: 0.393 (data_loss: 0.288, reg_loss: 0.106), currect_learning_rate: 0.049885288778453954\n",
      "epoch: 4700, acc: 0.917, loss: 0.380 (data_loss: 0.275, reg_loss: 0.105), currect_learning_rate: 0.049882800360552884\n",
      "epoch: 4800, acc: 0.893, loss: 0.388 (data_loss: 0.283, reg_loss: 0.105), currect_learning_rate: 0.04988031219089794\n",
      "epoch: 4900, acc: 0.920, loss: 0.377 (data_loss: 0.273, reg_loss: 0.104), currect_learning_rate: 0.04987782426945198\n",
      "epoch: 5000, acc: 0.907, loss: 0.379 (data_loss: 0.275, reg_loss: 0.104), currect_learning_rate: 0.04987533659617785\n",
      "epoch: 5100, acc: 0.897, loss: 0.382 (data_loss: 0.279, reg_loss: 0.103), currect_learning_rate: 0.049872849171038444\n",
      "epoch: 5200, acc: 0.917, loss: 0.373 (data_loss: 0.270, reg_loss: 0.103), currect_learning_rate: 0.0498703619939966\n",
      "epoch: 5300, acc: 0.900, loss: 0.378 (data_loss: 0.276, reg_loss: 0.102), currect_learning_rate: 0.04986787506501525\n",
      "epoch: 5400, acc: 0.913, loss: 0.370 (data_loss: 0.268, reg_loss: 0.102), currect_learning_rate: 0.049865388384057234\n",
      "epoch: 5500, acc: 0.883, loss: 0.394 (data_loss: 0.293, reg_loss: 0.101), currect_learning_rate: 0.049862901951085496\n",
      "epoch: 5600, acc: 0.913, loss: 0.361 (data_loss: 0.260, reg_loss: 0.101), currect_learning_rate: 0.049860415766062906\n",
      "epoch: 5700, acc: 0.923, loss: 0.358 (data_loss: 0.258, reg_loss: 0.100), currect_learning_rate: 0.0498579298289524\n",
      "epoch: 5800, acc: 0.893, loss: 0.368 (data_loss: 0.268, reg_loss: 0.100), currect_learning_rate: 0.04985544413971689\n",
      "epoch: 5900, acc: 0.917, loss: 0.358 (data_loss: 0.258, reg_loss: 0.100), currect_learning_rate: 0.049852958698319315\n",
      "epoch: 6000, acc: 0.903, loss: 0.389 (data_loss: 0.290, reg_loss: 0.099), currect_learning_rate: 0.04985047350472258\n",
      "epoch: 6100, acc: 0.910, loss: 0.363 (data_loss: 0.262, reg_loss: 0.101), currect_learning_rate: 0.04984798855888967\n",
      "epoch: 6200, acc: 0.920, loss: 0.348 (data_loss: 0.249, reg_loss: 0.099), currect_learning_rate: 0.0498455038607835\n",
      "epoch: 6300, acc: 0.923, loss: 0.346 (data_loss: 0.248, reg_loss: 0.098), currect_learning_rate: 0.049843019410367055\n",
      "epoch: 6400, acc: 0.920, loss: 0.346 (data_loss: 0.249, reg_loss: 0.097), currect_learning_rate: 0.04984053520760327\n",
      "epoch: 6500, acc: 0.917, loss: 0.353 (data_loss: 0.256, reg_loss: 0.097), currect_learning_rate: 0.049838051252455155\n",
      "epoch: 6600, acc: 0.923, loss: 0.343 (data_loss: 0.246, reg_loss: 0.097), currect_learning_rate: 0.049835567544885655\n",
      "epoch: 6700, acc: 0.923, loss: 0.341 (data_loss: 0.244, reg_loss: 0.097), currect_learning_rate: 0.04983308408485778\n",
      "epoch: 6800, acc: 0.927, loss: 0.342 (data_loss: 0.245, reg_loss: 0.096), currect_learning_rate: 0.0498306008723345\n",
      "epoch: 6900, acc: 0.927, loss: 0.336 (data_loss: 0.241, reg_loss: 0.096), currect_learning_rate: 0.04982811790727884\n",
      "epoch: 7000, acc: 0.917, loss: 0.345 (data_loss: 0.249, reg_loss: 0.096), currect_learning_rate: 0.04982563518965381\n",
      "epoch: 7100, acc: 0.900, loss: 0.352 (data_loss: 0.257, reg_loss: 0.095), currect_learning_rate: 0.049823152719422406\n",
      "epoch: 7200, acc: 0.927, loss: 0.332 (data_loss: 0.237, reg_loss: 0.095), currect_learning_rate: 0.04982067049654768\n",
      "epoch: 7300, acc: 0.923, loss: 0.336 (data_loss: 0.242, reg_loss: 0.094), currect_learning_rate: 0.04981818852099264\n",
      "epoch: 7400, acc: 0.920, loss: 0.343 (data_loss: 0.249, reg_loss: 0.094), currect_learning_rate: 0.049815706792720335\n",
      "epoch: 7500, acc: 0.917, loss: 0.334 (data_loss: 0.240, reg_loss: 0.094), currect_learning_rate: 0.049813225311693805\n",
      "epoch: 7600, acc: 0.920, loss: 0.337 (data_loss: 0.244, reg_loss: 0.093), currect_learning_rate: 0.04981074407787611\n",
      "epoch: 7700, acc: 0.910, loss: 0.337 (data_loss: 0.243, reg_loss: 0.093), currect_learning_rate: 0.049808263091230306\n",
      "epoch: 7800, acc: 0.927, loss: 0.326 (data_loss: 0.233, reg_loss: 0.093), currect_learning_rate: 0.04980578235171947\n",
      "epoch: 7900, acc: 0.927, loss: 0.323 (data_loss: 0.230, reg_loss: 0.093), currect_learning_rate: 0.04980330185930667\n",
      "epoch: 8000, acc: 0.927, loss: 0.322 (data_loss: 0.230, reg_loss: 0.092), currect_learning_rate: 0.04980082161395499\n",
      "epoch: 8100, acc: 0.927, loss: 0.320 (data_loss: 0.228, reg_loss: 0.092), currect_learning_rate: 0.04979834161562752\n",
      "epoch: 8200, acc: 0.913, loss: 0.330 (data_loss: 0.238, reg_loss: 0.092), currect_learning_rate: 0.04979586186428736\n",
      "epoch: 8300, acc: 0.590, loss: 1.991 (data_loss: 1.900, reg_loss: 0.091), currect_learning_rate: 0.04979338235989761\n",
      "epoch: 8400, acc: 0.933, loss: 0.337 (data_loss: 0.230, reg_loss: 0.107), currect_learning_rate: 0.04979090310242139\n",
      "epoch: 8500, acc: 0.930, loss: 0.328 (data_loss: 0.227, reg_loss: 0.101), currect_learning_rate: 0.049788424091821805\n",
      "epoch: 8600, acc: 0.933, loss: 0.324 (data_loss: 0.226, reg_loss: 0.098), currect_learning_rate: 0.049785945328062006\n",
      "epoch: 8700, acc: 0.930, loss: 0.322 (data_loss: 0.226, reg_loss: 0.096), currect_learning_rate: 0.0497834668111051\n",
      "epoch: 8800, acc: 0.930, loss: 0.320 (data_loss: 0.225, reg_loss: 0.094), currect_learning_rate: 0.049780988540914256\n",
      "epoch: 8900, acc: 0.933, loss: 0.319 (data_loss: 0.226, reg_loss: 0.093), currect_learning_rate: 0.0497785105174526\n",
      "epoch: 9000, acc: 0.927, loss: 0.319 (data_loss: 0.226, reg_loss: 0.093), currect_learning_rate: 0.04977603274068329\n",
      "epoch: 9100, acc: 0.930, loss: 0.320 (data_loss: 0.227, reg_loss: 0.093), currect_learning_rate: 0.04977355521056952\n",
      "epoch: 9200, acc: 0.933, loss: 0.315 (data_loss: 0.223, reg_loss: 0.092), currect_learning_rate: 0.04977107792707442\n",
      "epoch: 9300, acc: 0.923, loss: 0.321 (data_loss: 0.229, reg_loss: 0.091), currect_learning_rate: 0.0497686008901612\n",
      "epoch: 9400, acc: 0.933, loss: 0.313 (data_loss: 0.222, reg_loss: 0.091), currect_learning_rate: 0.04976612409979302\n",
      "epoch: 9500, acc: 0.930, loss: 0.312 (data_loss: 0.221, reg_loss: 0.091), currect_learning_rate: 0.0497636475559331\n",
      "epoch: 9600, acc: 0.933, loss: 0.317 (data_loss: 0.226, reg_loss: 0.091), currect_learning_rate: 0.049761171258544616\n",
      "epoch: 9700, acc: 0.923, loss: 0.326 (data_loss: 0.236, reg_loss: 0.090), currect_learning_rate: 0.04975869520759079\n",
      "epoch: 9800, acc: 0.920, loss: 0.318 (data_loss: 0.228, reg_loss: 0.090), currect_learning_rate: 0.04975621940303483\n",
      "epoch: 9900, acc: 0.933, loss: 0.311 (data_loss: 0.221, reg_loss: 0.090), currect_learning_rate: 0.04975374384483997\n",
      "epoch: 10000, acc: 0.930, loss: 0.309 (data_loss: 0.219, reg_loss: 0.090), currect_learning_rate: 0.04975126853296942\n"
     ]
    }
   ],
   "source": [
    "# so full code up to now \n",
    "# now lets add all the classes and code up to this point in one cell\n",
    "import numpy as np \n",
    "import nnfs \n",
    "from nnfs.datasets import spiral_data \n",
    " \n",
    "nnfs.init() \n",
    " \n",
    " \n",
    "# Dense layer \n",
    "class Layer_Dense: \n",
    " \n",
    "    # Layer initialization \n",
    "    def __init__(self, n_inputs, n_neurons,weight_regularizer_l1 =0,weight_regularizer_l2=0,bias_regularizer_l1 =0, bias_regularizer_l2=0): \n",
    "        # Initialize weights and biases \n",
    "        self.weights = 0.01 * np.random.randn(n_inputs, n_neurons) \n",
    "        self.biases = np.zeros((1, n_neurons)) \n",
    "        # Set regularization strength \n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1 \n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2 \n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1 \n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2 \n",
    "     # Forward pass \n",
    "    def forward(self, inputs): \n",
    "        # Remember input values \n",
    "        self.inputs = inputs \n",
    "        # Calculate output values from inputs, weights and biases \n",
    "        self.output = np.dot(inputs, self.weights) + self.biases \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues): \n",
    "        # Gradients on parameters \n",
    "        self.dweights = np.dot(self.inputs.T, dvalues) \n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True) \n",
    "        # Gradients on regularization \n",
    "    # L1 on weights \n",
    "        if self.weight_regularizer_l1 > 0: # i dont get why only when the regularization constant are positive we are doin this \n",
    "            dL1 = np.ones_like(self.weights) \n",
    "            dL1[self.weights < 0] = -1 \n",
    "            self.dweights += self.weight_regularizer_l1 * dL1 \n",
    "        # L2 on weights \n",
    "        if self.weight_regularizer_l2 > 0: \n",
    "            self.dweights += 2 * self.weight_regularizer_l2 *  self.weights \n",
    "        # L1 on biases \n",
    "        if self.bias_regularizer_l1 > 0: \n",
    "            dL1 = np.ones_like(self.biases) \n",
    "            dL1[self.biases < 0] = -1 \n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1 \n",
    "        # L2 on biases \n",
    "        if self.bias_regularizer_l2 > 0: \n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 *  self.biases \n",
    "        # Gradient on values \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T) \n",
    " \n",
    " \n",
    "# ReLU activation \n",
    "class Activation_ReLU: \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs):\n",
    " # Remember input values \n",
    "        self.inputs = inputs \n",
    "        # Calculate output values from inputs \n",
    "        self.output = np.maximum(0, inputs) \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues): \n",
    "        # Since we need to modify original variable, \n",
    "        # let's make a copy of values first \n",
    "        self.dinputs = dvalues.copy() \n",
    " \n",
    "        # Zero gradient where input values were negative \n",
    "        self.dinputs[self.inputs <= 0] = 0 \n",
    " \n",
    " \n",
    "# Softmax activation \n",
    "class Activation_Softmax: \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs): \n",
    "        # Remember input values \n",
    "        self.inputs = inputs \n",
    " \n",
    "        # Get unnormalized probabilities \n",
    "        exp_values = np.exp(inputs - np.max(inputs, axis=1, \n",
    "                                            keepdims=True)) \n",
    "        # Normalize them for each sample \n",
    "        probabilities = exp_values / np.sum(exp_values, axis=1, \n",
    "                                            keepdims=True) \n",
    " \n",
    "        self.output = probabilities \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues): \n",
    " \n",
    "        # Create uninitialized array \n",
    "        self.dinputs = np.empty_like(dvalues) \n",
    " \n",
    "        # Enumerate outputs and gradients \n",
    "        for index, (single_output, single_dvalues) in enumerate(zip(self.output, dvalues)): \n",
    "            # Flatten output array \n",
    "            single_output = single_output.reshape(-1, 1) \n",
    "            # Calculate Jacobian matrix of the output and \n",
    "            jacobian_matrix = np.diagflat(single_output) - np.dot(single_output, single_output.T) \n",
    " # Calculate sample-wise gradient \n",
    "            # and add it to the array of sample gradients \n",
    "            self.dinputs[index] = np.dot(jacobian_matrix,single_dvalues) \n",
    " \n",
    " \n",
    "# SGD optimizer \n",
    "class Optimizer_SGD: \n",
    " \n",
    "    # Initialize optimizer - set settings,  \n",
    "    # learning rate of 1. is default for this optimizer \n",
    "    def __init__(self, learning_rate=1., decay=0., momentum=0.): \n",
    "        self.learning_rate = learning_rate \n",
    "        self.current_learning_rate = learning_rate \n",
    "        self.decay = decay \n",
    "        self.iterations = 0 \n",
    "        self.momentum = momentum \n",
    " \n",
    "    # Call once before any parameter updates \n",
    "    def pre_update_params(self): \n",
    "        if self.decay: \n",
    "            self.current_learning_rate = self.learning_rate * (1. / (1. + self.decay * self.iterations)) \n",
    " \n",
    "    # Update parameters \n",
    "    def update_params(self, layer): \n",
    " \n",
    "        # If we use momentum \n",
    "        if self.momentum: \n",
    " \n",
    "            # If layer does not contain momentum arrays, create them \n",
    "            # filled with zeros \n",
    "            if not hasattr(layer, 'weight_momentums'): \n",
    "                layer.weight_momentums = np.zeros_like(layer.weights) \n",
    "                # If there is no momentum array for weights \n",
    "                # The array doesn't exist for biases yet either. \n",
    "                layer.bias_momentums = np.zeros_like(layer.biases) \n",
    " \n",
    "            # Build weight updates with momentum - take previous \n",
    "            # updates multiplied by retain factor and update with \n",
    "            # current gradients \n",
    "            weight_updates = self.momentum * layer.weight_momentums - self.current_learning_rate * layer.dweights \n",
    "            layer.weight_momentums = weight_updates \n",
    " \n",
    "            # Build bias updates \n",
    "            bias_updates = self.momentum * layer.bias_momentums - self.current_learning_rate * layer.dbiases \n",
    "            layer.bias_momentums = bias_updates \n",
    " \n",
    "\n",
    "        # Vanilla SGD updates (as before momentum update) \n",
    "        else: \n",
    "            weight_updates = -self.current_learning_rate * layer.dweights \n",
    "            bias_updates = -self.current_learning_rate *layer.dbiases \n",
    " \n",
    "        # Update weights and biases using either \n",
    "        # vanilla or momentum updates \n",
    "        layer.weights += weight_updates \n",
    "        layer.biases += bias_updates \n",
    " \n",
    "    # Call once after any parameter updates \n",
    "    def post_update_params(self): \n",
    "        self.iterations += 1\n",
    "\n",
    "\n",
    "class Optimizer_Adagrad: \n",
    " \n",
    "    # Initialize optimizer - set settings \n",
    "    def __init__(self, learning_rate=1., decay=0., epsilon=1e-7): \n",
    "        self.learning_rate = learning_rate \n",
    "        self.current_learning_rate = learning_rate \n",
    "        self.decay = decay \n",
    "        self.iterations = 0 \n",
    "        self.epsilon = epsilon \n",
    " \n",
    "    # Call once before any parameter updates \n",
    "    def pre_update_params(self): \n",
    "        if self.decay: \n",
    "            self.current_learning_rate = self.learning_rate *   (1. / (1. + self.decay * self.iterations)) \n",
    " \n",
    "\n",
    "    # Update parameters \n",
    "    def update_params(self, layer): \n",
    " \n",
    "        # If layer does not contain cache arrays, \n",
    "        # create them filled with zeros \n",
    "        if not hasattr(layer, 'weight_cache'): \n",
    "            layer.weight_cache = np.zeros_like(layer.weights) \n",
    "            layer.bias_cache = np.zeros_like(layer.biases) \n",
    " \n",
    "        # Update cache with squared current gradients \n",
    "        layer.weight_cache += layer.dweights**2 \n",
    "        layer.bias_cache += layer.dbiases**2 \n",
    " \n",
    "        # Vanilla SGD parameter update + normalization \n",
    "        # with square rooted cache \n",
    "        layer.weights += -self.current_learning_rate *layer.dweights /(np.sqrt(layer.weight_cache) + self.epsilon) \n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases /(np.sqrt(layer.bias_cache) + self.epsilon) \n",
    " \n",
    "    # Call once after any parameter updates \n",
    "    def post_update_params(self): \n",
    "        self.iterations += 1 \n",
    "# RMSprop optimizer \n",
    "class Optimizer_RMSprop: \n",
    " \n",
    "    # Initialize optimizer - set settings \n",
    "    def __init__(self, learning_rate=0.001, decay=0., epsilon=1e-7, \n",
    "                 rho=0.9): \n",
    "        self.learning_rate = learning_rate \n",
    "        self.current_learning_rate = learning_rate \n",
    "        self.decay = decay \n",
    "        self.iterations = 0 \n",
    "        self.epsilon = epsilon \n",
    "        self.rho = rho \n",
    " \n",
    "    # Call once before any parameter updates \n",
    "    def pre_update_params(self): \n",
    "        if self.decay: \n",
    "            self.current_learning_rate = self.learning_rate *  (1. / (1. + self.decay * self.iterations)) \n",
    " \n",
    "    # Update parameters \n",
    "    def update_params(self, layer): \n",
    " \n",
    "        # If layer does not contain cache arrays, \n",
    "        # create them filled with zeros \n",
    "        if not hasattr(layer, 'weight_cache'): \n",
    "            layer.weight_cache = np.zeros_like(layer.weights) \n",
    "            layer.bias_cache = np.zeros_like(layer.biases) \n",
    " \n",
    "        # Update cache with squared current gradients \n",
    "        layer.weight_cache = self.rho * layer.weight_cache + (1 - self.rho) * layer.dweights**2 \n",
    "        layer.bias_cache = self.rho * layer.bias_cache +  (1 - self.rho) * layer.dbiases**2 \n",
    " \n",
    "        # Vanilla SGD parameter update + normalization \n",
    "        # with square rooted cache \n",
    "        layer.weights += -self.current_learning_rate *  layer.dweights / (np.sqrt(layer.weight_cache) + self.epsilon) \n",
    "        layer.biases += -self.current_learning_rate * layer.dbiases /  (np.sqrt(layer.bias_cache) + self.epsilon) \n",
    " \n",
    "    # Call once after any parameter updates \n",
    "    def post_update_params(self): \n",
    "        self.iterations += 1 \n",
    "\n",
    "class Optimizer_Adam:\n",
    "  def __init__(self, Learning_rate=0.001, decay=0., epsilon=1e-7, beta_1 = 0.9, beta_2 = 0.999):\n",
    "    self.Learning_rate = Learning_rate\n",
    "    self.decay = decay\n",
    "    self.current_Learning_rate = Learning_rate\n",
    "    self.iterations = 0\n",
    "    self.beta_1 = beta_1\n",
    "    self.beta_2 = beta_2\n",
    "    self.epsilon = epsilon\n",
    "  def pre_update_params(self):\n",
    "    if self.decay:\n",
    "      self.current_Learning_rate = self.Learning_rate /(1+self.decay*self.iterations)\n",
    "\n",
    "  def update_params(self, Layer):\n",
    "    if not hasattr(Layer, 'weight_cache'):\n",
    "      Layer.weight_momentums = np.zeros_like(Layer.weights) \n",
    "      Layer.weight_cache = np.zeros_like(Layer.weights) \n",
    "      Layer.bias_momentums = np.zeros_like(Layer.biases) \n",
    "      Layer.bias_cache = np.zeros_like(Layer.biases) \n",
    "    Layer.weight_momentums = self.beta_1 *Layer.weight_momentums + (1-self.beta_1)*Layer.dweights\n",
    "    Layer.bias_momentums = self.beta_1 *Layer.bias_momentums + (1-self.beta_1)*Layer.dbiases\n",
    "    weight_momentums_corrected = Layer.weight_momentums/(1-self.beta_1**(self.iterations + 1))\n",
    "    bias_momentums_corrected = Layer.bias_momentums/(1-self.beta_1**(self.iterations+1))\n",
    "\n",
    "    Layer.weight_cache = self.beta_2 *Layer.weight_cache + (1-self.beta_2)*Layer.dweights**2\n",
    "    Layer.bias_cache = self.beta_2 * Layer.bias_cache + (1-self.beta_2)*Layer.dbiases**2\n",
    "    weight_cache_corrected = Layer.weight_cache/(1-self.beta_2**(self.iterations + 1))\n",
    "    bias_cache_corrected = Layer.bias_cache/(1-self.beta_2**(self.iterations + 1))\n",
    "\n",
    "    Layer.weights += -self.current_Learning_rate*weight_momentums_corrected/(np.sqrt(bias_cache_corrected)+ self.epsilon)\n",
    "    Layer.biases += -self.current_Learning_rate*bias_momentums_corrected/(np.sqrt(bias_cache_corrected) + self.epsilon)\n",
    "  def post_update_params(self):\n",
    "    self.iterations += 1\n",
    "\n",
    "\n",
    "# Common loss class \n",
    "class Loss: \n",
    " \n",
    "    # Calculates the data and regularization losses\n",
    "     # Regularization loss calculation \n",
    "    def regularization_loss(self, layer): \n",
    " \n",
    "        # 0 by default \n",
    "        regularization_loss = 0 \n",
    " \n",
    "        # L1 regularization - weights \n",
    "        # calculate only when factor greater than 0 \n",
    "        if layer.weight_regularizer_l1 > 0: \n",
    "            regularization_loss += layer.weight_regularizer_l1 * np.sum(np.abs(layer.weights)) \n",
    " \n",
    "        # L2 regularization - weights \n",
    "        if layer.weight_regularizer_l2 > 0: \n",
    "            regularization_loss += layer.weight_regularizer_l2 *np.sum(layer.weights * layer.weights) \n",
    " \n",
    "  \n",
    " \n",
    "        # L1 regularization - biases \n",
    "        # calculate only when factor greater than 0 \n",
    "        if layer.bias_regularizer_l1 > 0: \n",
    "            regularization_loss += layer.bias_regularizer_l1 *np.sum(np.abs(layer.biases)) \n",
    " \n",
    "        # L2 regularization - biases \n",
    "        if layer.bias_regularizer_l2 > 0: \n",
    "            regularization_loss += layer.bias_regularizer_l2 * np.sum(layer.biases * layer.biases) \n",
    " \n",
    "        return regularization_loss  \n",
    "    # given model output and ground truth values \n",
    "    def calculate(self, output, y): \n",
    " \n",
    "        # Calculate sample losses \n",
    "        sample_losses = self.forward(output, y) \n",
    " \n",
    "        # Calculate mean loss \n",
    "        data_loss = np.mean(sample_losses) \n",
    " \n",
    "        # Return loss \n",
    "        return data_loss \n",
    "    \n",
    " \n",
    "# Cross-entropy loss \n",
    "class Loss_CategoricalCrossentropy(Loss): \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, y_pred, y_true): \n",
    " \n",
    "        # Number of samples in a batch \n",
    "        samples = len(y_pred) \n",
    " \n",
    "        # Clip data to prevent division by 0 \n",
    "        # Clip both sides to not drag mean towards any value \n",
    "        y_pred_clipped = np.clip(y_pred, 1e-7, 1 - 1e-7) \n",
    " \n",
    "        # Probabilities for target values - \n",
    "        # only if categorical labels \n",
    "        if len(y_true.shape) == 1: \n",
    "            correct_confidences = y_pred_clipped[ \n",
    "                range(samples), \n",
    "                y_true \n",
    "            ] \n",
    " # Mask values - only for one-hot encoded labels \n",
    "        elif len(y_true.shape) == 2: \n",
    "            correct_confidences = np.sum( \n",
    "                y_pred_clipped * y_true, \n",
    "                axis=1 \n",
    "            ) \n",
    " \n",
    "        # Losses \n",
    "        negative_log_likelihoods = -np.log(correct_confidences) \n",
    "        return negative_log_likelihoods \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues, y_true): \n",
    " \n",
    "        # Number of samples \n",
    "        samples = len(dvalues) \n",
    "        # Number of labels in every sample \n",
    "        # We'll use the first sample to count them \n",
    "        labels = len(dvalues[0]) \n",
    " \n",
    "        # If labels are sparse, turn them into one-hot vector \n",
    "        if len(y_true.shape) == 1: \n",
    "            y_true = np.eye(labels)[y_true] \n",
    " \n",
    "        # Calculate gradient \n",
    "        self.dinputs = -y_true / dvalues \n",
    "        # Normalize gradient \n",
    "        self.dinputs = self.dinputs / samples \n",
    " \n",
    " \n",
    "# Softmax classifier - combined Softmax activation \n",
    "# and cross-entropy loss for faster backward step \n",
    "class Activation_Softmax_Loss_CategoricalCrossentropy(): \n",
    " \n",
    "    # Creates activation and loss function objects \n",
    "    def __init__(self): \n",
    "        self.activation = Activation_Softmax() \n",
    "        self.loss = Loss_CategoricalCrossentropy() \n",
    " \n",
    "    # Forward pass \n",
    "    def forward(self, inputs, y_true): \n",
    "        # Output layer's activation function \n",
    "        self.activation.forward(inputs) \n",
    "        # Set the output \n",
    "        self.output = self.activation.output \n",
    "        # Calculate and return loss value \n",
    "        return self.loss.calculate(self.output, y_true) \n",
    "\n",
    "  # Backward pass \n",
    "    def backward(self, dvalues, y_true): \n",
    " \n",
    "        # Number of samples \n",
    "        samples = len(dvalues) \n",
    " \n",
    "        # If labels are one-hot encoded, \n",
    "        # turn them into discrete values \n",
    "        if len(y_true.shape) == 2: \n",
    "            y_true = np.argmax(y_true, axis=1) \n",
    " \n",
    "        # Copy so we can safely modify \n",
    "        self.dinputs = dvalues.copy() \n",
    "        # Calculate gradient \n",
    "        self.dinputs[range(samples), y_true] -= 1 \n",
    "        # Normalize gradient \n",
    "        self.dinputs = self.dinputs / samples \n",
    "\n",
    "# now lets add softmax also in this \n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    " \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 256,weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    "activation2 = Activation_Softmax()\n",
    "# Create second Dense layer with 64 input features (as we take output  \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(256, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "optimizer = Optimizer_Adam(decay=5e-7,Learning_rate=0.05) \n",
    " \n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    " \n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(activation1.output) \n",
    "\n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "  \n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "     # Calculate regularization penalty \n",
    "    regularization_loss = loss_activation.loss.regularization_loss(dense1) + loss_activation.loss.regularization_loss(dense2) \n",
    "    #overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f} (' + \n",
    "              f'data_loss: {data_loss:.3f}, ' + \n",
    "              f'reg_loss: {regularization_loss:.3f}), ' + \n",
    "              f'currect_learning_rate: {optimizer.current_Learning_rate}')\n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    activation1.backward(dense2.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " # i noticed that softmax was not added in this so i made activation 2 and also applied it here but now its taking a lot more time, a lot more time.\n",
    "    # Update weights and biases \n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1) \n",
    "    optimizer.update_params(dense2) \n",
    "    optimizer.post_update_params()\n",
    "    # with softmax_activations acc: 78%"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "ae1770e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.813, loss: 0.448\n"
     ]
    }
   ],
   "source": [
    "# validating the model\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "predictions = np.argmax(loss_activation.output,axis=1)\n",
    "if(len(y_test.shape)==2):\n",
    "  y_test = np.argmax(y_test,axis=1)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}') \n",
    "# increasing the number of neurons did help and also bringing the regu. in made the validation accuracy match the trainig accuracy removing overfitting,\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81240d20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, -1.03, 0, 0.99, 0, -0.37, 0, 0, -0.07, 0.73]\n"
     ]
    }
   ],
   "source": [
    "# now, lets study DROPOUT layer tech.\n",
    "import random\n",
    "dropout_rate = 0.5\n",
    "example_output = [0.27,-1.03,0.67,0.99,0.05,-0.37,-2.01,1.13,-00.07,0.73]\n",
    "while True:\n",
    "  index = random.randint(0,len(example_output)-1)\n",
    "  example_output[index]=0\n",
    "  dropped_out = 0\n",
    "  for value in example_output:\n",
    "    if value==0:\n",
    "      dropped_out +=1\n",
    "  if dropped_out/len(example_output)>= dropout_rate:\n",
    "    break\n",
    "print(example_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a687925",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[11 10  7  8 14  4 10 12  8 10]\n"
     ]
    }
   ],
   "source": [
    "n, p = 20, .5  # number of trials, probability of each trial\n",
    "s = np.random.binomial(n, p, 100)# takes the n and p vlaue and perform the experiment n times and put the results as number of success , and this whole things happens for size number of times adding size no. of numbers in array\n",
    "print(s)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "5ba5ab92",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.54 -0.    1.34  0.    0.  ]\n",
      " [-0.74 -0.    2.26 -0.14  0.  ]]\n"
     ]
    }
   ],
   "source": [
    "dropout_rate = 0.5\n",
    "example_output = np.array([[0.27, -1.03, 0.67, 0.99, 0.05],[ -0.37, -2.01, 1.13, -0.07, 0.73]])\n",
    "# example_output *= np.random.binomial(1, 1-dropout_rate, example_output.shape)# so it to one trial so output in array is out*1 or out*0 1- dropoutrate is probability of success and total number we want this is example.output.shape\n",
    "# print(example_output)\n",
    "# to ensure no change in sum happens for detail look in book\n",
    "example_output *= np.random.binomial(1,1-dropout_rate,example_output.shape)/(1-dropout_rate)\n",
    "print(example_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "cb3cbde7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sum initial [ 1.27  0.97  3.67 -3.01  5.05  0.33  5.99  1.53  0.53  1.63]\n",
      "mean sum: 1.800353875\n"
     ]
    }
   ],
   "source": [
    "\n",
    "dropout_rate = 0.2 \n",
    "example_output = np.array([[0.27, -1.03, 0.67, 0.99, 0.05, -0.37, -2.01, 1.13, -0.07, 0.73],[1,2,3,-4,5,0.7,8,0.4,0.6,0.9]])  \n",
    "print(f'sum initial {sum(example_output)}') \n",
    " \n",
    "sums = [] \n",
    "for i in range(10000): \n",
    " \n",
    "    example_output2 = example_output * np.random.binomial(1, 1-dropout_rate, example_output.shape) / (1-dropout_rate) \n",
    "    sums.append(sum(example_output2)) \n",
    "    \n",
    " \n",
    "print(f'mean sum: {np.mean(sums)}') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "3d4d04a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# backward pass for this change\n",
    "class Layer_dropout:\n",
    "  def __init__(self,rate):\n",
    "    self.rate = 1- rate\n",
    "  def forward(self, inputs):\n",
    "    self.inputs = inputs\n",
    "    self.binary_mask = np.random.binomial(1,self.rate, size = inputs.shape)/self.rate\n",
    "    self.output = inputs*self.binary_mask\n",
    "  def backward(self,dvalues):\n",
    "    self.dinputs = dvalues*self.binary_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "42010583",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.330, loss: 1.099 (data_loss: 1.099, reg_loss: 0.000), currect_learning_rate: 0.05\n",
      "epoch: 100, acc: 0.483, loss: 0.989 (data_loss: 0.979, reg_loss: 0.010), currect_learning_rate: 0.04999752512250644\n",
      "epoch: 200, acc: 0.600, loss: 0.933 (data_loss: 0.909, reg_loss: 0.024), currect_learning_rate: 0.049995025494963256\n",
      "epoch: 300, acc: 0.600, loss: 0.913 (data_loss: 0.879, reg_loss: 0.034), currect_learning_rate: 0.049992526117345455\n",
      "epoch: 400, acc: 0.587, loss: 0.914 (data_loss: 0.874, reg_loss: 0.040), currect_learning_rate: 0.04999002698961558\n",
      "epoch: 500, acc: 0.593, loss: 0.890 (data_loss: 0.845, reg_loss: 0.045), currect_learning_rate: 0.049987528111736124\n",
      "epoch: 600, acc: 0.563, loss: 0.869 (data_loss: 0.821, reg_loss: 0.048), currect_learning_rate: 0.049985029483669646\n",
      "epoch: 700, acc: 0.627, loss: 0.833 (data_loss: 0.784, reg_loss: 0.049), currect_learning_rate: 0.049982531105378675\n",
      "epoch: 800, acc: 0.607, loss: 0.862 (data_loss: 0.810, reg_loss: 0.052), currect_learning_rate: 0.04998003297682575\n",
      "epoch: 900, acc: 0.653, loss: 0.813 (data_loss: 0.760, reg_loss: 0.054), currect_learning_rate: 0.049977535097973466\n",
      "epoch: 1000, acc: 0.637, loss: 0.841 (data_loss: 0.786, reg_loss: 0.055), currect_learning_rate: 0.04997503746878434\n",
      "epoch: 1100, acc: 0.673, loss: 0.827 (data_loss: 0.772, reg_loss: 0.055), currect_learning_rate: 0.049972540089220974\n",
      "epoch: 1200, acc: 0.650, loss: 0.802 (data_loss: 0.747, reg_loss: 0.055), currect_learning_rate: 0.04997004295924593\n",
      "epoch: 1300, acc: 0.647, loss: 0.803 (data_loss: 0.749, reg_loss: 0.054), currect_learning_rate: 0.04996754607882181\n",
      "epoch: 1400, acc: 0.677, loss: 0.816 (data_loss: 0.762, reg_loss: 0.054), currect_learning_rate: 0.049965049447911185\n",
      "epoch: 1500, acc: 0.620, loss: 0.820 (data_loss: 0.766, reg_loss: 0.054), currect_learning_rate: 0.04996255306647668\n",
      "epoch: 1600, acc: 0.690, loss: 0.783 (data_loss: 0.729, reg_loss: 0.054), currect_learning_rate: 0.049960056934480884\n",
      "epoch: 1700, acc: 0.653, loss: 0.796 (data_loss: 0.741, reg_loss: 0.054), currect_learning_rate: 0.04995756105188643\n",
      "epoch: 1800, acc: 0.677, loss: 0.797 (data_loss: 0.744, reg_loss: 0.053), currect_learning_rate: 0.04995506541865592\n",
      "epoch: 1900, acc: 0.643, loss: 0.755 (data_loss: 0.702, reg_loss: 0.053), currect_learning_rate: 0.04995257003475201\n",
      "epoch: 2000, acc: 0.650, loss: 0.776 (data_loss: 0.723, reg_loss: 0.053), currect_learning_rate: 0.049950074900137316\n",
      "epoch: 2100, acc: 0.697, loss: 0.769 (data_loss: 0.715, reg_loss: 0.054), currect_learning_rate: 0.0499475800147745\n",
      "epoch: 2200, acc: 0.667, loss: 0.816 (data_loss: 0.762, reg_loss: 0.054), currect_learning_rate: 0.0499450853786262\n",
      "epoch: 2300, acc: 0.677, loss: 0.818 (data_loss: 0.764, reg_loss: 0.054), currect_learning_rate: 0.0499425909916551\n",
      "epoch: 2400, acc: 0.667, loss: 0.780 (data_loss: 0.727, reg_loss: 0.054), currect_learning_rate: 0.04994009685382384\n",
      "epoch: 2500, acc: 0.687, loss: 0.767 (data_loss: 0.714, reg_loss: 0.053), currect_learning_rate: 0.04993760296509512\n",
      "epoch: 2600, acc: 0.703, loss: 0.764 (data_loss: 0.711, reg_loss: 0.053), currect_learning_rate: 0.049935109325431604\n",
      "epoch: 2700, acc: 0.687, loss: 0.771 (data_loss: 0.718, reg_loss: 0.053), currect_learning_rate: 0.049932615934796004\n",
      "epoch: 2800, acc: 0.683, loss: 0.748 (data_loss: 0.694, reg_loss: 0.053), currect_learning_rate: 0.04993012279315099\n",
      "epoch: 2900, acc: 0.690, loss: 0.737 (data_loss: 0.685, reg_loss: 0.052), currect_learning_rate: 0.049927629900459285\n",
      "epoch: 3000, acc: 0.677, loss: 0.771 (data_loss: 0.719, reg_loss: 0.052), currect_learning_rate: 0.049925137256683606\n",
      "epoch: 3100, acc: 0.660, loss: 0.772 (data_loss: 0.720, reg_loss: 0.052), currect_learning_rate: 0.04992264486178666\n",
      "epoch: 3200, acc: 0.657, loss: 0.725 (data_loss: 0.673, reg_loss: 0.052), currect_learning_rate: 0.04992015271573119\n",
      "epoch: 3300, acc: 0.700, loss: 0.764 (data_loss: 0.713, reg_loss: 0.051), currect_learning_rate: 0.04991766081847992\n",
      "epoch: 3400, acc: 0.693, loss: 0.747 (data_loss: 0.696, reg_loss: 0.051), currect_learning_rate: 0.049915169169995596\n",
      "epoch: 3500, acc: 0.703, loss: 0.728 (data_loss: 0.678, reg_loss: 0.050), currect_learning_rate: 0.049912677770240964\n",
      "epoch: 3600, acc: 0.677, loss: 0.773 (data_loss: 0.723, reg_loss: 0.050), currect_learning_rate: 0.049910186619178794\n",
      "epoch: 3700, acc: 0.637, loss: 0.823 (data_loss: 0.772, reg_loss: 0.050), currect_learning_rate: 0.04990769571677183\n",
      "epoch: 3800, acc: 0.627, loss: 0.827 (data_loss: 0.776, reg_loss: 0.051), currect_learning_rate: 0.04990520506298287\n",
      "epoch: 3900, acc: 0.700, loss: 0.727 (data_loss: 0.677, reg_loss: 0.050), currect_learning_rate: 0.04990271465777467\n",
      "epoch: 4000, acc: 0.707, loss: 0.718 (data_loss: 0.668, reg_loss: 0.050), currect_learning_rate: 0.049900224501110035\n",
      "epoch: 4100, acc: 0.693, loss: 0.797 (data_loss: 0.747, reg_loss: 0.051), currect_learning_rate: 0.04989773459295175\n",
      "epoch: 4200, acc: 0.687, loss: 0.731 (data_loss: 0.681, reg_loss: 0.050), currect_learning_rate: 0.049895244933262625\n",
      "epoch: 4300, acc: 0.637, loss: 0.771 (data_loss: 0.721, reg_loss: 0.050), currect_learning_rate: 0.04989275552200545\n",
      "epoch: 4400, acc: 0.690, loss: 0.731 (data_loss: 0.681, reg_loss: 0.050), currect_learning_rate: 0.04989026635914307\n",
      "epoch: 4500, acc: 0.670, loss: 0.811 (data_loss: 0.761, reg_loss: 0.050), currect_learning_rate: 0.049887777444638286\n",
      "epoch: 4600, acc: 0.660, loss: 0.788 (data_loss: 0.737, reg_loss: 0.050), currect_learning_rate: 0.049885288778453954\n",
      "epoch: 4700, acc: 0.690, loss: 0.769 (data_loss: 0.718, reg_loss: 0.051), currect_learning_rate: 0.049882800360552884\n",
      "epoch: 4800, acc: 0.677, loss: 0.754 (data_loss: 0.703, reg_loss: 0.050), currect_learning_rate: 0.04988031219089794\n",
      "epoch: 4900, acc: 0.693, loss: 0.751 (data_loss: 0.701, reg_loss: 0.050), currect_learning_rate: 0.04987782426945198\n",
      "epoch: 5000, acc: 0.657, loss: 0.795 (data_loss: 0.745, reg_loss: 0.050), currect_learning_rate: 0.04987533659617785\n",
      "epoch: 5100, acc: 0.680, loss: 0.755 (data_loss: 0.705, reg_loss: 0.050), currect_learning_rate: 0.049872849171038444\n",
      "epoch: 5200, acc: 0.680, loss: 0.798 (data_loss: 0.748, reg_loss: 0.050), currect_learning_rate: 0.0498703619939966\n",
      "epoch: 5300, acc: 0.723, loss: 0.721 (data_loss: 0.671, reg_loss: 0.050), currect_learning_rate: 0.04986787506501525\n",
      "epoch: 5400, acc: 0.693, loss: 0.786 (data_loss: 0.736, reg_loss: 0.050), currect_learning_rate: 0.049865388384057234\n",
      "epoch: 5500, acc: 0.727, loss: 0.733 (data_loss: 0.683, reg_loss: 0.050), currect_learning_rate: 0.049862901951085496\n",
      "epoch: 5600, acc: 0.717, loss: 0.723 (data_loss: 0.673, reg_loss: 0.050), currect_learning_rate: 0.049860415766062906\n",
      "epoch: 5700, acc: 0.710, loss: 0.753 (data_loss: 0.703, reg_loss: 0.050), currect_learning_rate: 0.0498579298289524\n",
      "epoch: 5800, acc: 0.690, loss: 0.744 (data_loss: 0.694, reg_loss: 0.050), currect_learning_rate: 0.04985544413971689\n",
      "epoch: 5900, acc: 0.670, loss: 0.777 (data_loss: 0.727, reg_loss: 0.050), currect_learning_rate: 0.049852958698319315\n",
      "epoch: 6000, acc: 0.663, loss: 0.760 (data_loss: 0.711, reg_loss: 0.050), currect_learning_rate: 0.04985047350472258\n",
      "epoch: 6100, acc: 0.683, loss: 0.780 (data_loss: 0.731, reg_loss: 0.050), currect_learning_rate: 0.04984798855888967\n",
      "epoch: 6200, acc: 0.713, loss: 0.732 (data_loss: 0.682, reg_loss: 0.050), currect_learning_rate: 0.0498455038607835\n",
      "epoch: 6300, acc: 0.670, loss: 0.787 (data_loss: 0.737, reg_loss: 0.049), currect_learning_rate: 0.049843019410367055\n",
      "epoch: 6400, acc: 0.713, loss: 0.715 (data_loss: 0.666, reg_loss: 0.049), currect_learning_rate: 0.04984053520760327\n",
      "epoch: 6500, acc: 0.697, loss: 0.749 (data_loss: 0.700, reg_loss: 0.050), currect_learning_rate: 0.049838051252455155\n",
      "epoch: 6600, acc: 0.703, loss: 0.726 (data_loss: 0.676, reg_loss: 0.050), currect_learning_rate: 0.049835567544885655\n",
      "epoch: 6700, acc: 0.710, loss: 0.712 (data_loss: 0.662, reg_loss: 0.050), currect_learning_rate: 0.04983308408485778\n",
      "epoch: 6800, acc: 0.733, loss: 0.735 (data_loss: 0.686, reg_loss: 0.049), currect_learning_rate: 0.0498306008723345\n",
      "epoch: 6900, acc: 0.737, loss: 0.728 (data_loss: 0.679, reg_loss: 0.049), currect_learning_rate: 0.04982811790727884\n",
      "epoch: 7000, acc: 0.717, loss: 0.700 (data_loss: 0.650, reg_loss: 0.050), currect_learning_rate: 0.04982563518965381\n",
      "epoch: 7100, acc: 0.700, loss: 0.711 (data_loss: 0.661, reg_loss: 0.050), currect_learning_rate: 0.049823152719422406\n",
      "epoch: 7200, acc: 0.683, loss: 0.734 (data_loss: 0.683, reg_loss: 0.050), currect_learning_rate: 0.04982067049654768\n",
      "epoch: 7300, acc: 0.713, loss: 0.703 (data_loss: 0.652, reg_loss: 0.051), currect_learning_rate: 0.04981818852099264\n",
      "epoch: 7400, acc: 0.707, loss: 0.705 (data_loss: 0.655, reg_loss: 0.051), currect_learning_rate: 0.049815706792720335\n",
      "epoch: 7500, acc: 0.693, loss: 0.747 (data_loss: 0.696, reg_loss: 0.051), currect_learning_rate: 0.049813225311693805\n",
      "epoch: 7600, acc: 0.723, loss: 0.718 (data_loss: 0.667, reg_loss: 0.051), currect_learning_rate: 0.04981074407787611\n",
      "epoch: 7700, acc: 0.727, loss: 0.705 (data_loss: 0.654, reg_loss: 0.051), currect_learning_rate: 0.049808263091230306\n",
      "epoch: 7800, acc: 0.700, loss: 0.763 (data_loss: 0.712, reg_loss: 0.051), currect_learning_rate: 0.04980578235171947\n",
      "epoch: 7900, acc: 0.697, loss: 0.700 (data_loss: 0.649, reg_loss: 0.052), currect_learning_rate: 0.04980330185930667\n",
      "epoch: 8000, acc: 0.667, loss: 0.714 (data_loss: 0.662, reg_loss: 0.051), currect_learning_rate: 0.04980082161395499\n",
      "epoch: 8100, acc: 0.733, loss: 0.690 (data_loss: 0.639, reg_loss: 0.051), currect_learning_rate: 0.04979834161562752\n",
      "epoch: 8200, acc: 0.727, loss: 0.748 (data_loss: 0.698, reg_loss: 0.050), currect_learning_rate: 0.04979586186428736\n",
      "epoch: 8300, acc: 0.680, loss: 0.750 (data_loss: 0.699, reg_loss: 0.051), currect_learning_rate: 0.04979338235989761\n",
      "epoch: 8400, acc: 0.720, loss: 0.705 (data_loss: 0.654, reg_loss: 0.051), currect_learning_rate: 0.04979090310242139\n",
      "epoch: 8500, acc: 0.697, loss: 0.750 (data_loss: 0.700, reg_loss: 0.050), currect_learning_rate: 0.049788424091821805\n",
      "epoch: 8600, acc: 0.687, loss: 0.726 (data_loss: 0.677, reg_loss: 0.050), currect_learning_rate: 0.049785945328062006\n",
      "epoch: 8700, acc: 0.720, loss: 0.692 (data_loss: 0.641, reg_loss: 0.050), currect_learning_rate: 0.0497834668111051\n",
      "epoch: 8800, acc: 0.677, loss: 0.716 (data_loss: 0.666, reg_loss: 0.050), currect_learning_rate: 0.049780988540914256\n",
      "epoch: 8900, acc: 0.697, loss: 0.737 (data_loss: 0.687, reg_loss: 0.050), currect_learning_rate: 0.0497785105174526\n",
      "epoch: 9000, acc: 0.697, loss: 0.730 (data_loss: 0.680, reg_loss: 0.050), currect_learning_rate: 0.04977603274068329\n",
      "epoch: 9100, acc: 0.733, loss: 0.715 (data_loss: 0.666, reg_loss: 0.050), currect_learning_rate: 0.04977355521056952\n",
      "epoch: 9200, acc: 0.717, loss: 0.693 (data_loss: 0.643, reg_loss: 0.049), currect_learning_rate: 0.04977107792707442\n",
      "epoch: 9300, acc: 0.673, loss: 0.746 (data_loss: 0.696, reg_loss: 0.050), currect_learning_rate: 0.0497686008901612\n",
      "epoch: 9400, acc: 0.697, loss: 0.725 (data_loss: 0.676, reg_loss: 0.050), currect_learning_rate: 0.04976612409979302\n",
      "epoch: 9500, acc: 0.713, loss: 0.676 (data_loss: 0.627, reg_loss: 0.049), currect_learning_rate: 0.0497636475559331\n",
      "epoch: 9600, acc: 0.697, loss: 0.682 (data_loss: 0.632, reg_loss: 0.050), currect_learning_rate: 0.049761171258544616\n",
      "epoch: 9700, acc: 0.700, loss: 0.701 (data_loss: 0.651, reg_loss: 0.050), currect_learning_rate: 0.04975869520759079\n",
      "epoch: 9800, acc: 0.730, loss: 0.722 (data_loss: 0.673, reg_loss: 0.049), currect_learning_rate: 0.04975621940303483\n",
      "epoch: 9900, acc: 0.710, loss: 0.727 (data_loss: 0.677, reg_loss: 0.050), currect_learning_rate: 0.04975374384483997\n",
      "epoch: 10000, acc: 0.680, loss: 0.762 (data_loss: 0.713, reg_loss: 0.050), currect_learning_rate: 0.04975126853296942\n"
     ]
    }
   ],
   "source": [
    "# now lets run it with out prevoius network\n",
    "# now lets add softmax also in this \n",
    "X, y = spiral_data(samples=100, classes=3) \n",
    " \n",
    "# Create Dense layer with 2 input features and 64 output values \n",
    "dense1 = Layer_Dense(2, 64,weight_regularizer_l2=5e-4, bias_regularizer_l2=5e-4) \n",
    " \n",
    "# Create ReLU activation (to be used with Dense layer): \n",
    "activation1 = Activation_ReLU() \n",
    "dropout1 = Layer_dropout(0.1)\n",
    "\n",
    "# Create second Dense layer with 64 input features (as we take output  \n",
    "# of previous layer here) and 3 output values (output values) \n",
    "dense2 = Layer_Dense(64, 3) \n",
    " \n",
    "# Create Softmax classifier's combined loss and activation \n",
    "loss_activation = Activation_Softmax_Loss_CategoricalCrossentropy() \n",
    " \n",
    "# Create optimizer \n",
    "optimizer = Optimizer_Adam(decay=5e-7,Learning_rate=0.05) \n",
    " \n",
    "# Train in loop \n",
    "for epoch in range(10001): \n",
    " \n",
    "    # Perform a forward pass of our training data through this layer \n",
    "    dense1.forward(X) \n",
    " \n",
    "    # Perform a forward pass through activation function \n",
    "    # takes the output of first dense layer here \n",
    "    activation1.forward(dense1.output) \n",
    "    dropout1.forward(activation1.output)\n",
    "    # Perform a forward pass through second Dense layer \n",
    "    # takes outputs of activation function of first layer as inputs \n",
    "    dense2.forward(dropout1.output) \n",
    "\n",
    "    # Perform a forward pass through the activation/loss function \n",
    "    # takes the output of second dense layer here and returns loss \n",
    "  \n",
    "    data_loss = loss_activation.forward(dense2.output, y)\n",
    "     # Calculate regularization penalty \n",
    "    regularization_loss = loss_activation.loss.regularization_loss(dense1) + loss_activation.loss.regularization_loss(dense2) \n",
    "    #overall loss\n",
    "    loss = data_loss + regularization_loss\n",
    "    # Calculate accuracy from output of activation2 and targets \n",
    "    # calculate values along first axis \n",
    "    predictions = np.argmax(loss_activation.output, axis=1) \n",
    "    if len(y.shape) == 2: \n",
    "        y = np.argmax(y, axis=1) \n",
    "    accuracy = np.mean(predictions==y) \n",
    " \n",
    "    if not epoch % 100: \n",
    "        print(f'epoch: {epoch}, ' + \n",
    "              f'acc: {accuracy:.3f}, ' + \n",
    "              f'loss: {loss:.3f} (' + \n",
    "              f'data_loss: {data_loss:.3f}, ' + \n",
    "              f'reg_loss: {regularization_loss:.3f}), ' + \n",
    "              f'currect_learning_rate: {optimizer.current_Learning_rate}')\n",
    " \n",
    "    # Backward pass \n",
    "    loss_activation.backward(loss_activation.output, y)\n",
    "    dense2.backward(loss_activation.dinputs)\n",
    "    dropout1.backward(dense2.dinputs)\n",
    "    activation1.backward(dropout1.dinputs) \n",
    "    dense1.backward(activation1.dinputs) \n",
    " # i noticed that softmax was not added in this so i made activation 2 and also applied it here but now its taking a lot more time, a lot more time.\n",
    "    # Update weights and biases \n",
    "    optimizer.pre_update_params()\n",
    "    optimizer.update_params(dense1) \n",
    "    optimizer.update_params(dense2) \n",
    "    optimizer.post_update_params()\n",
    "    # with softmax_activations acc: 78%\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "017bfd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "validation, acc: 0.723, loss: 0.858\n"
     ]
    }
   ],
   "source": [
    "# validating the model\n",
    "X_test, y_test = spiral_data(samples=100, classes=3)\n",
    "dense1.forward(X_test)\n",
    "activation1.forward(dense1.output)\n",
    "dense2.forward(activation1.output)\n",
    "loss = loss_activation.forward(dense2.output, y_test)\n",
    "predictions = np.argmax(loss_activation.output,axis=1)\n",
    "if(len(y_test.shape)==2):\n",
    "  y_test = np.argmax(y_test,axis=1)\n",
    "accuracy = np.mean(predictions == y_test)\n",
    "print(f'validation, acc: {accuracy:.3f}, loss: {loss:.3f}') \n",
    "# increasing the number of neurons did help and also bringing the regu. in made the validation accuracy match the trainig accuracy removing overfitting,\n",
    "# here our validation accuracy is better than the training accuracy and same for loss\n",
    "# its remarkable meaning there is no overfitting at all\n",
    "# here our dropout concept is completed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "id": "7dbab245",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will learn binary logistic regression\n",
    "# we use sigmoid in this so lets code that \n",
    "class Activation_sigmoid():\n",
    "  def forward(self,inputs):\n",
    "    self.inputs = inputs\n",
    "    self.output = 1/(1+np.exp(-inputs))\n",
    "  def backward(self, dvalues):\n",
    "    self.dinputs = dvalues*(1-self.output)*self.output\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "11732f3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "#now we will see binary cross entropy loss\n",
    "class Loss_BinaryCrossentropy(Loss):\n",
    "  def forward(self, y_pred, y_true):\n",
    "    y_pred_clipped = np.clip(y_pred,1e-7, 1-1e-7)\n",
    "    sample_losses = -(y_true*np.log(y_pred_clipped)+ (1-y_true)*(np.log(1-y_pred_clipped)))\n",
    "    sample_losses = np.mean(sample_losses, axis = -1)\n",
    "    return sample_losses\n",
    "  def backward(self,dvalues, y_true):\n",
    "    samples = len(dvalues)\n",
    "    outputs = len(dvalues[0])\n",
    "    clipped_dvalues = np.clip(dvalues,1e-7,1-1e-7)\n",
    "    self.dinputs= -(y_true/clipped_dvalues-(1-y_true)/(1-clipped_dvalues))/output\n",
    "    self.dinputs = self.dinputs/samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "963fbf8f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [0]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]\n",
      " [1]]\n"
     ]
    }
   ],
   "source": [
    "#lets implement this on the X,y spira; dataset\n",
    "X,y = spiral_data(samples=100, classes=2)\n",
    "y = y.reshape(-1,1)# its quite important\n",
    "print(y)\n",
    "# i am leaving this for now and will complete regression first then i will come back to it and do it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "3c00598c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# linear regression or just regression \n",
    "# we willtry to fit sine so lets import it \n",
    "import matplotlib.pyplot as plt\n",
    "import nnfs\n",
    "from nnfs.datasets import sine_data\n",
    "\n",
    "nnfs.init()\n",
    "\n",
    "X,y = sine_data()\n",
    "plt.plot(X,y)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "id": "61a92fc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sice we are no longer using classification lables and want to predict a scalar value, we're going to use a linear activation function for the output layer, This linear function does not modify its input asn passes it to the output y =x> for the backward pass, we already know the derivative of f(x) = x is 1, \n",
    "# so\n",
    "class Activation_Linear():\n",
    "  def forward (self, inputs):\n",
    "    self.inputs = inputs \n",
    "    self.output = inputs\n",
    "\n",
    "  def backward(self,dvalues):\n",
    "    self.dinputs = dvalues.copy()\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "id": "43e27682",
   "metadata": {},
   "outputs": [],
   "source": [
    "# now we will see mean squre loss for regression \n",
    "class Loss_MeanSquaredError(Loss):\n",
    "  def forward(self, y_pred, y_true):\n",
    "    sample_losses = np.mean((y_true-y_pred)**2, axis=-1)\n",
    "    return sample_losses\n",
    "  def backward(self,dvalues,y_true):\n",
    "    samples = len(dvalues)\n",
    "    outputs = len(dvalues[0])\n",
    "    self.dinputs = -2*(y_true-dvalues)/outputs\n",
    "    self.dinputs = self.dinputs/samples\n",
    "  # for mean absolute error \n",
    "class Loss_MeanAbsoluteError(Loss):\n",
    "  def forward(self,y_pred,y_true):\n",
    "    sample_losses = np.mean(np.abs(y_true-y_pred), axis=-1)\n",
    "    return sample_losses\n",
    "  def backward(self,dvalues,y_true):\n",
    "    samples = len(dvalues)\n",
    "    outputs = len(dvalues[0])\n",
    "    self.dinputs = np.sign(y_true-dvalues)/outputs\n",
    "    self.dinputs  = self.dinputs/samples\n",
    "# accuracy calcultion\n",
    "accuracy_precision = np.std(y)/250# std takes the standard devviation of input given \n",
    "accuracy = np.mean(np.absolute(predictions-y)<accuracy_precision)# only taking those output which are in the precisison range this is our accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "id": "a2e754bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dense layer \n",
    "class Layer_Dense: \n",
    " \n",
    "    # Layer initialization \n",
    "    def __init__(self, n_inputs, n_neurons,weight_regularizer_l1 =0,weight_regularizer_l2=0,bias_regularizer_l1 =0, bias_regularizer_l2=0): \n",
    "        # Initialize weights and biases \n",
    "        self.weights = 0.1 * np.random.randn(n_inputs, n_neurons) \n",
    "        self.biases = np.zeros((1, n_neurons)) \n",
    "        # Set regularization strength \n",
    "        self.weight_regularizer_l1 = weight_regularizer_l1 \n",
    "        self.weight_regularizer_l2 = weight_regularizer_l2 \n",
    "        self.bias_regularizer_l1 = bias_regularizer_l1 \n",
    "        self.bias_regularizer_l2 = bias_regularizer_l2 \n",
    "     # Forward pass \n",
    "    def forward(self, inputs): \n",
    "        # Remember input values \n",
    "        self.inputs = inputs \n",
    "        # Calculate output values from inputs, weights and biases \n",
    "        self.output = np.dot(inputs, self.weights) + self.biases \n",
    " \n",
    "    # Backward pass \n",
    "    def backward(self, dvalues): \n",
    "        # Gradients on parameters \n",
    "        self.dweights = np.dot(self.inputs.T, dvalues) \n",
    "        self.dbiases = np.sum(dvalues, axis=0, keepdims=True) \n",
    "        # Gradients on regularization \n",
    "    # L1 on weights \n",
    "        if self.weight_regularizer_l1 > 0: # i dont get why only when the regularization constant are positive we are doin this \n",
    "            dL1 = np.ones_like(self.weights) \n",
    "            dL1[self.weights < 0] = -1 \n",
    "            self.dweights += self.weight_regularizer_l1 * dL1 \n",
    "        # L2 on weights \n",
    "        if self.weight_regularizer_l2 > 0: \n",
    "            self.dweights += 2 * self.weight_regularizer_l2 *  self.weights \n",
    "        # L1 on biases \n",
    "        if self.bias_regularizer_l1 > 0: \n",
    "            dL1 = np.ones_like(self.biases) \n",
    "            dL1[self.biases < 0] = -1 \n",
    "            self.dbiases += self.bias_regularizer_l1 * dL1 \n",
    "        # L2 on biases \n",
    "        if self.bias_regularizer_l2 > 0: \n",
    "            self.dbiases += 2 * self.bias_regularizer_l2 *  self.biases \n",
    "        # Gradient on values \n",
    "        self.dinputs = np.dot(dvalues, self.weights.T) \n",
    " \n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "id": "e5e059c7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch: 0, acc: 0.000, loss: 4.546 (data_loss: 4.546, reg_loss: 0.000), lr: 0.005\n",
      "epoch: 100, acc: 0.000, loss: 0.625 (data_loss: 0.625, reg_loss: 0.000), lr: 0.004549590536851684\n",
      "epoch: 200, acc: 0.003, loss: 0.423 (data_loss: 0.423, reg_loss: 0.000), lr: 0.004170141784820684\n",
      "epoch: 300, acc: 0.003, loss: 0.193 (data_loss: 0.193, reg_loss: 0.000), lr: 0.003849114703618168\n",
      "epoch: 400, acc: 0.004, loss: 0.152 (data_loss: 0.152, reg_loss: 0.000), lr: 0.0035739814152966403\n",
      "epoch: 500, acc: 0.005, loss: 0.136 (data_loss: 0.136, reg_loss: 0.000), lr: 0.00333555703802535\n",
      "epoch: 600, acc: 0.004, loss: 0.108 (data_loss: 0.108, reg_loss: 0.000), lr: 0.0031269543464665416\n",
      "epoch: 700, acc: 0.005, loss: 0.068 (data_loss: 0.068, reg_loss: 0.000), lr: 0.002942907592701589\n",
      "epoch: 800, acc: 0.008, loss: 0.040 (data_loss: 0.040, reg_loss: 0.000), lr: 0.0027793218454697055\n",
      "epoch: 900, acc: 0.014, loss: 0.030 (data_loss: 0.030, reg_loss: 0.000), lr: 0.0026329647182727752\n",
      "epoch: 1000, acc: 0.019, loss: 0.022 (data_loss: 0.022, reg_loss: 0.000), lr: 0.0025012506253126563\n",
      "epoch: 1100, acc: 0.024, loss: 0.014 (data_loss: 0.014, reg_loss: 0.000), lr: 0.0023820867079561692\n",
      "epoch: 1200, acc: 0.026, loss: 0.009 (data_loss: 0.009, reg_loss: 0.000), lr: 0.002273760800363802\n",
      "epoch: 1300, acc: 0.032, loss: 0.005 (data_loss: 0.005, reg_loss: 0.000), lr: 0.002174858634188778\n",
      "epoch: 1400, acc: 0.043, loss: 0.003 (data_loss: 0.003, reg_loss: 0.000), lr: 0.0020842017507294707\n",
      "epoch: 1500, acc: 0.058, loss: 0.002 (data_loss: 0.002, reg_loss: 0.000), lr: 0.0020008003201280513\n",
      "epoch: 1600, acc: 0.073, loss: 0.002 (data_loss: 0.002, reg_loss: 0.000), lr: 0.001923816852635629\n",
      "epoch: 1700, acc: 0.116, loss: 0.002 (data_loss: 0.002, reg_loss: 0.000), lr: 0.0018525379770285293\n",
      "epoch: 1800, acc: 0.164, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0017863522686673813\n",
      "epoch: 1900, acc: 0.218, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0017247326664367024\n",
      "epoch: 2000, acc: 0.244, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0016672224074691564\n",
      "epoch: 2100, acc: 0.250, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0016134236850596966\n",
      "epoch: 2200, acc: 0.256, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0015629884338855893\n",
      "epoch: 2300, acc: 0.267, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.001515610791148833\n",
      "epoch: 2400, acc: 0.275, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0014710208884966167\n",
      "epoch: 2500, acc: 0.272, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0014289797084881394\n",
      "epoch: 2600, acc: 0.275, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.001389274798555154\n",
      "epoch: 2700, acc: 0.274, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0013517166801838335\n",
      "epoch: 2800, acc: 0.274, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0013161358252171624\n",
      "epoch: 2900, acc: 0.281, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0012823800974608875\n",
      "epoch: 3000, acc: 0.282, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.001250312578144536\n",
      "epoch: 3100, acc: 0.283, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0012198097096852891\n",
      "epoch: 3200, acc: 0.281, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0011907597046915933\n",
      "epoch: 3300, acc: 0.283, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0011630611770179114\n",
      "epoch: 3400, acc: 0.282, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0011366219595362582\n",
      "epoch: 3500, acc: 0.301, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0011113580795732384\n",
      "epoch: 3600, acc: 0.302, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0010871928680147858\n",
      "epoch: 3700, acc: 0.303, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0010640561821664185\n",
      "epoch: 3800, acc: 0.307, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0010418837257762034\n",
      "epoch: 3900, acc: 0.307, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0010206164523372118\n",
      "epoch: 4000, acc: 0.309, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0010002000400080016\n",
      "epoch: 4100, acc: 0.313, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0009805844283192783\n",
      "epoch: 4200, acc: 0.312, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0009617234083477592\n",
      "epoch: 4300, acc: 0.318, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0009435742592942064\n",
      "epoch: 4400, acc: 0.319, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0009260974254491573\n",
      "epoch: 4500, acc: 0.320, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0009092562284051647\n",
      "epoch: 4600, acc: 0.324, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.000893016610108948\n",
      "epoch: 4700, acc: 0.324, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0008773469029654326\n",
      "epoch: 4800, acc: 0.329, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.000862217623728229\n",
      "epoch: 4900, acc: 0.329, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0008476012883539584\n",
      "epoch: 5000, acc: 0.332, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0008334722453742291\n",
      "epoch: 5100, acc: 0.334, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0008198065256599442\n",
      "epoch: 5200, acc: 0.339, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0008065817067268914\n",
      "epoch: 5300, acc: 0.340, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0007937767899666614\n",
      "epoch: 5400, acc: 0.341, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.000781372089388967\n",
      "epoch: 5500, acc: 0.342, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0007693491306354824\n",
      "epoch: 5600, acc: 0.342, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0007576905591756327\n",
      "epoch: 5700, acc: 0.346, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0007463800567248844\n",
      "epoch: 5800, acc: 0.350, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0007354022650389762\n",
      "epoch: 5900, acc: 0.354, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0007247427163357008\n",
      "epoch: 6000, acc: 0.358, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.000714387769681383\n",
      "epoch: 6100, acc: 0.357, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.000704324552753909\n",
      "epoch: 6200, acc: 0.361, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0006945409084595083\n",
      "epoch: 6300, acc: 0.365, loss: 0.001 (data_loss: 0.001, reg_loss: 0.000), lr: 0.0006850253459377997\n",
      "epoch: 6400, acc: 0.368, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006757669955399379\n",
      "epoch: 6500, acc: 0.370, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006667555674089878\n",
      "epoch: 6600, acc: 0.372, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006579813133307014\n",
      "epoch: 6700, acc: 0.375, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006494349915573452\n",
      "epoch: 6800, acc: 0.380, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006411078343377355\n",
      "epoch: 6900, acc: 0.381, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006329915179136599\n",
      "epoch: 7000, acc: 0.383, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006250781347668458\n",
      "epoch: 7100, acc: 0.387, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006173601679219657\n",
      "epoch: 7200, acc: 0.389, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006098304671301379\n",
      "epoch: 7300, acc: 0.390, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0006024822267743102\n",
      "epoch: 7400, acc: 0.392, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005953089653530181\n",
      "epoch: 7500, acc: 0.395, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005883045064125191\n",
      "epoch: 7600, acc: 0.400, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005814629608093965\n",
      "epoch: 7700, acc: 0.401, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005747787101965744\n",
      "epoch: 7800, acc: 0.406, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005682463916354131\n",
      "epoch: 7900, acc: 0.407, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005618608832453085\n",
      "epoch: 8000, acc: 0.409, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00055561729081009\n",
      "epoch: 8100, acc: 0.409, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005495109352676119\n",
      "epoch: 8200, acc: 0.412, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005435373410153278\n",
      "epoch: 8300, acc: 0.413, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005376922249704269\n",
      "epoch: 8400, acc: 0.417, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005319714863283328\n",
      "epoch: 8500, acc: 0.416, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005263711969681019\n",
      "epoch: 8600, acc: 0.417, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005208875924575477\n",
      "epoch: 8700, acc: 0.413, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005155170636148056\n",
      "epoch: 8800, acc: 0.412, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005102561485865905\n",
      "epoch: 8900, acc: 0.417, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005051015254066067\n",
      "epoch: 9000, acc: 0.417, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0005000500050005\n",
      "epoch: 9100, acc: 0.414, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004950985246063966\n",
      "epoch: 9200, acc: 0.415, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004902441415825081\n",
      "epoch: 9300, acc: 0.412, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004854840275754928\n",
      "epoch: 9400, acc: 0.414, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00048081546302529085\n",
      "epoch: 9500, acc: 0.409, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00047623583198399844\n",
      "epoch: 9600, acc: 0.414, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00047174261722804036\n",
      "epoch: 9700, acc: 0.408, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00046733339564445275\n",
      "epoch: 9800, acc: 0.402, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004630058338735068\n",
      "epoch: 9900, acc: 0.399, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00045875768419121016\n",
      "epoch: 10000, acc: 0.405, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00045458678061641964\n",
      "epoch: 10100, acc: 0.400, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00045049103522839894\n",
      "epoch: 10200, acc: 0.398, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00044646843468166804\n",
      "epoch: 10300, acc: 0.401, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004425170369059209\n",
      "epoch: 10400, acc: 0.398, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004386349679796473\n",
      "epoch: 10500, acc: 0.399, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004348204191668841\n",
      "epoch: 10600, acc: 0.400, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004310716441072506\n",
      "epoch: 10700, acc: 0.400, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004273869561500983\n",
      "epoch: 10800, acc: 0.399, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004237647258242224\n",
      "epoch: 10900, acc: 0.401, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004202033784351626\n",
      "epoch: 11000, acc: 0.403, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00041670139178264854\n",
      "epoch: 11100, acc: 0.400, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00041325729399123895\n",
      "epoch: 11200, acc: 0.406, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00040986966144765965\n",
      "epoch: 11300, acc: 0.405, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004065371168387674\n",
      "epoch: 11400, acc: 0.403, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004032583272844584\n",
      "epoch: 11500, acc: 0.404, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0004000320025602048\n",
      "epoch: 11600, acc: 0.405, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00039685689340423846\n",
      "epoch: 11700, acc: 0.409, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003937317899047169\n",
      "epoch: 11800, acc: 0.405, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003906555199624971\n",
      "epoch: 11900, acc: 0.408, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003876269478254128\n",
      "epoch: 12000, acc: 0.410, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00038464497269020693\n",
      "epoch: 12100, acc: 0.410, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00038170852736850143\n",
      "epoch: 12200, acc: 0.409, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003788165770134101\n",
      "epoch: 12300, acc: 0.419, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003759681179036018\n",
      "epoch: 12400, acc: 0.414, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003731621762818121\n",
      "epoch: 12500, acc: 0.420, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003703978072449811\n",
      "epoch: 12600, acc: 0.415, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00036767409368335905\n",
      "epoch: 12700, acc: 0.427, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00036499014526607785\n",
      "epoch: 12800, acc: 0.420, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00036234509747083124\n",
      "epoch: 12900, acc: 0.417, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00035973811065544283\n",
      "epoch: 13000, acc: 0.423, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003571683691692264\n",
      "epoch: 13100, acc: 0.424, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00035463508050216327\n",
      "epoch: 13200, acc: 0.324, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003521374744700331\n",
      "epoch: 13300, acc: 0.425, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00034967480243373666\n",
      "epoch: 13400, acc: 0.423, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00034724633655114936\n",
      "epoch: 13500, acc: 0.430, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00034485136905993514\n",
      "epoch: 13600, acc: 0.430, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003424892115898349\n",
      "epoch: 13700, acc: 0.433, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00034015919450302743\n",
      "epoch: 13800, acc: 0.436, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003378606662612339\n",
      "epoch: 13900, acc: 0.437, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00033559299281830994\n",
      "epoch: 14000, acc: 0.437, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003333555570371358\n",
      "epoch: 14100, acc: 0.439, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003311477581296775\n",
      "epoch: 14200, acc: 0.437, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003289690111191526\n",
      "epoch: 14300, acc: 0.434, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003268187463232891\n",
      "epoch: 14400, acc: 0.440, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.000324696408857718\n",
      "epoch: 14500, acc: 0.442, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00032260145815859087\n",
      "epoch: 14600, acc: 0.441, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003205333675235592\n",
      "epoch: 14700, acc: 0.445, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003184916236702975\n",
      "epoch: 14800, acc: 0.446, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003164757263117919\n",
      "epoch: 14900, acc: 0.448, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003144851877476571\n",
      "epoch: 15000, acc: 0.449, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003125195324707794\n",
      "epoch: 15100, acc: 0.452, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003105782967886204\n",
      "epoch: 15200, acc: 0.448, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003086610284585469\n",
      "epoch: 15300, acc: 0.453, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003067672863365851\n",
      "epoch: 15400, acc: 0.450, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00030489664003902674\n",
      "epoch: 15500, acc: 0.451, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00030304866961634034\n",
      "epoch: 15600, acc: 0.451, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0003012229652388698\n",
      "epoch: 15700, acc: 0.443, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.000299419126893826\n",
      "epoch: 15800, acc: 0.453, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0002976367640931008\n",
      "epoch: 15900, acc: 0.447, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0002958754955914551\n",
      "epoch: 16000, acc: 0.457, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00029413494911465377\n",
      "epoch: 16100, acc: 0.458, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0002924147610971402\n",
      "epoch: 16200, acc: 0.463, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00029071457642886213\n",
      "epoch: 16300, acc: 0.465, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00028903404821087927\n",
      "epoch: 16400, acc: 0.451, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0002873728375193977\n",
      "epoch: 16500, acc: 0.471, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0002857306131778959\n",
      "epoch: 16600, acc: 0.455, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00028410705153701914\n",
      "epoch: 16700, acc: 0.470, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0002825018362619357\n",
      "epoch: 16800, acc: 0.489, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00028091465812686106\n",
      "epoch: 16900, acc: 0.474, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0002793452148164702\n",
      "epoch: 17000, acc: 0.470, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0002777932107339297\n",
      "epoch: 17100, acc: 0.479, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00027625835681529366\n",
      "epoch: 17200, acc: 0.484, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0002747403703500192\n",
      "epoch: 17300, acc: 0.482, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00027323897480736653\n",
      "epoch: 17400, acc: 0.482, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00027175389966846024\n",
      "epoch: 17500, acc: 0.482, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0002702848802637981\n",
      "epoch: 17600, acc: 0.478, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00026883165761600085\n",
      "epoch: 17700, acc: 0.483, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00026739397828760895\n",
      "epoch: 17800, acc: 0.469, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00026597159423373583\n",
      "epoch: 17900, acc: 0.485, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00026456426265939994\n",
      "epoch: 18000, acc: 0.486, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0002631717458813622\n",
      "epoch: 18100, acc: 0.486, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00026179381119430337\n",
      "epoch: 18200, acc: 0.488, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00026043023074118443\n",
      "epoch: 18300, acc: 0.484, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0002590807813876367\n",
      "epoch: 18400, acc: 0.489, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0002577452446002371\n",
      "epoch: 18500, acc: 0.484, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00025642340632852967\n",
      "epoch: 18600, acc: 0.491, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0002551150568906577\n",
      "epoch: 18700, acc: 0.493, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0002538199908624803\n",
      "epoch: 18800, acc: 0.494, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.000252538006970049\n",
      "epoch: 18900, acc: 0.489, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0002512689079853259\n",
      "epoch: 19000, acc: 0.497, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00025001250062503126\n",
      "epoch: 19100, acc: 0.494, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00024876859545251007\n",
      "epoch: 19200, acc: 0.498, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.000247537006782514\n",
      "epoch: 19300, acc: 0.500, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0002463175525887975\n",
      "epoch: 19400, acc: 0.498, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0002451100544144321\n",
      "epoch: 19500, acc: 0.502, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00024391433728474562\n",
      "epoch: 19600, acc: 0.501, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00024273022962279723\n",
      "epoch: 19700, acc: 0.493, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00024155756316730277\n",
      "epoch: 19800, acc: 0.502, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00024039617289292755\n",
      "epoch: 19900, acc: 0.504, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.0002392458969328676\n",
      "epoch: 20000, acc: 0.504, loss: 0.000 (data_loss: 0.000, reg_loss: 0.000), lr: 0.00023810657650364306\n"
     ]
    }
   ],
   "source": [
    "X,y = sine_data()\n",
    "y=y+2\n",
    "dense1 = Layer_Dense(1,64)\n",
    "activation1 = Activation_ReLU()\n",
    "dense2 = Layer_Dense(64,64)\n",
    "activation2 = Activation_ReLU()\n",
    "dense3 = Layer_Dense(64,1)\n",
    "activation3 = Activation_Linear()\n",
    "loss_function = Loss_MeanSquaredError()\n",
    "optimizer = Optimizer_Adam(Learning_rate=.005,decay=1e-3)\n",
    "accuracy_precision = np.std(y)/250\n",
    "for epoch in range(20001):\n",
    "  dense1.forward(X)\n",
    "  activation1.forward(dense1.output)\n",
    "  dense2.forward(activation1.output)\n",
    "\n",
    "  activation2.forward(dense2.output)\n",
    "  dense3.forward(activation2.output)\n",
    "  activation3.forward(dense3.output)\n",
    "  data_loss = loss_function.calculate(activation3.output,y)\n",
    "  regularization_loss = loss_function.regularization_loss(dense1)+ loss_function.regularization_loss(dense2)  + loss_function.regularization_loss(dense3)\n",
    "  loss = data_loss + regularization_loss\n",
    "  predictions = activation3.output\n",
    "  accuracy = np.mean(np.absolute(predictions-y)<accuracy_precision)\n",
    "  if not epoch % 100: \n",
    "    print(f'epoch: {epoch}, ' + \n",
    "          f'acc: {accuracy:.3f}, ' + \n",
    "          f'loss: {loss:.3f} (' + \n",
    "          f'data_loss: {data_loss:.3f}, ' + \n",
    "          f'reg_loss: {regularization_loss:.3f}), ' + \n",
    "          f'lr: {optimizer.current_Learning_rate}') \n",
    "\n",
    "# Backward pass \n",
    "  loss_function.backward(activation3.output, y) \n",
    "  activation3.backward(loss_function.dinputs) \n",
    "  dense3.backward(activation3.dinputs) \n",
    "  activation2.backward(dense3.dinputs) \n",
    "  dense2.backward(activation2.dinputs) \n",
    "  activation1.backward(dense2.dinputs) \n",
    "  dense1.backward(activation1.dinputs) \n",
    "\n",
    "  optimizer.pre_update_params()\n",
    "  optimizer.update_params(dense1)\n",
    "  optimizer.update_params(dense2)\n",
    "  optimizer.update_params(dense3)\n",
    "  optimizer.post_update_params()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "id": "687857c2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt \n",
    " \n",
    "X_test, y_test = sine_data() \n",
    "y_test+=2\n",
    "dense1.forward(X_test) \n",
    "activation1.forward(dense1.output) \n",
    "dense2.forward(activation1.output) \n",
    "activation2.forward(dense2.output) \n",
    "dense3.forward(activation2.output) \n",
    "activation3.forward(dense3.output) \n",
    "plt.plot(X_test, y_test) \n",
    "plt.plot(X_test, activation3.output) \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c54260",
   "metadata": {},
   "outputs": [],
   "source": [
    "# so all the relevant part of the neural network is done now i will do different libraries and will use them to make different models and see how convienient are they\n",
    "# but first i will revise what i have learnt\n",
    "# also questions for todays meet\n",
    "# start with back propogation\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
